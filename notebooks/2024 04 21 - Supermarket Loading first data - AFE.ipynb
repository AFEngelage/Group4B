{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpLWCBFNuons"
   },
   "source": [
    "### Supermarket data science case study - Exploring first data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPQo8za16cQT"
   },
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1608147217504,
     "user": {
      "displayName": "pi1234",
      "photoUrl": "",
      "userId": "00485952581093036663"
     },
     "user_tz": -60
    },
    "id": "OlwScieYRTmf",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "#import vegafusion as vf\n",
    "import sklearn\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm7aDtim6guo"
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1608147217505,
     "user": {
      "displayName": "pi1234",
      "photoUrl": "",
      "userId": "00485952581093036663"
     },
     "user_tz": -60
    },
    "id": "f1PDhgg8ZBbf",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def f_concat(l_input):\n",
    "\n",
    "    # Initialize.\n",
    "    dummy = \"\"\n",
    "    n_len = len(l_input)\n",
    "\n",
    "    if n_len == 1:\n",
    "        return l_input[0]\n",
    "\n",
    "    # Loop through text elements.\n",
    "    for i in range(n_len - 1):\n",
    "        dummy = dummy + l_input[i] + \", \"\n",
    "\n",
    "    # Append last element.\n",
    "    dummy = dummy + \"and \" + l_input[n_len - 1]\n",
    "\n",
    "    # Return result.\n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def f_describe(df_input, n_top=10):\n",
    "\n",
    "    print(\"First \" + str(n_top) + \" rows in de data:\")\n",
    "    display(df_input.head(n_top))\n",
    "\n",
    "    df_numeric = df_input.select_dtypes(\n",
    "        include=[\n",
    "            \"uint8\",\n",
    "            \"uint16\",\n",
    "            \"uint32\",\n",
    "            \"uint64\",\n",
    "            \"int8\",\n",
    "            \"int16\",\n",
    "            \"int32\",\n",
    "            \"int64\",\n",
    "            \"float16\",\n",
    "            \"float32\",\n",
    "            \"float64\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if len(df_numeric.columns):\n",
    "        print(\"Numerical data:\")\n",
    "        display(df_numeric.describe())\n",
    "\n",
    "    df_textual = df_input.select_dtypes(include=[\"category\", \"object\", \"bool\"])\n",
    "\n",
    "    if len(df_textual.columns):\n",
    "        print(\"Textual data:\")\n",
    "        display(df_textual.describe())\n",
    "\n",
    "    v_na = [\n",
    "        col\n",
    "        + \" (\"\n",
    "        + str(df[col].isna().sum())\n",
    "        + \", \"\n",
    "        + str(round(100 * df[col].isna().sum() / df.shape[0], 1))\n",
    "        + \"%)\"\n",
    "        for col in df.columns\n",
    "        if df[col].isna().sum() > 0\n",
    "    ]\n",
    "\n",
    "    if len(v_na) > 0:\n",
    "        print(\"Features and their number of missing values:\")\n",
    "        display(f_concat(v_na))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downcast and transform data\n",
    "Update formatting of features to optimize memory and standardize column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def standardize_column_names(s):\n",
    "    return s.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "def optimize_memory(df):\n",
    "    # Change: Objects to Categorical.                                               #WHEN needed to transform Objects to Categorical?\n",
    "    # object_cols = df.select_dtypes(include=\"object\").columns\n",
    "    # if not object_cols.empty:\n",
    "    #     print(\"Change: Objects to Categorical\")\n",
    "    #     df[object_cols] = df[object_cols].astype(\"category\")\n",
    "\n",
    "    # Change: Convert integers to smallest unsigned integer and floats to smallest.\n",
    "    for old, new in [(\"integer\", \"unsigned\"), (\"float\", \"float\")]:\n",
    "        print(\"Change: \" + old + \" --> \" + new)\n",
    "        for col in df.select_dtypes(include=old).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=new)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def month_year_to_int(df, i):\n",
    "    # Change: Month and Year to integer\n",
    "    if i == 0:\n",
    "        print(\"Change: Month and Year to integer\")\n",
    "        df = df.astype({\"month\": int, \"year\": int})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform date-related columns to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Convert datasets to time series\n",
    "def transform_date_to_datetime(df, i):\n",
    "    if i == 0:\n",
    "        print(\"Change: Transformed 'year', 'month', 'day' columns to Datetime feature\")\n",
    "        df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]], unit=\"us\")\n",
    "\n",
    "        print(\n",
    "            \"Change: Dropped 'year', 'month', 'day' columns and transformed to Datetime64[us] feature\"\n",
    "        )\n",
    "        df.drop(columns=[\"day\", \"month\", \"year\"], inplace=True)\n",
    "\n",
    "    else:\n",
    "        if \"date\" in df.columns:\n",
    "            print(\"Change: Transformed 'date' column to Datetime Dtype\")\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data from local PATH\n",
    "Import data trough pipeline to downcast the data and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XaDlRA_wRK0U",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def f_get_data(i=0):\n",
    "\n",
    "    # Define path.\n",
    "    c_path = \"/Users/Georgi/Documents/EAISI/EASI_4B_Supermarket/Group4B/data/raw/\"\n",
    "    # Identify file.\n",
    "    v_file = (\n",
    "        \"history-per-year\",  # 0\n",
    "        \"history_aggregated\",  # 1\n",
    "        \"holidays_events\",  # 2\n",
    "        \"items\",  # 3\n",
    "        \"oil\",  # 4\n",
    "        \"stores\",  # 5\n",
    "        \"transactions\",\n",
    "    )  # 6\n",
    "\n",
    "    # Load data.\n",
    "    df = (\n",
    "        pd.read_parquet(c_path + v_file[i] + \".parquet\")\n",
    "        .rename(columns=standardize_column_names)\n",
    "        .pipe(optimize_memory)\n",
    "        .pipe(month_year_to_int, i)\n",
    "        .pipe(transform_date_to_datetime, i)\n",
    "    )\n",
    "\n",
    "    # Return data.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zXWuFInRee5"
   },
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "df = f_get_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "f_describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRjToxz4Rb3h"
   },
   "source": [
    "### Some Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11505,
     "status": "ok",
     "timestamp": 1608136751408,
     "user": {
      "displayName": "pi1234",
      "photoUrl": "",
      "userId": "00485952581093036663"
     },
     "user_tz": -60
    },
    "id": "HV1Ee1SwSefa",
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "outputId": "d77a35c0-17b1-4328-f216-9a9d85c81220"
   },
   "outputs": [],
   "source": [
    "print(\"The data\\n\")\n",
    "print(\n",
    "    f\"-> Contains:                {round(df.shape[0]/1e6, 1)} million observations and {df.shape[1]} features.\\n\"\n",
    ")\n",
    "print(\n",
    "    f\"-> Contains:                {df.shape[0]} observations and {df.shape[1]} features.\\n\"\n",
    ")\n",
    "print(f\"-> Have feature names:      {f_concat(df.columns)}.\\n\")\n",
    "print(f\"-> Has optimized size of    {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = f_get_data(0)\n",
    "# f_describe(df)\n",
    "\n",
    "# df.head()\n",
    "# df.tail(10)\n",
    "df.sample(20)\n",
    "# df.info()\n",
    "# df.describe()\n",
    "# df.nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "df = f_get_data(1)\n",
    "# f_describe(df)\n",
    "df.info()\n",
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "df = f_get_data(2)\n",
    "f_describe(df)\n",
    "df.info()\n",
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "df = f_get_data(3)\n",
    "f_describe(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "df = f_get_data(4)\n",
    "f_describe(df)\n",
    "df.info()\n",
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "df = f_get_data(5)\n",
    "f_describe(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "df = f_get_data(6)\n",
    "f_describe(df)\n",
    "df.info()\n",
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = f_get_data(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt.data_transformers.enable(\"default\")\n",
    "# alt.data_transformers.disable_max_rows()\n",
    "\n",
    "\n",
    "aggregated_df = df.groupby(\"date\").sum().reset_index()\n",
    "\n",
    "print(\n",
    "    f\"-> Contains:                {df.shape[0]} observations and {df.shape[1]} features.\\n\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"-> Contains:                {aggregated_df.shape[0]} observations and {aggregated_df.shape[1]} features.\\n\"\n",
    ")\n",
    "\n",
    "aggregated_df2 = df.groupby([\"date\", \"store_nbr\"]).sum().reset_index()\n",
    "print(\n",
    "    f\"-> Contains:                {aggregated_df2.shape[0]} observations and {aggregated_df2.shape[1]} features.\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "print(1682 / 5)\n",
    "# aggregated_df.head(20)\n",
    "\n",
    "print(df.dtypes)\n",
    "print(aggregated_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the Vegafusion transformer and disable the maximum rows limit\n",
    "vf.enable()\n",
    "# alt.data_transformers.enable(\"vegafusion\")\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "aggregated_df = df.groupby(\"date\").sum().reset_index()\n",
    "\n",
    "chart = (\n",
    "    alt.Chart(aggregated_df)\n",
    "    .mark_circle()\n",
    "    .encode(x=\"date:T\", y=\"transactions:Q\")  # , color=\"store_nbr:N\") #Dtype = uint64\n",
    "    .properties(width=1200, height=480)\n",
    "    .interactive()\n",
    ")\n",
    "\n",
    "chart  # .display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the Vegafusion transformer and disable the maximum rows limit\n",
    "# vf.enable()\n",
    "# alt.data_transformers.enable(\"vegafusion\")\n",
    "# alt.data_transformers.disable_max_rows()\n",
    "\n",
    "# Enable Vegafusion and set a higher row limit\n",
    "vf.enable(row_limit=100000)\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "filtered_df = df[df[\"date\"].dt.year == 2014]\n",
    "\n",
    "df[\"store_nbr\"] = df[\"store_nbr\"].astype(\"category\")\n",
    "\n",
    "chart = (\n",
    "    alt.Chart(filtered_df)\n",
    "    .mark_circle()\n",
    "    .encode(x=\"date:T\", y=\"transactions:Q\", color=\"store_nbr:N\")  # Dtype = uint64\n",
    "    .properties(width=1200, height=480)\n",
    "    .interactive()\n",
    ")\n",
    "\n",
    "\n",
    "chart  # .display()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP1XEh6OeJMgPK4tWBSiOU8",
   "collapsed_sections": [],
   "mount_file_id": "1tas4gpn15avV6RH91pDuPfl4mfnjqven",
   "name": "Copy of 2020 12 13 - EyeOn Supermarket - v1.ipynb",
   "provenance": [
    {
     "file_id": "1tas4gpn15avV6RH91pDuPfl4mfnjqven",
     "timestamp": 1608148229653
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
