{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpLWCBFNuons"
   },
   "source": [
    "### Supermarket data science case study - Test en junk notebook for writing data prep pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Always run step 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPQo8za16cQT"
   },
   "source": [
    "### >> Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1608147217504,
     "user": {
      "displayName": "pi1234",
      "photoUrl": "",
      "userId": "00485952581093036663"
     },
     "user_tz": -60
    },
    "id": "OlwScieYRTmf",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import altair as alt\n",
    "\n",
    "import vegafusion as vf\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm7aDtim6guo",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "### >> Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1608147217505,
     "user": {
      "displayName": "pi1234",
      "photoUrl": "",
      "userId": "00485952581093036663"
     },
     "user_tz": -60
    },
    "id": "f1PDhgg8ZBbf",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def f_concat(l_input):\n",
    "\n",
    "    # Initialize.\n",
    "    dummy = \"\"\n",
    "    n_len = len(l_input)\n",
    "\n",
    "    if n_len == 1:\n",
    "        return l_input[0]\n",
    "\n",
    "    # Loop through text elements.\n",
    "    for i in range(n_len - 1):\n",
    "        dummy = dummy + l_input[i] + \", \"\n",
    "\n",
    "    # Append last element.\n",
    "    dummy = dummy + \"and \" + l_input[n_len - 1]\n",
    "\n",
    "    # Return result.\n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_description(df):\n",
    "    print(\n",
    "        f\"-> Contains:                {round(df.shape[0]/1e6, 4)} million observations and has {df.shape[1]} Feature names: {f_concat(df.columns)}.\\n\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-> Has size of              {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime_features(df):\n",
    "    \"\"\"\n",
    "    Extracting datetime features\n",
    "    year, month, day of month, weekday (1-7), week number-year, week_year_date\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure the date column is sorted\n",
    "    df = df.sort_values(\"date\")\n",
    "\n",
    "    # df[\"year\"] = df[\"date\"].dt.year\n",
    "    # df[\"month\"] = df[\"date\"].dt.month\n",
    "    # df[\"day\"] = df[\"date\"].dt.day\n",
    "\n",
    "    # Adjusting weekday to start from 1 (Monday) to 7 (Sunday)\n",
    "    df[\"weekday\"] = df[\"date\"].dt.dayofweek + 1\n",
    "\n",
    "    # Adding week number feature\n",
    "    df[\"week_number\"] = df[\"date\"].dt.isocalendar().week\n",
    "\n",
    "    # Adding week number-year feature\n",
    "    df[\"week_year\"] = df[\"week_number\"].astype(str).str.zfill(2) + df[\"year\"].astype(\n",
    "        str\n",
    "    )\n",
    "\n",
    "    # Convert week_year to datetime with monday as startdate of week\n",
    "    df[\"week_year_date\"] = pd.to_datetime(\n",
    "        df[\"year\"].astype(str) + df[\"week_number\"].astype(str).str.zfill(2) + \"1\",\n",
    "        format=\"%Y%W%w\",\n",
    "    )\n",
    "\n",
    "    # Adding trend feature: number of weeks since the start of the dataset\n",
    "    start_date = df[\"date\"].min()\n",
    "\n",
    "    df[\"weeks_since_start\"] = ((df[\"date\"] - start_date).dt.days / 7).astype(int) + 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime_features(df):\n",
    "    \"\"\"\n",
    "    Extracting datetime features:\n",
    "    year, month, day of month, weekday (1-7), week number-year, and trend (weeks since start, starting at 1)\n",
    "    \"\"\"\n",
    "    # Ensure the date column is sorted\n",
    "    df = df.copy().sort_values(\"date\")\n",
    "\n",
    "    # Use isocalendar for consistent week-based calculations\n",
    "\n",
    "    iso_calendar = df[\"date\"].dt.isocalendar()\n",
    "\n",
    "    # Year, Month, Day\n",
    "    # df[\"year\"] = iso_calendar.year\n",
    "    # df[\"month\"] = df[\"date\"].dt.month\n",
    "    # df[\"day\"] = df[\"date\"].dt.day\n",
    "\n",
    "    # Weekday (1 = Monday, 7 = Sunday)\n",
    "    df[\"weekday\"] = iso_calendar.day\n",
    "\n",
    "    # Week number\n",
    "    df[\"week_number\"] = iso_calendar.week\n",
    "\n",
    "    # Week-year\n",
    "    df[\"week_year\"] = df[\"week_number\"].astype(str).str.zfill(2) + df[\"year\"].astype(\n",
    "        str\n",
    "    )\n",
    "\n",
    "    # Convert week_year to datetime with monday as startdate of week\n",
    "    df[\"week_year_date\"] = pd.to_datetime(\n",
    "        df[\"year\"].astype(str) + df[\"week_number\"].astype(str).str.zfill(2) + \"1\",\n",
    "        format=\"%Y%W%w\",\n",
    "    )\n",
    "\n",
    "    # First day of the ISO year containing the start date\n",
    "\n",
    "    start_date = df[\"date\"].min()\n",
    "    start_year_first_day = datetime(start_date.year, 1, 1)\n",
    "\n",
    "    # 'search' for first monday of year\n",
    "    while start_year_first_day.isocalendar()[1] != 1:\n",
    "\n",
    "        start_year_first_day = start_year_first_day + pd.Timedelta(days=1)\n",
    "\n",
    "    ##Itemweek number\n",
    "    # Weeks since start (aligned with ISO week numbers)\n",
    "    df[\"weeks_since_start\"] = (\n",
    "        iso_calendar.week + (iso_calendar.year - start_year_first_day.year) * 52\n",
    "    )\n",
    "\n",
    "    # Adjust weeks_since_start to start from 1\n",
    "    df[\"weeks_since_start\"] = (\n",
    "        df[\"weeks_since_start\"] - df[\"weeks_since_start\"].min() + 1\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> Downcast and transform data\n",
    "Update formatting of features to optimize memory and standardize column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_column_names(s):\n",
    "    return s.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "def optimize_memory(df):\n",
    "    # Change: Objects to Categorical.\n",
    "    object_cols = df.select_dtypes(include=\"object\").columns\n",
    "\n",
    "    if not object_cols.empty:\n",
    "        print(\"Change: Objects to Categorical\")\n",
    "        df[object_cols] = df[object_cols].astype(\"category\")\n",
    "\n",
    "    # Change: Convert integers to smallest unsigned integer and floats to smallest.\n",
    "    for old, new in [(\"integer\", \"unsigned\"), (\"float\", \"float\")]:\n",
    "        print(\"Change: \" + old + \" --> \" + new)\n",
    "        for col in df.select_dtypes(include=old).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=new)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def month_year_to_int(df, i):\n",
    "    # Change: Month and Year to integer.\n",
    "    if i == 0:\n",
    "        print(\"Change: Month and Year to integer\")\n",
    "        df = df.astype({\"month\": int, \"year\": int})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> Transform date-related columns to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert datasets to time series\n",
    "# def transform_date_to_datetime(df, i):\n",
    "#     if i == 0:\n",
    "#         print(\"Change: Transformed 'year', 'month', 'day' columns to Datetime feature\")\n",
    "#         df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]], unit=\"us\")\n",
    "\n",
    "#         print(\n",
    "#             \"Change: Dropped 'id', 'month', 'day' columns and transformed to Datetime64[us] feature\"\n",
    "#         )\n",
    "#         df.drop(columns=[\"day\", \"month\", \"id\"], inplace=True)\n",
    "\n",
    "#     else:\n",
    "#         if \"date\" in df.columns:\n",
    "#             print(\"Change: Transformed 'date' column to Datetime Dtype\")\n",
    "#             df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Convert datasets to time series\n",
    "def transform_date_to_datetime(df, i):\n",
    "    if i == 0:\n",
    "        print(\"Change: Transformed 'year', 'month', 'day' columns to Datetime feature\")\n",
    "        # Convert year, month, and day to a single datetime column\n",
    "        df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]]).dt.floor(\"D\")\n",
    "\n",
    "        print(\n",
    "            \"Change: Dropped 'id', 'month', 'day' columns and transformed to Datetime feature\"\n",
    "        )\n",
    "        df.drop(columns=[\"day\", \"month\", \"id\"], inplace=True)\n",
    "\n",
    "    else:\n",
    "        if \"date\" in df.columns:\n",
    "            print(\"Change: Transformed 'date' column to Datetime Dtype\")\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None).dt.floor(\"D\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> Import data from local PATH\n",
    "Import data trough pipeline to downcast the data and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_data(i=0):\n",
    "\n",
    "    # Define path.\n",
    "    c_path = \"C:/Users/alexander/Documents/0. Data Science and AI for Experts/EAISI_4B_Supermarket/data/raw/\"\n",
    "\n",
    "    # Identify file.\n",
    "    v_file = (\n",
    "        \"history-per-year\",  # 0\n",
    "        \"history_aggregated\",  # 1\n",
    "        \"holidays_events\",  # 2\n",
    "        \"items\",  # 3\n",
    "        \"oil\",  # 4\n",
    "        \"stores\",  # 5\n",
    "        \"transactions\",  # 6\n",
    "    )\n",
    "\n",
    "    # Load data.\n",
    "    df = (\n",
    "        pd.read_parquet(c_path + v_file[i] + \".parquet\")\n",
    "        .rename(columns=standardize_column_names)\n",
    "        .pipe(optimize_memory)\n",
    "        .pipe(month_year_to_int, i)\n",
    "        .pipe(transform_date_to_datetime, i)\n",
    "    )\n",
    "\n",
    "    # Return data.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change: integer --> unsigned\n",
      "Change: float --> float\n",
      "Change: Month and Year to integer\n",
      "Change: Transformed 'year', 'month', 'day' columns to Datetime feature\n",
      "Change: Dropped 'id', 'month', 'day' columns and transformed to Datetime feature\n"
     ]
    }
   ],
   "source": [
    "# Sales History per year\n",
    "df_sales = f_get_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change: Objects to Categorical\n",
      "Change: integer --> unsigned\n",
      "Change: float --> float\n",
      "Change: Transformed 'date' column to Datetime Dtype\n"
     ]
    }
   ],
   "source": [
    "# Holidays\n",
    "df_holidays = f_get_data(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change: Objects to Categorical\n",
      "Change: integer --> unsigned\n",
      "Change: float --> float\n"
     ]
    }
   ],
   "source": [
    "# Items\n",
    "df_items = f_get_data(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change: Objects to Categorical\n",
      "Change: integer --> unsigned\n",
      "Change: float --> float\n"
     ]
    }
   ],
   "source": [
    "# Stores\n",
    "df_stores = f_get_data(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imputing sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = f_get_data(0)\n",
    "\n",
    "# df_0 = df_0[df_0[\"year\"].isin([2014, 2015])].drop(columns=['id', 'day','month' ])\n",
    "df_sales = df_sales.drop(columns=[\"id\", \"day\", \"month\"])\n",
    "print(\"-\" * 50)\n",
    "df_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check how big df wil get after NaN inputing per year\n",
    "df_sales_2013 = df_sales[(df_sales[\"year\"] == 2013)]\n",
    "df_sales_2014 = df_sales[(df_sales[\"year\"] == 2014)]\n",
    "df_sales_2015 = df_sales[(df_sales[\"year\"] == 2015)]\n",
    "df_sales_2016 = df_sales[(df_sales[\"year\"] == 2016)]\n",
    "df_sales_2017 = df_sales[(df_sales[\"year\"] == 2017)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_description(df_sales_2017)  # --> 23.8 million observations and 0.69 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_description(df_sales)  # orginal = 2.7GB and after NaN inputing ~7GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_36_648313 = df_0[\n",
    "    (df_0[\"store_nbr\"] == 53)\n",
    "    & (\n",
    "        df_0[\"item_nbr\"].isin(\n",
    "            [\n",
    "                627887,\n",
    "                759890,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "].drop(\n",
    "    columns=[\n",
    "        \"weekday\",\n",
    "        \"week_number\",\n",
    "        \"week_year\",\n",
    "        \"weeks_since_start\",\n",
    "        \"weeks_since_start_2\",\n",
    "    ]\n",
    ")\n",
    "# 53 759890 --> starts 2014-05-29, next date 2024-05-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_36_759890 = df_0[(df_0[\"store_nbr\"] == 53) & (df_0[\"item_nbr\"] == 759890)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_36_3items_3stores = df_0[\n",
    "    (df_0[\"store_nbr\"].isin([53, 6]))\n",
    "    & (df_0[\"item_nbr\"].isin([627887, 759890, 1160872]))\n",
    "].drop(columns=[\"id\"])\n",
    "\n",
    "df_final_check2 = df_sales_nan[(df_sales_nan[\"date\"] > \"2015-31-12\")].sort_values(\n",
    "    by=[\"date\", \"store_nbr\", \"item_nbr\"]\n",
    ")\n",
    "\n",
    "df_final_check2.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_36_3items_3stores.tail(40).sort_values(by=[\"date\", \"store_nbr\", \"item_nbr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_36_3items_3stores = filling_dates_NaN(df_0_36_3items_3stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_check = df_0_36_3items_3stores[\n",
    "    (df_0_36_3items_3stores[\"date\"] > \"2015-09-12\")\n",
    "].sort_values(by=[\"date\", \"store_nbr\", \"item_nbr\"])\n",
    "\n",
    "df_final_check.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_36_759890.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_36_648313.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filling_dates_NaN(df):\n",
    "\n",
    "    # Create new df to include all daily dates in the range, filling missing dates with NaNs\n",
    "    df = df.copy()\n",
    "\n",
    "    # Print first and last date of df\n",
    "    print(f'First date in df: {df[\"date\"].min()}')\n",
    "    print(f'Last date in df:  {df[\"date\"].max()}')\n",
    "    print(\"-\" * 71)\n",
    "\n",
    "    # Calculate memory size and shape size of start df\n",
    "    df_mem_start = sys.getsizeof(df)\n",
    "    df_shape_start = df.shape[0] / 1e6\n",
    "    print(\n",
    "        f\"Start size of df:     {round(df_mem_start/1024/1024/1024, 2)} GB and start observations:     {round(df_shape_start, 1)} million.\"\n",
    "    )\n",
    "\n",
    "    # Create a complete date range for the entire dataset\n",
    "    all_dates = pd.date_range(start=df[\"date\"].min(), end=df[\"date\"].max(), freq=\"D\")\n",
    "\n",
    "    # Create a multi-index from all possible combinations of 'item_nbr' and 'date'\n",
    "    all_combinations = pd.MultiIndex.from_product(\n",
    "        [df[\"store_nbr\"].unique(), df[\"item_nbr\"].unique(), all_dates],\n",
    "        names=[\"store_nbr\", \"item_nbr\", \"date\"],\n",
    "    )\n",
    "\n",
    "    # Reindex the original DataFrame to include all combinations of 'store_nbr', 'item_nbr', and 'date'\n",
    "    df_reindexed = df.set_index([\"store_nbr\", \"item_nbr\", \"date\"]).reindex(\n",
    "        all_combinations\n",
    "    )\n",
    "\n",
    "    # Reset the index to turn the multi-index back into regular columns\n",
    "    df_final = df_reindexed.reset_index()\n",
    "\n",
    "    # Calculate memory size and shape size of final end df\n",
    "    df_mem_end = sys.getsizeof(df_final)\n",
    "    df_mem_change_perc = ((df_mem_end - df_mem_start) / df_mem_start) * 100\n",
    "    df_mem_change = df_mem_end - df_mem_start\n",
    "\n",
    "    df_shape_end = df_final.shape[0] / 1e6\n",
    "    df_shape_change_perc = ((df_shape_end - df_shape_start) / df_shape_start) * 100\n",
    "    df_shape_change = df_shape_end - df_shape_start\n",
    "\n",
    "    print(\n",
    "        f\"Final size of df:     {round(df_mem_end/1024/1024/1024, 2)} GB and end observations:       {round(df_shape_end, 1)} million.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Change in size of df: {round(df_mem_change_perc, 2)} % and observations:           {round(df_shape_change_perc, 2)}     %.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Increased size of df: {round(df_mem_change/1024/1024/1024, 2)} GB and increased observations: {round(df_shape_change, 1)} million.\"\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 71)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--> df_sales_2013 Contains:\")\n",
    "df_sales_2013_nan = filling_dates_NaN(df_sales_2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--> df_sales_2014 Contains:\")\n",
    "df_sales_2014_nan = filling_dates_NaN(df_sales_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--> df_sales_2015 Contains:\")\n",
    "df_sales_2015_nan = filling_dates_NaN(df_sales_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--> df_sales_2016 Contains:\")\n",
    "df_sales_2016_nan = filling_dates_NaN(df_sales_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--> df_sales_2017 Contains:\")\n",
    "df_sales_2017_nan = filling_dates_NaN(df_sales_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_check = df_sales_nan[(df_sales_nan[\"date\"] > \"2015-30-12\")].sort_values(\n",
    "    by=[\"date\", \"store_nbr\", \"item_nbr\"]\n",
    ")\n",
    "\n",
    "df_final_check.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_check2 = df_sales_nan[(df_sales_nan[\"date\"] > \"2015-31-12\")].sort_values(\n",
    "    by=[\"date\", \"store_nbr\", \"item_nbr\"]\n",
    ")\n",
    "\n",
    "df_final_check2.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_nan.head(30).sort_values(by=[\"date\", \"store_nbr\", \"item_nbr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_nan.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_nan.sample(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "\n",
    "df = df_0_36_648313.copy()\n",
    "\n",
    "# Create a complete date range for the entire dataset\n",
    "all_dates = pd.date_range(start=\"2014-01-01\", end=\"2016-01-18\", freq=\"D\")\n",
    "\n",
    "# Create a multi-index from all possible combinations of 'store_nbr', 'item_nbr', and 'date'\n",
    "all_combinations = pd.MultiIndex.from_product(\n",
    "    [df[\"store_nbr\"].unique(), df[\"item_nbr\"].unique(), all_dates],\n",
    "    names=[\"store_nbr\", \"item_nbr\", \"date\"],\n",
    ")\n",
    "\n",
    "# Reindex the original DataFrame to include all combinations of 'store_nbr', 'item_nbr', and 'date'\n",
    "df_reindexed = df.set_index([\"store_nbr\", \"item_nbr\", \"date\"]).reindex(all_combinations)\n",
    "\n",
    "# Reset the index to turn the multi-index back into regular columns\n",
    "df_final = df_reindexed.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Fill missing values for non-sales columns using forward fill and backward fill\n",
    "non_sales_columns = [\n",
    "    \"onpromotion\",\n",
    "    \"day\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"item_family\",\n",
    "    \"item_class\",\n",
    "    \"store_cluster\",\n",
    "]\n",
    "df[non_sales_columns] = df.groupby([\"store_nbr\", \"item_nbr\"])[non_sales_columns].apply(\n",
    "    lambda group: group.ffill().bfill()\n",
    ")\n",
    "\n",
    "# Interpolate missing values for the 'unit_sales' column\n",
    "df[\"unit_sales\"] = df.groupby([\"store_nbr\", \"item_nbr\"])[\"unit_sales\"].apply(\n",
    "    lambda group: group.interpolate(method=\"linear\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(df, column_name):\n",
    "    \"\"\"Get the all values and the count for specific column\"\"\"\n",
    "    unique_values_count = df[column_name].nunique()\n",
    "    unique_values = df[column_name].unique()\n",
    "\n",
    "    # Convert unique values to a single string to print\n",
    "    unique_values_str = \", \".join(map(str, unique_values))\n",
    "\n",
    "    print(f\"Number of unique values in {column_name}: {unique_values_count}\")\n",
    "    print(\"Unique values:\")\n",
    "    print(unique_values_str)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect negative values\n",
    "\n",
    "•\tAction: Delete unit_sales if values are lower than zero --> N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = f_get_data(0)\n",
    "df_sales = df_sales.drop(columns=[\"id\", \"day\", \"month\"])\n",
    "print(\"-\" * 80)\n",
    "df_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_negative = df_sales[df_sales[\"unit_sales\"] < 0].sort_values(\n",
    "    by=[\"date\", \"store_nbr\", \"item_nbr\"]\n",
    ")\n",
    "df_sales_negative.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some CLAUDE.ai generated plots, to get a bit of an idea how the negative values look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def visualize_negative_sales(df_sales_negative):\n",
    "\n",
    "    # Check if the DataFrame is empty\n",
    "    if df_sales_negative.empty:\n",
    "        print(\"No negative sales data to visualize.\")\n",
    "        return\n",
    "\n",
    "    # Create a line plot of negative sales over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(x=\"date\", y=\"unit_sales\", data=df_sales_negative)\n",
    "    plt.title(\"Negative Sales Over Time\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Unit Sales\")\n",
    "    plt.show()\n",
    "\n",
    "    # # Create a bar plot of negative sales by store and item\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # sns.barplot(x=\"store_nbr\", y=\"unit_sales\", hue=\"item_nbr\", data=df_sales_negative)\n",
    "    # plt.title(\"Negative Sales by Store and Item\")\n",
    "    # plt.xlabel(\"Store Number\")\n",
    "    # plt.ylabel(\"Unit Sales\")\n",
    "    # plt.xticks(rotation=90)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_negative = df_sales[df_sales[\"unit_sales\"] < 0].sort_values(\n",
    "    by=[\"date\", \"store_nbr\", \"item_nbr\"]\n",
    ")\n",
    "visualize_negative_sales(df_sales_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=df_sales_negative, x=\"unit_sales\", bins=200)\n",
    "plt.title(\"Distribution of Negative Sales Amounts\")\n",
    "plt.xlabel(\"Unit Sales\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_sales_negative, x=\"store_nbr\", y=\"unit_sales\")\n",
    "plt.title(\"Negative Sales by Store Number\")\n",
    "plt.xlabel(\"Store Number\")\n",
    "plt.ylabel(\"Unit Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 20th percentile of the negative sales values\n",
    "negative_sales_threshold = df_sales_negative[\"unit_sales\"].quantile(0.001)\n",
    "\n",
    "# Filter the 20% most negative sales\n",
    "df_sales_negative_filtered = df_sales_negative[\n",
    "    df_sales_negative[\"unit_sales\"] <= negative_sales_threshold\n",
    "]\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df_sales_negative_filtered, x=\"item_nbr\", y=\"unit_sales\")\n",
    "plt.title(\"Negative Sales by Item Number (Top 20% Most Negative)\")\n",
    "plt.xlabel(\"Item Number\")\n",
    "plt.ylabel(\"Unit Sales\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "pivot_table = df_sales_negative.pivot_table(\n",
    "    index=\"month\", columns=\"store_nbr\", values=\"unit_sales\"\n",
    ")\n",
    "sns.heatmap(pivot_table, annot=True, cmap=\"RdBu_r\")\n",
    "plt.title(\"Negative Sales by Month and Store Number\")\n",
    "plt.xlabel(\"Store Number\")\n",
    "plt.ylabel(\"Month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing and research negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_negative = df_sales[df_sales[\"unit_sales\"] < 0].sort_values(\n",
    "    by=[\"date\", \"store_nbr\", \"item_nbr\"]\n",
    ")\n",
    "\n",
    "# --> 7795 records with negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = f_get_data(0)\n",
    "df_sales = df_sales.drop(columns=[\"id\", \"day\", \"month\"])\n",
    "df_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of negative values before replacement\n",
    "before_replacement = (df_sales[\"unit_sales\"] < 0).sum()\n",
    "print(f\"Number of negative values before replacement: {before_replacement}\")\n",
    "\n",
    "# df_sales = df_sales[df_sales[\"unit_sales\"] < 0]\n",
    "\n",
    "# Detect and replace negative values with NAs --> .transform() --> should more efficient for larger df's then apply() --> takes 20 min to run for full df_sales and then crashes....\n",
    "df_sales[\"unit_sales\"] = df_sales.groupby([\"date\", \"store_nbr\", \"item_nbr\"])[\n",
    "    \"unit_sales\"\n",
    "].transform(lambda x: x.where(x >= 0, np.nan))\n",
    "\n",
    "# Check the number of negative values after replacement\n",
    "after_replacement = (df_sales[\"unit_sales\"] < 0).sum()\n",
    "\n",
    "print(f\"Number of negative values after replacement: {after_replacement}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of negative values before replacement\n",
    "before_replacement = (df_sales[\"unit_sales\"] < 0).sum()\n",
    "print(f\"Number of negative values before replacement: {before_replacement}\")\n",
    "\n",
    "# Create a boolean mask for the negative sales rows to create df containing all negative rows, used to filter full df_sales df\n",
    "negative_sales_mask = df_sales[\"unit_sales\"] < 0\n",
    "\n",
    "# Use the mask to update the 'unit_sales' column in the original DataFrame\n",
    "df_sales.loc[negative_sales_mask, \"unit_sales\"] = df_sales.loc[\n",
    "    negative_sales_mask, \"unit_sales\"\n",
    "].where(df_sales.loc[negative_sales_mask, \"unit_sales\"] >= 0, np.nan)\n",
    "\n",
    "# Check the number of negative values after replacement\n",
    "after_replacement = (df_sales[\"unit_sales\"] < 0).sum()\n",
    "print(f\"Number of negative values after replacement: {after_replacement}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sales_cleaned(df):\n",
    "\n",
    "    # Check the number of negative values before replacement\n",
    "    before_replacement = (df[\"unit_sales\"] < 0).sum()\n",
    "    print(f\"Number of negative values before replacement: {before_replacement}\")\n",
    "\n",
    "    # Create a boolean mask for the negative sales rows to create a 'boolean flag-list' containing all negative rows, used to filter full df_sales df\n",
    "    negative_sales_mask = df[\"unit_sales\"] < 0\n",
    "\n",
    "    # Use the mask to update the flagged 'unit_sales' column in the original DataFrame\n",
    "    df.loc[negative_sales_mask, \"unit_sales\"] = df.loc[\n",
    "        negative_sales_mask, \"unit_sales\"\n",
    "    ].where(df.loc[negative_sales_mask, \"unit_sales\"] >= 0, np.nan)\n",
    "\n",
    "    # Check the number of negative values after replacement\n",
    "    after_replacement = (df[\"unit_sales\"] < 0).sum()\n",
    "    print(f\"Number of negative values after replacement: {after_replacement}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sales_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_nan = negative_sales_cleaned(df_sales)\n",
    "\n",
    "df_sales_nan.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NaN data inputing testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_nan_check = df_sales_nan[df_sales_nan[\"unit_sales\"].isna()].sort_values(\n",
    "    by=[\"date\", \"store_nbr\", \"item_nbr\"]\n",
    ")\n",
    "\n",
    "df_sales_nan_check.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_nan.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_nan.sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fill missing values for non-sales columns using forward fill and backward fill --> items, stores, holidays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Items\n",
    "df_items = f_get_data(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_2013 = df_sales[(df_sales[\"year\"] == 2013)]\n",
    "df_sales_2014 = df_sales[(df_sales[\"year\"] == 2014)]\n",
    "df_sales_2015 = df_sales[(df_sales[\"year\"] == 2015)]\n",
    "df_sales_2016 = df_sales[(df_sales[\"year\"] == 2016)]\n",
    "df_sales_2017 = df_sales[(df_sales[\"year\"] == 2017)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--> df_sales_2013 Contains:\")\n",
    "\n",
    "print(df_sales_2013[\"item_nbr\"].dtype)\n",
    "print(df_items[\"item_nbr\"].dtype)\n",
    "\n",
    "# # Change the dtype for item_nbr from uint32 to int32\n",
    "df_sales_2013[\"item_nbr\"] = df_sales_2013[\"item_nbr\"].astype(int)\n",
    "df_items[\"item_nbr\"] = df_items[\"item_nbr\"].astype(int)\n",
    "print(\"-\" * 30)\n",
    "print(df_sales_2013[\"item_nbr\"].dtype)\n",
    "print(df_items[\"item_nbr\"].dtype)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "df_sales_2013_stores = df_sales_2013.merge(df_stores, on=\"store_nbr\", how=\"left\")\n",
    "\n",
    "df_sales_2013_stores_items = df_sales_2013_stores.merge(\n",
    "    df_items, on=\"item_nbr\", how=\"left\"\n",
    ")\n",
    "\n",
    "df_sales_2013_nan = filling_dates_NaN(df_sales_2013_stores_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--> df_sales_2017 Contains:\")\n",
    "df_sales_2017_nan = filling_dates_NaN(df_sales_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Check Results of NaN inputing : \n",
    "# TO-DO: Check grouppy item-nbr, store-nbr or both #    [\"item_nbr\", \"store_nbr\"]\n",
    "\n",
    "to-do check same item for different stores\n",
    "\n",
    "\n",
    "to-do: check same store for different items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_2013_nan.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df sales for year 2013\n",
    "\n",
    "forward and backward fill based on --> groupby()= item_nbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_2013_nan_item_nbr = df_sales_2013_nan.copy()\n",
    "\n",
    "# Fill missing values for non-sales columns using forward fill and backward fill --> items, stores, holidays\n",
    "\n",
    "\n",
    "non_sales_columns = [\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"type\",\n",
    "    \"cluster\",\n",
    "    \"family\",\n",
    "    \"class\",\n",
    "    \"perishable\",\n",
    "]\n",
    "\n",
    "# Filling missing values based on available values for the same !!item_nbr!!\n",
    "\n",
    "df_sales_2013_nan_item_nbr[non_sales_columns] = df_sales_2013_nan_item_nbr.groupby(\n",
    "    [\"item_nbr\"]\n",
    ")[non_sales_columns].transform(lambda group: group.ffill().bfill())\n",
    "\n",
    "df_sales_2013_nan_item_nbr.head(20)\n",
    "\n",
    "# TO-DO: Check grouppy item-nbr, store-nbr or both #    [\"item_nbr\", \"store_nbr\"]\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_2013_nan_item_nbr_627887 = df_sales_2013_nan_item_nbr[\n",
    "    # (df_0[\"store_nbr\"].isin([53, 6])) &\n",
    "    (\n",
    "        df_sales_2013_nan_item_nbr[\"item_nbr\"].isin(\n",
    "            [\n",
    "                627887\n",
    "                # , 759890, 1160872\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "].sort_values(by=[\"date\", \"store_nbr\"])\n",
    "\n",
    "df_sales_2013_nan_item_nbr_627887.tail(\n",
    "    60\n",
    ")  # --> item_nbr works well for 1 items, across all stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df sales for year 2013\n",
    "\n",
    "forward and backward fill based on --> groupby()= store_nbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_2013_nan_store_nbr = df_sales_2013_nan.copy()\n",
    "\n",
    "# Fill missing values for non-sales columns using forward fill and backward fill --> items, stores, holidays\n",
    "non_sales_columns = [\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"type\",\n",
    "    \"cluster\",\n",
    "    \"family\",\n",
    "    \"class\",\n",
    "    \"perishable\",\n",
    "]\n",
    "\n",
    "# Filling missing values based on available values for the same !!store_nbr!!\n",
    "df_sales_2013_nan_store_nbr[non_sales_columns] = df_sales_2013_nan_store_nbr.groupby(\n",
    "    [\"store_nbr\"]\n",
    ")[non_sales_columns].transform(lambda group: group.ffill().bfill())\n",
    "\n",
    "df_sales_2013_nan_store_nbr.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df sales for year 2013\n",
    "\n",
    "forward and backward fill based on --> groupby()= item_nbr AND! store_nbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_2013_nan_item_store_nbr = df_sales_2013_nan.copy()\n",
    "\n",
    "# Fill missing values for non-sales columns using forward fill and backward fill --> items, stores, holidays\n",
    "non_sales_columns = [\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"type\",\n",
    "    \"cluster\",\n",
    "    \"family\",\n",
    "    \"class\",\n",
    "    \"perishable\",\n",
    "]\n",
    "\n",
    "# TO-DO: Check grouppy item-nbr, store-nbr or both #    [\"item_nbr\", \"store_nbr\"]\n",
    "\n",
    "# Filling missing values based on available values for the same !!store_nbr, item_nbr!!\n",
    "df_sales_2013_nan_item_store_nbr[non_sales_columns] = (\n",
    "    df_sales_2013_nan_item_store_nbr.groupby([\"item_nbr\", \"store_nbr\"])[\n",
    "        non_sales_columns\n",
    "    ].transform(lambda group: group.ffill().bfill())\n",
    ")\n",
    "\n",
    "df_sales_2013_nan_item_store_nbr.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Define new, old and closed stores for 4.3\n",
    "\n",
    "•\tCondition: sales for all items a given store and date are NA\n",
    "\n",
    "•\tAction: Impute with 0\n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    "\n",
    "Sum/Agg all sales group  by store, date\n",
    "\n",
    " --> Sum_sales > 0 then store_opened\n",
    "\n",
    " --> first date store_opened --> before keep N/A\n",
    " \n",
    " else --> closed --> inputed with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filling_dates_NaN(df):\n",
    "\n",
    "    # Create new df to include all daily dates in the range, filling missing dates with NaNs\n",
    "    df = df.copy()\n",
    "\n",
    "    # Print first and last date of df\n",
    "    print(f'First date in df: {df[\"date\"].min()}')\n",
    "    print(f'Last date in df:  {df[\"date\"].max()}')\n",
    "    print(\"-\" * 71)\n",
    "\n",
    "    # Calculate memory size and shape size of start df\n",
    "    df_mem_start = sys.getsizeof(df)\n",
    "    df_shape_start = df.shape[0] / 1e6\n",
    "    print(\n",
    "        f\"Start size of df:     {round(df_mem_start/1024/1024/1024, 2)} GB and start observations:     {round(df_shape_start, 1)} million.\"\n",
    "    )\n",
    "\n",
    "    # Create a complete date range for the entire dataset\n",
    "    all_dates = pd.date_range(start=df[\"date\"].min(), end=df[\"date\"].max(), freq=\"D\")\n",
    "\n",
    "    # Create a multi-index from all possible combinations of 'item_nbr' and 'date'\n",
    "    all_combinations = pd.MultiIndex.from_product(\n",
    "        [df[\"store_nbr\"].unique(), df[\"item_nbr\"].unique(), all_dates],\n",
    "        names=[\"store_nbr\", \"item_nbr\", \"date\"],\n",
    "    )\n",
    "\n",
    "    # Reindex the original DataFrame to include all combinations of 'store_nbr', 'item_nbr', and 'date'\n",
    "    df_reindexed = df.set_index([\"store_nbr\", \"item_nbr\", \"date\"]).reindex(\n",
    "        all_combinations\n",
    "    )\n",
    "\n",
    "    # Reset the index to turn the multi-index back into regular columns\n",
    "    df_final = df_reindexed.reset_index()\n",
    "\n",
    "    # Calculate memory size and shape size of final end df\n",
    "    df_mem_end = sys.getsizeof(df_final)\n",
    "    df_mem_change_perc = ((df_mem_end - df_mem_start) / df_mem_start) * 100\n",
    "    df_mem_change = df_mem_end - df_mem_start\n",
    "\n",
    "    df_shape_end = df_final.shape[0] / 1e6\n",
    "    df_shape_change_perc = ((df_shape_end - df_shape_start) / df_shape_start) * 100\n",
    "    df_shape_change = df_shape_end - df_shape_start\n",
    "\n",
    "    print(\n",
    "        f\"Final size of df:     {round(df_mem_end/1024/1024/1024, 2)} GB and end observations:       {round(df_shape_end, 1)} million.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Change in size of df: {round(df_mem_change_perc, 2)} % and observations:           {round(df_shape_change_perc, 2)}     %.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Increased size of df: {round(df_mem_change/1024/1024/1024, 2)} GB and increased observations: {round(df_shape_change, 1)} million.\"\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 71)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date in df: 2013-01-01 00:00:00\n",
      "Last date in df:  2017-08-15 00:00:00\n",
      "-----------------------------------------------------------------------\n",
      "Start size of df:     1.99 GB and start observations:     125.5 million.\n",
      "Final size of df:     5.82 GB and end observations:       367.9 million.\n",
      "Change in size of df: 193.15 % and observations:           193.15     %.\n",
      "Increased size of df: 3.84 GB and increased observations: 242.4 million.\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_sales = df_sales.drop(columns=[\"onpromotion\", \"year\"])\n",
    "\n",
    "df_sales = filling_dates_NaN(df_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sales = df_sales[(df_sales[\"store_nbr\"] == 1)]\n",
    "\n",
    "df_sales = df_sales[df_sales[\"store_nbr\"].isin([1, 2, 3, 4, 5, 6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = df_sales[(df_sales[\"date\"] < \"2015-07-01\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 testing merge_store_status function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea: use the same function/logic to label a item_nbr as new product for step 4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: discuss do we need to inpute all stores on 01-01-2013 with 0, as almost all the stores are  (only 25=open) closed by result of NYE/1jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_store_status(df):\n",
    "\n",
    "    # Label Variable for atributing numbers to store status, to save memory in df\n",
    "    OPEN = 0\n",
    "    NEW = 2\n",
    "    CLOSED = 4\n",
    "    OLD = 6\n",
    "    NEVER_OPENED = 8\n",
    "\n",
    "    # Group by store and date, then sum sales\n",
    "    df_grouped = (\n",
    "        df.groupby([\"store_nbr\", \"date\"])[\"unit_sales\"]\n",
    "        .sum()  # --> sum or agg ?\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Sort by store and date\n",
    "    df_grouped = df_grouped.sort_values([\"store_nbr\", \"date\"])\n",
    "\n",
    "    # Create a new column for store status, label al stores as 'open' by default\n",
    "    df_grouped[\"store_status\"] = OPEN\n",
    "\n",
    "    # Find the first and last day with sales for each store\n",
    "    first_sale_date = (\n",
    "        df_grouped[df_grouped[\"unit_sales\"] > 0].groupby(\"store_nbr\")[\"date\"].min()\n",
    "    )\n",
    "\n",
    "    last_sale_date = (\n",
    "        df_grouped[df_grouped[\"unit_sales\"] > 0].groupby(\"store_nbr\")[\"date\"].max()\n",
    "    )\n",
    "\n",
    "    # Loop trhough stores by lapeling them as 'NEW', 'CLOSED', 'OLD' or 'NEVER_OPENED' based on first sale date and last sale date\n",
    "    for store in df_grouped[\"store_nbr\"].unique():\n",
    "        store_data = df_grouped[df_grouped[\"store_nbr\"] == store]\n",
    "\n",
    "        if store in first_sale_date.index:\n",
    "            first_date = first_sale_date[store]\n",
    "            last_date = last_sale_date[store]\n",
    "\n",
    "            # Mark as 'NEW' before first sale date\n",
    "            df_grouped.loc[\n",
    "                (df_grouped[\"store_nbr\"] == store) & (df_grouped[\"date\"] < first_date),\n",
    "                \"store_status\",\n",
    "            ] = NEW\n",
    "            # --> To-do: Do we call this  not opened' or a 'new store'?\n",
    "\n",
    "            # Mark as 'closed' after first sale date if sales are 0\n",
    "            df_grouped.loc[\n",
    "                (df_grouped[\"store_nbr\"] == store)\n",
    "                & (df_grouped[\"date\"] > first_date)\n",
    "                & (df_grouped[\"unit_sales\"] == 0),\n",
    "                \"store_status\",\n",
    "            ] = CLOSED\n",
    "\n",
    "            # Mark as 'OLD' after last sale date\n",
    "            df_grouped.loc[\n",
    "                (df_grouped[\"store_nbr\"] == store) & (df_grouped[\"date\"] > last_date),\n",
    "                \"store_status\",\n",
    "            ] = OLD\n",
    "\n",
    "        else:\n",
    "            # If a store never had any sales, mark all dates as 'NEVER_OPENED' --> no records?\n",
    "            df_grouped.loc[df_grouped[\"store_nbr\"] == store, \"store_status\"] = (\n",
    "                NEVER_OPENED\n",
    "            )\n",
    "\n",
    "    # Change the data type of store_status column to int8\n",
    "    df_grouped[\"store_status\"] = df_grouped[\"store_status\"].astype(\"int8\")\n",
    "\n",
    "    # Merging store_status on df_sales\n",
    "    df = df.merge(\n",
    "        df_grouped[[\"store_nbr\", \"date\", \"store_status\"]],\n",
    "        left_on=[\"store_nbr\", \"date\"],\n",
    "        right_on=[\"store_nbr\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Using a mask to flag al 'CLOSED'=4 stores and impute 'closed' stores with 0, not opened stores stay NA/NaN\n",
    "    mask = df[\"store_status\"] == 4\n",
    "    df.loc[mask, \"unit_sales\"] = 0\n",
    "\n",
    "    print(\"-\" * 72)\n",
    "    print(\n",
    "        f\"Size of df:     {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB and end observations:       {round(df.shape[0] / 1e6, 1)} million.\"\n",
    "    )\n",
    "    print(\"- \" * 36)\n",
    "    print(\"df_grouped store_status value counts:\")\n",
    "    print(df_grouped[\"store_status\"].value_counts())\n",
    "\n",
    "    print(\"-\" * 72)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: discuss do we need to inpute all stores on 01-01-2013 with 0, as almost all the stores are  (only 25=open) closed by result of NYE/1jan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: think if it also a possibility to label the closed stores on 25-12 and 01-01 as Closed_All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_grouped.sort_values(by=[\"date\", \"store_nbr\"]).head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_grouped_check = df_sales_grouped[\n",
    "    (df_sales_grouped[\"store_status\"] == \"closed\")\n",
    "].sort_values(by=[\"date\", \"store_nbr\"])\n",
    "\n",
    "\n",
    "df_sales_grouped_check.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TO-DO: think about if it is needed to drop the 'store_status'-column after this inpute? or keep it as a feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_check = df_sales[(df_sales[\"store_status\"] == \"closed\")].sort_values(\n",
    "    by=[\"date\", \"store_nbr\"]\n",
    ")\n",
    "\n",
    "df_sales_check.sample(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "junk code to select df based on store or specific item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_2013_nan_item_nbr = df_0[\n",
    "    (df_0[\"store_nbr\"].isin([53, 6])) &\n",
    "    & (df_0[\"item_nbr\"].isin([627887, 759890, 1160872]))\n",
    "].drop(columns=[\"id\"])\n",
    "\n",
    "df_final_check2 = df_sales_nan[(df_sales_nan[\"date\"] > \"2017-31-12\")].sort_values(\n",
    "    by=[\"date\", \"store_nbr\", \"item_nbr\"]\n",
    ")\n",
    "\n",
    "df_final_check2.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 367889472 entries, 0 to 367889471\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   store_nbr   uint8         \n",
      " 1   item_nbr    uint32        \n",
      " 2   date        datetime64[ns]\n",
      " 3   unit_sales  float32       \n",
      "dtypes: datetime64[ns](1), float32(1), uint32(1), uint8(1)\n",
      "memory usage: 5.8 GB\n"
     ]
    }
   ],
   "source": [
    "df_sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final function of merge_store_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEA: transform all to int32 instead of int64 dtype\n",
    "\n",
    "IDea: check newly created columns if can transform int dtype to uint8 or int16 to save memory\n",
    "\n",
    "Idea: check of date dtype can be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_store_status(df):\n",
    "\n",
    "    # Label Variable for atributing numbers to store status, to save memory in df\n",
    "    OPEN = 0\n",
    "    NEW = 2\n",
    "    CLOSED = 4\n",
    "    OLD = 6\n",
    "    NEVER_OPENED = 8\n",
    "\n",
    "    # Group by store and date, then sum sales\n",
    "    df_grouped = (\n",
    "        df.groupby([\"store_nbr\", \"date\"])[\"unit_sales\"]\n",
    "        .sum()  # --> sum or agg ?\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Sort by store and date\n",
    "    df_grouped = df_grouped.sort_values([\"store_nbr\", \"date\"])\n",
    "\n",
    "    # Create a new column for store status, label al stores as 'open' by default\n",
    "    df_grouped[\"store_status\"] = OPEN\n",
    "\n",
    "    # Find the first and last day with sales for each store\n",
    "    first_sale_date = (\n",
    "        df_grouped[df_grouped[\"unit_sales\"] > 0].groupby(\"store_nbr\")[\"date\"].min()\n",
    "    )\n",
    "\n",
    "    last_sale_date = (\n",
    "        df_grouped[df_grouped[\"unit_sales\"] > 0].groupby(\"store_nbr\")[\"date\"].max()\n",
    "    )\n",
    "\n",
    "    # Loop trhough stores by lapeling them as 'NEW', 'CLOSED', 'OLD' or 'NEVER_OPENED' based on first sale date and last sale date\n",
    "    for store in df_grouped[\"store_nbr\"].unique():\n",
    "        store_data = df_grouped[df_grouped[\"store_nbr\"] == store]\n",
    "\n",
    "        if store in first_sale_date.index:\n",
    "            first_date = first_sale_date[store]\n",
    "            last_date = last_sale_date[store]\n",
    "\n",
    "            # Mark as 'NEW' before first sale date\n",
    "            df_grouped.loc[\n",
    "                (df_grouped[\"store_nbr\"] == store) & (df_grouped[\"date\"] < first_date),\n",
    "                \"store_status\",\n",
    "            ] = NEW\n",
    "            # --> To-do: Do we call this  not opened' or a 'new store'?\n",
    "\n",
    "            # Mark as 'closed' after first sale date if sales are 0\n",
    "            df_grouped.loc[\n",
    "                (df_grouped[\"store_nbr\"] == store)\n",
    "                & (df_grouped[\"date\"] > first_date)\n",
    "                & (df_grouped[\"unit_sales\"] == 0),\n",
    "                \"store_status\",\n",
    "            ] = CLOSED\n",
    "\n",
    "            # Mark as 'OLD' after last sale date\n",
    "            df_grouped.loc[\n",
    "                (df_grouped[\"store_nbr\"] == store) & (df_grouped[\"date\"] > last_date),\n",
    "                \"store_status\",\n",
    "            ] = OLD\n",
    "\n",
    "        else:\n",
    "            # If a store never had any sales, mark all dates as 'NEVER_OPENED' --> no records?\n",
    "            df_grouped.loc[df_grouped[\"store_nbr\"] == store, \"store_status\"] = (\n",
    "                NEVER_OPENED\n",
    "            )\n",
    "\n",
    "    # Change the data type of store_status column to int8 to save memory\n",
    "    df_grouped[\"store_status\"] = df_grouped[\"store_status\"].astype(\"int8\")\n",
    "\n",
    "    # Merging store_status on df_sales\n",
    "    df = df.merge(\n",
    "        df_grouped[[\"store_nbr\", \"date\", \"store_status\"]],\n",
    "        left_on=[\"store_nbr\", \"date\"],\n",
    "        right_on=[\"store_nbr\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Using a mask to flag al 'CLOSED'=4 stores and impute 'closed' stores with 0, not opened stores stay NA/NaN\n",
    "    mask = df[\"store_status\"] == 4\n",
    "    df.loc[mask, \"unit_sales\"] = 0\n",
    "\n",
    "    print(\"-\" * 72)\n",
    "    print(\n",
    "        f\"Size of df:     {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB and end observations:       {round(df.shape[0] / 1e6, 1)} million.\"\n",
    "    )\n",
    "    print(\"- \" * 36)\n",
    "    print(\"df_grouped store_status value counts:\")\n",
    "    print(df_grouped[\"store_status\"].value_counts())\n",
    "\n",
    "    print(\"-\" * 72)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.37 GiB for an array with shape (367889472,) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_sales \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_store_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_sales\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 71\u001b[0m, in \u001b[0;36mmerge_store_status\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     68\u001b[0m df_grouped[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_status\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_grouped[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_status\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Merging store_status on df_sales\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_grouped\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore_nbr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore_status\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore_nbr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore_nbr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Using a mask to flag al 'CLOSED'=4 stores and impute 'closed' stores with 0, not opened stores stay NA/NaN\u001b[39;00m\n\u001b[0;32m     79\u001b[0m mask \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_status\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alexander\\Documents\\0. Data Science and AI for Experts\\EAISI_4B_Supermarket\\venv_case_project\\Lib\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexander\\Documents\\0. Data Science and AI for Experts\\EAISI_4B_Supermarket\\venv_case_project\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexander\\Documents\\0. Data Science and AI for Experts\\EAISI_4B_Supermarket\\venv_case_project\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:886\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m--> 886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[0;32m    889\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    890\u001b[0m )\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n",
      "File \u001b[1;32mc:\\Users\\alexander\\Documents\\0. Data Science and AI for Experts\\EAISI_4B_Supermarket\\venv_case_project\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1151\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     join_index, right_indexer, left_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[0;32m   1148\u001b[0m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1151\u001b[0m     (left_indexer, right_indexer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index:\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\alexander\\Documents\\0. Data Science and AI for Experts\\EAISI_4B_Supermarket\\venv_case_project\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1125\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# make mypy happy\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masof\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_join_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhow\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexander\\Documents\\0. Data Science and AI for Experts\\EAISI_4B_Supermarket\\venv_case_project\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1759\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[1;34m(left_keys, right_keys, sort, how)\u001b[0m\n\u001b[0;32m   1757\u001b[0m     _, lidx, ridx \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39mjoin(right, how\u001b[38;5;241m=\u001b[39mhow, return_indexers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1759\u001b[0m     lidx, ridx \u001b[38;5;241m=\u001b[39m \u001b[43mget_join_indexers_non_unique\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\n\u001b[0;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lidx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_range_indexer(lidx, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[0;32m   1764\u001b[0m     lidx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexander\\Documents\\0. Data Science and AI for Experts\\EAISI_4B_Supermarket\\venv_case_project\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1795\u001b[0m, in \u001b[0;36mget_join_indexers_non_unique\u001b[1;34m(left, right, sort, how)\u001b[0m\n\u001b[0;32m   1793\u001b[0m lkey, rkey, count \u001b[38;5;241m=\u001b[39m _factorize_keys(left, right, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1795\u001b[0m     lidx, ridx \u001b[38;5;241m=\u001b[39m \u001b[43mlibjoin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_outer_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1797\u001b[0m     ridx, lidx \u001b[38;5;241m=\u001b[39m libjoin\u001b[38;5;241m.\u001b[39mleft_outer_join(rkey, lkey, count, sort\u001b[38;5;241m=\u001b[39msort)\n",
      "File \u001b[1;32mjoin.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.join.left_outer_join\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.37 GiB for an array with shape (367889472,) and data type int32"
     ]
    }
   ],
   "source": [
    "df_sales = merge_store_status(df_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>date</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>store_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32697713</th>\n",
       "      <td>5</td>\n",
       "      <td>1466039</td>\n",
       "      <td>2016-02-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2978868</th>\n",
       "      <td>1</td>\n",
       "      <td>1174923</td>\n",
       "      <td>2016-05-21</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12665904</th>\n",
       "      <td>2</td>\n",
       "      <td>1957773</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5350858</th>\n",
       "      <td>1</td>\n",
       "      <td>1915102</td>\n",
       "      <td>2017-05-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16592877</th>\n",
       "      <td>3</td>\n",
       "      <td>1157685</td>\n",
       "      <td>2017-03-06</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10692387</th>\n",
       "      <td>2</td>\n",
       "      <td>1686637</td>\n",
       "      <td>2014-08-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21842427</th>\n",
       "      <td>4</td>\n",
       "      <td>563654</td>\n",
       "      <td>2016-10-27</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23741971</th>\n",
       "      <td>4</td>\n",
       "      <td>1243817</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39572827</th>\n",
       "      <td>6</td>\n",
       "      <td>2010755</td>\n",
       "      <td>2015-11-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15858192</th>\n",
       "      <td>3</td>\n",
       "      <td>793345</td>\n",
       "      <td>2016-01-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7984261</th>\n",
       "      <td>2</td>\n",
       "      <td>305341</td>\n",
       "      <td>2013-01-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22264362</th>\n",
       "      <td>4</td>\n",
       "      <td>1057491</td>\n",
       "      <td>2016-08-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279155</th>\n",
       "      <td>1</td>\n",
       "      <td>409904</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39875182</th>\n",
       "      <td>6</td>\n",
       "      <td>1950029</td>\n",
       "      <td>2016-05-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28758944</th>\n",
       "      <td>5</td>\n",
       "      <td>716241</td>\n",
       "      <td>2014-05-04</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926539</th>\n",
       "      <td>1</td>\n",
       "      <td>1348487</td>\n",
       "      <td>2015-08-14</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15021937</th>\n",
       "      <td>3</td>\n",
       "      <td>559494</td>\n",
       "      <td>2014-03-02</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5732734</th>\n",
       "      <td>1</td>\n",
       "      <td>1938490</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11704296</th>\n",
       "      <td>2</td>\n",
       "      <td>1401733</td>\n",
       "      <td>2016-10-24</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84677</th>\n",
       "      <td>1</td>\n",
       "      <td>172343</td>\n",
       "      <td>2013-10-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24265484</th>\n",
       "      <td>4</td>\n",
       "      <td>1464088</td>\n",
       "      <td>2014-04-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30859023</th>\n",
       "      <td>5</td>\n",
       "      <td>1502383</td>\n",
       "      <td>2014-11-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39459575</th>\n",
       "      <td>6</td>\n",
       "      <td>1988377</td>\n",
       "      <td>2015-06-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21703698</th>\n",
       "      <td>4</td>\n",
       "      <td>402136</td>\n",
       "      <td>2015-12-19</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980683</th>\n",
       "      <td>3</td>\n",
       "      <td>2028898</td>\n",
       "      <td>2017-02-24</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35290832</th>\n",
       "      <td>6</td>\n",
       "      <td>360703</td>\n",
       "      <td>2017-02-13</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14768979</th>\n",
       "      <td>3</td>\n",
       "      <td>265237</td>\n",
       "      <td>2014-10-30</td>\n",
       "      <td>13.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23903430</th>\n",
       "      <td>4</td>\n",
       "      <td>1463808</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23228145</th>\n",
       "      <td>4</td>\n",
       "      <td>1146803</td>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22293539</th>\n",
       "      <td>4</td>\n",
       "      <td>1084436</td>\n",
       "      <td>2013-05-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38220885</th>\n",
       "      <td>6</td>\n",
       "      <td>1695961</td>\n",
       "      <td>2016-04-04</td>\n",
       "      <td>8.476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39350323</th>\n",
       "      <td>6</td>\n",
       "      <td>1402032</td>\n",
       "      <td>2016-09-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16506747</th>\n",
       "      <td>3</td>\n",
       "      <td>1179579</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>12.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19410117</th>\n",
       "      <td>3</td>\n",
       "      <td>1944889</td>\n",
       "      <td>2017-02-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6577848</th>\n",
       "      <td>1</td>\n",
       "      <td>906981</td>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31439430</th>\n",
       "      <td>5</td>\n",
       "      <td>1309246</td>\n",
       "      <td>2014-03-07</td>\n",
       "      <td>51.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6187014</th>\n",
       "      <td>1</td>\n",
       "      <td>2010315</td>\n",
       "      <td>2014-05-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7849908</th>\n",
       "      <td>2</td>\n",
       "      <td>155625</td>\n",
       "      <td>2014-12-10</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19913931</th>\n",
       "      <td>3</td>\n",
       "      <td>2017202</td>\n",
       "      <td>2014-08-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11831095</th>\n",
       "      <td>2</td>\n",
       "      <td>1466836</td>\n",
       "      <td>2017-05-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19714135</th>\n",
       "      <td>3</td>\n",
       "      <td>1976186</td>\n",
       "      <td>2017-07-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886528</th>\n",
       "      <td>1</td>\n",
       "      <td>586967</td>\n",
       "      <td>2013-02-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30894036</th>\n",
       "      <td>5</td>\n",
       "      <td>1579081</td>\n",
       "      <td>2013-09-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14366791</th>\n",
       "      <td>3</td>\n",
       "      <td>890371</td>\n",
       "      <td>2013-08-12</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4292571</th>\n",
       "      <td>1</td>\n",
       "      <td>1584348</td>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10552492</th>\n",
       "      <td>2</td>\n",
       "      <td>1300325</td>\n",
       "      <td>2015-03-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34092955</th>\n",
       "      <td>6</td>\n",
       "      <td>115892</td>\n",
       "      <td>2014-02-24</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20350031</th>\n",
       "      <td>3</td>\n",
       "      <td>2087933</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32211433</th>\n",
       "      <td>5</td>\n",
       "      <td>1441516</td>\n",
       "      <td>2015-10-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33199311</th>\n",
       "      <td>5</td>\n",
       "      <td>1981470</td>\n",
       "      <td>2016-11-16</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35616103</th>\n",
       "      <td>6</td>\n",
       "      <td>793346</td>\n",
       "      <td>2015-09-19</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24622661</th>\n",
       "      <td>4</td>\n",
       "      <td>1903391</td>\n",
       "      <td>2017-02-02</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26026958</th>\n",
       "      <td>4</td>\n",
       "      <td>2011155</td>\n",
       "      <td>2016-10-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37154413</th>\n",
       "      <td>6</td>\n",
       "      <td>1239807</td>\n",
       "      <td>2017-03-14</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30618134</th>\n",
       "      <td>5</td>\n",
       "      <td>1370568</td>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29936571</th>\n",
       "      <td>5</td>\n",
       "      <td>1109211</td>\n",
       "      <td>2017-04-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17521423</th>\n",
       "      <td>3</td>\n",
       "      <td>1695879</td>\n",
       "      <td>2017-07-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14581143</th>\n",
       "      <td>3</td>\n",
       "      <td>1091366</td>\n",
       "      <td>2013-07-19</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34432702</th>\n",
       "      <td>6</td>\n",
       "      <td>507479</td>\n",
       "      <td>2015-05-29</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12340108</th>\n",
       "      <td>2</td>\n",
       "      <td>1463451</td>\n",
       "      <td>2015-04-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          store_nbr  item_nbr       date  unit_sales  store_status\n",
       "32697713          5   1466039 2016-02-28         NaN             0\n",
       "2978868           1   1174923 2016-05-21       2.000             0\n",
       "12665904          2   1957773 2015-04-21         NaN             0\n",
       "5350858           1   1915102 2017-05-06         NaN             0\n",
       "16592877          3   1157685 2017-03-06       1.000             0\n",
       "10692387          2   1686637 2014-08-19         NaN             0\n",
       "21842427          4    563654 2016-10-27       4.000             0\n",
       "23741971          4   1243817 2013-09-09         NaN             0\n",
       "39572827          6   2010755 2015-11-10         NaN             0\n",
       "15858192          3    793345 2016-01-26         NaN             0\n",
       "7984261           2    305341 2013-01-22         NaN             0\n",
       "22264362          4   1057491 2016-08-23         NaN             0\n",
       "1279155           1    409904 2016-09-01       4.000             0\n",
       "39875182          6   1950029 2016-05-31         NaN             0\n",
       "28758944          5    716241 2014-05-04       3.000             0\n",
       "4926539           1   1348487 2015-08-14       3.000             0\n",
       "15021937          3    559494 2014-03-02      10.000             0\n",
       "5732734           1   1938490 2013-10-14         NaN             0\n",
       "11704296          2   1401733 2016-10-24       3.000             0\n",
       "84677             1    172343 2013-10-05         NaN             0\n",
       "24265484          4   1464088 2014-04-30         NaN             0\n",
       "30859023          5   1502383 2014-11-27         NaN             0\n",
       "39459575          6   1988377 2015-06-07         NaN             0\n",
       "21703698          4    402136 2015-12-19       1.000             0\n",
       "19980683          3   2028898 2017-02-24      10.000             0\n",
       "35290832          6    360703 2017-02-13      10.000             0\n",
       "14768979          3    265237 2014-10-30      13.000             0\n",
       "23903430          4   1463808 2016-09-12       1.000             0\n",
       "23228145          4   1146803 2016-06-19       5.000             0\n",
       "22293539          4   1084436 2013-05-04         NaN             0\n",
       "38220885          6   1695961 2016-04-04       8.476             0\n",
       "39350323          6   1402032 2016-09-17         NaN             0\n",
       "16506747          3   1179579 2017-01-23      12.000             0\n",
       "19410117          3   1944889 2017-02-02         NaN             0\n",
       "6577848           1    906981 2016-11-01         NaN             0\n",
       "31439430          5   1309246 2014-03-07      51.000             0\n",
       "6187014           1   2010315 2014-05-10         NaN             0\n",
       "7849908           2    155625 2014-12-10       3.000             0\n",
       "19913931          3   2017202 2014-08-19         NaN             0\n",
       "11831095          2   1466836 2017-05-11         NaN             0\n",
       "19714135          3   1976186 2017-07-30         NaN             0\n",
       "2886528           1    586967 2013-02-18         NaN             0\n",
       "30894036          5   1579081 2013-09-18         NaN             0\n",
       "14366791          3    890371 2013-08-12       2.000             0\n",
       "4292571           1   1584348 2017-08-03       1.000             0\n",
       "10552492          2   1300325 2015-03-16         NaN             0\n",
       "34092955          6    115892 2014-02-24      23.000             0\n",
       "20350031          3   2087933 2016-04-06         NaN             0\n",
       "32211433          5   1441516 2015-10-15         NaN             0\n",
       "33199311          5   1981470 2016-11-16       5.000             0\n",
       "35616103          6    793346 2015-09-19       2.000             0\n",
       "24622661          4   1903391 2017-02-02       1.000             0\n",
       "26026958          4   2011155 2016-10-06         NaN             0\n",
       "37154413          6   1239807 2017-03-14       2.000             0\n",
       "30618134          5   1370568 2016-04-05         NaN             0\n",
       "29936571          5   1109211 2017-04-29         NaN             0\n",
       "17521423          3   1695879 2017-07-30         NaN             0\n",
       "14581143          3   1091366 2013-07-19      10.000             0\n",
       "34432702          6    507479 2015-05-29       8.000             0\n",
       "12340108          2   1463451 2015-04-09         NaN             0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.sample(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Define New product for step 4.4\n",
    "\n",
    "•\tBefore the very first sale of an item, all observations are kept as NA\n",
    "\n",
    "•\tAfter the very first sale of an item, we go to step 3:  \n",
    "\n",
    " Sum/Agg all sales group  by item, date\n",
    " --> Sum_sales > 0 then first_sales_day of product\n",
    " else <first_sales_day of product --> delete unit_sales --> N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.drop(columns=[\"store_status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_item_status(df):\n",
    "\n",
    "    start_time = time.time()  # Record the start time of the function\n",
    "\n",
    "    # Label Variable for atributing numbers to store status, to save memory in df\n",
    "    EXISTING = 1\n",
    "    NEW = 3\n",
    "    # TEMPORARY_NOT_SOLD = 5 #---> Myabe to label delivery problems? for longer periods of stockout\n",
    "    OLD = 7\n",
    "    NEVER_SOLD = 9\n",
    "\n",
    "    # Sort by store, item, and date --> is this needed???\n",
    "    df_grouped = df.sort_values([\"store_nbr\", \"item_nbr\", \"date\"])\n",
    "\n",
    "    # Create a new column for item status, label all items as 'existing' by default\n",
    "    df_grouped[\"item_status\"] = EXISTING\n",
    "\n",
    "    # Find the first and last day with sales for each item per store\n",
    "    first_sale_date = (\n",
    "        df_grouped[df_grouped[\"unit_sales\"] > 0]\n",
    "        .groupby([\"store_nbr\", \"item_nbr\"])[\"date\"]\n",
    "        .min()\n",
    "    )\n",
    "\n",
    "    last_sale_date = (\n",
    "        df_grouped[df_grouped[\"unit_sales\"] > 0]\n",
    "        .groupby([\"store_nbr\", \"item_nbr\"])[\"date\"]\n",
    "        .max()\n",
    "    )\n",
    "\n",
    "    # Loop through store-item combinations and label them as 'new' or 'old' based on first and last sale date\n",
    "    for store in df_grouped[\"store_nbr\"].unique():\n",
    "\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        print(\n",
    "            f\"Processing current store number: {store} | Elapsed time: {elapsed_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "        for item in df_grouped[df_grouped[\"store_nbr\"] == store][\"item_nbr\"].unique():\n",
    "\n",
    "            store_item_data = df_grouped[\n",
    "                (df_grouped[\"store_nbr\"] == store) & (df_grouped[\"item_nbr\"] == item)\n",
    "            ]\n",
    "\n",
    "            if (store, item) in first_sale_date.index:\n",
    "                first_date = first_sale_date[(store, item)]\n",
    "                last_date = last_sale_date[(store, item)]\n",
    "\n",
    "                # Mark as 'NEW' before first sale date\n",
    "                df_grouped.loc[\n",
    "                    (df_grouped[\"store_nbr\"] == store)\n",
    "                    & (df_grouped[\"item_nbr\"] == item)\n",
    "                    & (df_grouped[\"date\"] < first_date),\n",
    "                    \"item_status\",\n",
    "                ] = NEW\n",
    "\n",
    "                # Mark as 'OLD' after last sale date\n",
    "                df_grouped.loc[\n",
    "                    (df_grouped[\"store_nbr\"] == store)\n",
    "                    & (df_grouped[\"item_nbr\"] == item)\n",
    "                    & (df_grouped[\"date\"] > last_date),\n",
    "                    \"item_status\",\n",
    "                ] = OLD\n",
    "\n",
    "            else:\n",
    "                # Mark as 'NEVER_SOLD' If an item never had any sales\n",
    "                df_grouped.loc[\n",
    "                    (df_grouped[\"store_nbr\"] == store)\n",
    "                    & (df_grouped[\"item_nbr\"] == item),\n",
    "                    \"item_status\",\n",
    "                ] = NEVER_SOLD  # --> DELETE? ITEM FROM STORE? example item_nbr 2011451 from store_nbr 6\n",
    "                # to-do: investigate if never_sold items are created beceause of NaN function\n",
    "\n",
    "    # Change the data type of store_status column to int8\n",
    "    df_grouped[\"item_status\"] = df_grouped[\"item_status\"].astype(\"int8\")\n",
    "\n",
    "    # Merging item_status on df\n",
    "    df = df.merge(\n",
    "        df_grouped[[\"store_nbr\", \"item_nbr\", \"date\", \"item_status\"]],\n",
    "        left_on=[\"store_nbr\", \"item_nbr\", \"date\"],\n",
    "        right_on=[\"store_nbr\", \"item_nbr\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(\"-\" * 72)\n",
    "    print(\n",
    "        f\"Size of df:     {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB and end observations:       {round(df.shape[0] / 1e6, 1)} million.\"\n",
    "    )\n",
    "    print(\"- \" * 36)\n",
    "    print(f\"Total execution time: {total_time / 60 / 60:.2f} hours\")\n",
    "    print(\"-\" * 72)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Processing current store number: 1 | Elapsed time: 15.05 seconds\n",
    "- Processing current store number: 2 | Elapsed time: 2210.48 seconds\n",
    "- Processing current store number: 3 | Elapsed time: 4455.33 seconds\n",
    "- Processing current store number: 4 | Elapsed time: 6553.91 seconds\n",
    "- Processing current store number: 5 | Elapsed time: 8607.63 seconds\n",
    "- Processing current store number: 6 | Elapsed time: 10650.11 seconds\n",
    "------------------------------------------------------------------------\n",
    "- Size of df:     0.95 GB and end observations:       40.9 million.\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "- Total execution time: 12732.79 seconds\n",
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing current store number: 1 | Elapsed time: 1.76 seconds\n",
      "------------------------------------------------------------------------\n",
      "Size of df:     0.16 GB and end observations:       6.8 million.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Total execution time: 0.12 hours\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# very time consuming function! takes sometimes more then 2000 minutes per store\n",
    "\n",
    "df_sales = merge_item_status(df_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>date</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>item_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-10</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-15</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-17</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-19</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-21</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    store_nbr  item_nbr       date  unit_sales  item_status\n",
       "0           1    103665 2013-01-01         NaN            3\n",
       "1           1    103665 2013-01-02         2.0            1\n",
       "2           1    103665 2013-01-03         3.0            1\n",
       "3           1    103665 2013-01-04         2.0            1\n",
       "4           1    103665 2013-01-05         4.0            1\n",
       "5           1    103665 2013-01-06         2.0            1\n",
       "6           1    103665 2013-01-07         1.0            1\n",
       "7           1    103665 2013-01-08         NaN            1\n",
       "8           1    103665 2013-01-09         1.0            1\n",
       "9           1    103665 2013-01-10         6.0            1\n",
       "10          1    103665 2013-01-11         3.0            1\n",
       "11          1    103665 2013-01-12         3.0            1\n",
       "12          1    103665 2013-01-13         NaN            1\n",
       "13          1    103665 2013-01-14         NaN            1\n",
       "14          1    103665 2013-01-15         6.0            1\n",
       "15          1    103665 2013-01-16         1.0            1\n",
       "16          1    103665 2013-01-17         8.0            1\n",
       "17          1    103665 2013-01-18         3.0            1\n",
       "18          1    103665 2013-01-19         6.0            1\n",
       "19          1    103665 2013-01-20         NaN            1\n",
       "20          1    103665 2013-01-21         3.0            1\n",
       "21          1    103665 2013-01-22         NaN            1\n",
       "22          1    103665 2013-01-23         3.0            1\n",
       "23          1    103665 2013-01-24         3.0            1\n",
       "24          1    103665 2013-01-25         7.0            1\n",
       "25          1    103665 2013-01-26         2.0            1\n",
       "26          1    103665 2013-01-27         NaN            1\n",
       "27          1    103665 2013-01-28         3.0            1\n",
       "28          1    103665 2013-01-29         2.0            1\n",
       "29          1    103665 2013-01-30         2.0            1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>date</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>item_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40876578</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876579</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876580</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876581</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876582</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876583</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876584</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876585</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876586</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876587</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876588</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876589</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876590</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876591</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876592</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876593</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876594</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876595</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876596</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876597</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876598</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876599</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876600</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876601</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876602</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876603</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876604</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876605</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876606</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876607</th>\n",
       "      <td>6</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          store_nbr  item_nbr       date  unit_sales  item_status\n",
       "40876578          6   2011451 2017-07-17         NaN            9\n",
       "40876579          6   2011451 2017-07-18         NaN            9\n",
       "40876580          6   2011451 2017-07-19         NaN            9\n",
       "40876581          6   2011451 2017-07-20         NaN            9\n",
       "40876582          6   2011451 2017-07-21         NaN            9\n",
       "40876583          6   2011451 2017-07-22         NaN            9\n",
       "40876584          6   2011451 2017-07-23         NaN            9\n",
       "40876585          6   2011451 2017-07-24         NaN            9\n",
       "40876586          6   2011451 2017-07-25         NaN            9\n",
       "40876587          6   2011451 2017-07-26         NaN            9\n",
       "40876588          6   2011451 2017-07-27         NaN            9\n",
       "40876589          6   2011451 2017-07-28         NaN            9\n",
       "40876590          6   2011451 2017-07-29         NaN            9\n",
       "40876591          6   2011451 2017-07-30         NaN            9\n",
       "40876592          6   2011451 2017-07-31         NaN            9\n",
       "40876593          6   2011451 2017-08-01         NaN            9\n",
       "40876594          6   2011451 2017-08-02         NaN            9\n",
       "40876595          6   2011451 2017-08-03         NaN            9\n",
       "40876596          6   2011451 2017-08-04         NaN            9\n",
       "40876597          6   2011451 2017-08-05         NaN            9\n",
       "40876598          6   2011451 2017-08-06         NaN            9\n",
       "40876599          6   2011451 2017-08-07         NaN            9\n",
       "40876600          6   2011451 2017-08-08         NaN            9\n",
       "40876601          6   2011451 2017-08-09         NaN            9\n",
       "40876602          6   2011451 2017-08-10         NaN            9\n",
       "40876603          6   2011451 2017-08-11         NaN            9\n",
       "40876604          6   2011451 2017-08-12         NaN            9\n",
       "40876605          6   2011451 2017-08-13         NaN            9\n",
       "40876606          6   2011451 2017-08-14         NaN            9\n",
       "40876607          6   2011451 2017-08-15         NaN            9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>date</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>item_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40563797</th>\n",
       "      <td>6</td>\n",
       "      <td>2048242</td>\n",
       "      <td>2016-03-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17953180</th>\n",
       "      <td>3</td>\n",
       "      <td>1717640</td>\n",
       "      <td>2016-07-24</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17157806</th>\n",
       "      <td>3</td>\n",
       "      <td>1472484</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4377794</th>\n",
       "      <td>1</td>\n",
       "      <td>1533364</td>\n",
       "      <td>2015-03-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27146118</th>\n",
       "      <td>4</td>\n",
       "      <td>2087567</td>\n",
       "      <td>2016-10-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24355051</th>\n",
       "      <td>4</td>\n",
       "      <td>1696036</td>\n",
       "      <td>2014-08-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905329</th>\n",
       "      <td>1</td>\n",
       "      <td>1047790</td>\n",
       "      <td>2014-07-16</td>\n",
       "      <td>12.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22468115</th>\n",
       "      <td>4</td>\n",
       "      <td>843609</td>\n",
       "      <td>2015-04-16</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25291219</th>\n",
       "      <td>4</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2017-05-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37412699</th>\n",
       "      <td>6</td>\n",
       "      <td>1342009</td>\n",
       "      <td>2017-04-05</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37774969</th>\n",
       "      <td>6</td>\n",
       "      <td>1695978</td>\n",
       "      <td>2015-06-25</td>\n",
       "      <td>4.961</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36557473</th>\n",
       "      <td>6</td>\n",
       "      <td>789906</td>\n",
       "      <td>2014-04-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6436347</th>\n",
       "      <td>1</td>\n",
       "      <td>2081139</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31437762</th>\n",
       "      <td>5</td>\n",
       "      <td>1909755</td>\n",
       "      <td>2014-03-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9425793</th>\n",
       "      <td>2</td>\n",
       "      <td>660321</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34158427</th>\n",
       "      <td>6</td>\n",
       "      <td>208384</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>23.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22346300</th>\n",
       "      <td>4</td>\n",
       "      <td>258537</td>\n",
       "      <td>2014-07-11</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8795453</th>\n",
       "      <td>2</td>\n",
       "      <td>671055</td>\n",
       "      <td>2015-09-01</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33943138</th>\n",
       "      <td>5</td>\n",
       "      <td>2059647</td>\n",
       "      <td>2015-04-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30479702</th>\n",
       "      <td>5</td>\n",
       "      <td>1239749</td>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33668942</th>\n",
       "      <td>5</td>\n",
       "      <td>2048374</td>\n",
       "      <td>2013-04-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40471895</th>\n",
       "      <td>6</td>\n",
       "      <td>2048193</td>\n",
       "      <td>2014-02-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22200360</th>\n",
       "      <td>4</td>\n",
       "      <td>986193</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12313318</th>\n",
       "      <td>2</td>\n",
       "      <td>2010519</td>\n",
       "      <td>2015-11-13</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12665441</th>\n",
       "      <td>2</td>\n",
       "      <td>1957773</td>\n",
       "      <td>2014-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12834566</th>\n",
       "      <td>2</td>\n",
       "      <td>2006089</td>\n",
       "      <td>2014-12-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39090690</th>\n",
       "      <td>6</td>\n",
       "      <td>1900715</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34454627</th>\n",
       "      <td>6</td>\n",
       "      <td>521818</td>\n",
       "      <td>2015-05-10</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39521181</th>\n",
       "      <td>6</td>\n",
       "      <td>1915000</td>\n",
       "      <td>2013-02-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6195714</th>\n",
       "      <td>1</td>\n",
       "      <td>2017173</td>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37272789</th>\n",
       "      <td>6</td>\n",
       "      <td>1230060</td>\n",
       "      <td>2013-03-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30614048</th>\n",
       "      <td>5</td>\n",
       "      <td>1364636</td>\n",
       "      <td>2014-04-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21112374</th>\n",
       "      <td>4</td>\n",
       "      <td>839363</td>\n",
       "      <td>2014-07-13</td>\n",
       "      <td>14.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25206831</th>\n",
       "      <td>4</td>\n",
       "      <td>1354382</td>\n",
       "      <td>2017-06-04</td>\n",
       "      <td>8.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27238010</th>\n",
       "      <td>4</td>\n",
       "      <td>2126842</td>\n",
       "      <td>2014-03-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19856048</th>\n",
       "      <td>3</td>\n",
       "      <td>2007505</td>\n",
       "      <td>2013-04-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20689691</th>\n",
       "      <td>4</td>\n",
       "      <td>362035</td>\n",
       "      <td>2017-04-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370288</th>\n",
       "      <td>1</td>\n",
       "      <td>1370588</td>\n",
       "      <td>2015-11-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10531393</th>\n",
       "      <td>2</td>\n",
       "      <td>1696038</td>\n",
       "      <td>2017-07-08</td>\n",
       "      <td>2.806</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12761587</th>\n",
       "      <td>2</td>\n",
       "      <td>1981607</td>\n",
       "      <td>2013-11-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38302111</th>\n",
       "      <td>6</td>\n",
       "      <td>1660239</td>\n",
       "      <td>2016-10-23</td>\n",
       "      <td>29.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37416548</th>\n",
       "      <td>6</td>\n",
       "      <td>1345070</td>\n",
       "      <td>2013-12-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37820713</th>\n",
       "      <td>6</td>\n",
       "      <td>1372585</td>\n",
       "      <td>2015-12-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11195224</th>\n",
       "      <td>2</td>\n",
       "      <td>1696039</td>\n",
       "      <td>2014-02-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23544625</th>\n",
       "      <td>4</td>\n",
       "      <td>1239853</td>\n",
       "      <td>2014-02-06</td>\n",
       "      <td>1.518</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676656</th>\n",
       "      <td>1</td>\n",
       "      <td>885553</td>\n",
       "      <td>2014-04-18</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33968912</th>\n",
       "      <td>5</td>\n",
       "      <td>2088946</td>\n",
       "      <td>2016-07-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30753763</th>\n",
       "      <td>5</td>\n",
       "      <td>1464029</td>\n",
       "      <td>2013-04-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247074</th>\n",
       "      <td>1</td>\n",
       "      <td>368136</td>\n",
       "      <td>2016-08-23</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545819</th>\n",
       "      <td>1</td>\n",
       "      <td>770463</td>\n",
       "      <td>2016-07-23</td>\n",
       "      <td>33.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17795461</th>\n",
       "      <td>3</td>\n",
       "      <td>1717181</td>\n",
       "      <td>2014-07-20</td>\n",
       "      <td>17.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34814882</th>\n",
       "      <td>6</td>\n",
       "      <td>907684</td>\n",
       "      <td>2017-04-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31586625</th>\n",
       "      <td>5</td>\n",
       "      <td>1379194</td>\n",
       "      <td>2015-02-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9528818</th>\n",
       "      <td>2</td>\n",
       "      <td>1114754</td>\n",
       "      <td>2013-02-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979395</th>\n",
       "      <td>1</td>\n",
       "      <td>1458842</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3025877</th>\n",
       "      <td>1</td>\n",
       "      <td>1213906</td>\n",
       "      <td>2015-09-09</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29276298</th>\n",
       "      <td>5</td>\n",
       "      <td>843589</td>\n",
       "      <td>2016-08-07</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9375331</th>\n",
       "      <td>2</td>\n",
       "      <td>1047395</td>\n",
       "      <td>2013-06-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6644868</th>\n",
       "      <td>1</td>\n",
       "      <td>2061121</td>\n",
       "      <td>2015-06-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29157821</th>\n",
       "      <td>5</td>\n",
       "      <td>223463</td>\n",
       "      <td>2015-09-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          store_nbr  item_nbr       date  unit_sales  item_status\n",
       "40563797          6   2048242 2016-03-03         NaN            3\n",
       "17953180          3   1717640 2016-07-24       1.000            1\n",
       "17157806          3   1472484 2015-09-02       3.000            1\n",
       "4377794           1   1533364 2015-03-22         NaN            1\n",
       "27146118          4   2087567 2016-10-22         NaN            3\n",
       "24355051          4   1696036 2014-08-11         NaN            9\n",
       "905329            1   1047790 2014-07-16      12.000            1\n",
       "22468115          4    843609 2015-04-16       1.000            1\n",
       "25291219          4   1398687 2017-05-23         NaN            7\n",
       "37412699          6   1342009 2017-04-05       2.000            1\n",
       "37774969          6   1695978 2015-06-25       4.961            1\n",
       "36557473          6    789906 2014-04-03         NaN            9\n",
       "6436347           1   2081139 2013-01-04         NaN            3\n",
       "31437762          5   1909755 2014-03-27         NaN            3\n",
       "9425793           2    660321 2013-01-02         NaN            9\n",
       "34158427          6    208384 2013-03-01      23.000            1\n",
       "22346300          4    258537 2014-07-11       2.000            1\n",
       "8795453           2    671055 2015-09-01       1.000            1\n",
       "33943138          5   2059647 2015-04-15         NaN            3\n",
       "30479702          5   1239749 2016-03-20       3.000            1\n",
       "33668942          5   2048374 2013-04-05         NaN            3\n",
       "40471895          6   2048193 2014-02-12         NaN            3\n",
       "22200360          4    986193 2017-01-12       3.000            1\n",
       "12313318          2   2010519 2015-11-13       2.000            1\n",
       "12665441          2   1957773 2014-01-13         NaN            3\n",
       "12834566          2   2006089 2014-12-04         NaN            3\n",
       "39090690          6   1900715 2017-08-02       3.000            1\n",
       "34454627          6    521818 2015-05-10       4.000            1\n",
       "39521181          6   1915000 2013-02-07         NaN            3\n",
       "6195714           1   2017173 2015-01-25         NaN            3\n",
       "37272789          6   1230060 2013-03-03         NaN            3\n",
       "30614048          5   1364636 2014-04-26         NaN            1\n",
       "21112374          4    839363 2014-07-13      14.000            1\n",
       "25206831          4   1354382 2017-06-04       8.000            1\n",
       "27238010          4   2126842 2014-03-19         NaN            9\n",
       "19856048          3   2007505 2013-04-15         NaN            3\n",
       "20689691          4    362035 2017-04-13         NaN            1\n",
       "3370288           1   1370588 2015-11-07         NaN            1\n",
       "10531393          2   1696038 2017-07-08       2.806            1\n",
       "12761587          2   1981607 2013-11-04         NaN            3\n",
       "38302111          6   1660239 2016-10-23      29.000            1\n",
       "37416548          6   1345070 2013-12-07         NaN            3\n",
       "37820713          6   1372585 2015-12-10         NaN            1\n",
       "11195224          2   1696039 2014-02-13         NaN            9\n",
       "23544625          4   1239853 2014-02-06       1.518            1\n",
       "1676656           1    885553 2014-04-18       1.000            1\n",
       "33968912          5   2088946 2016-07-12         NaN            9\n",
       "30753763          5   1464029 2013-04-02         NaN            3\n",
       "1247074           1    368136 2016-08-23       1.000            1\n",
       "1545819           1    770463 2016-07-23      33.000            1\n",
       "17795461          3   1717181 2014-07-20      17.000            1\n",
       "34814882          6    907684 2017-04-20         NaN            1\n",
       "31586625          5   1379194 2015-02-09         NaN            1\n",
       "9528818           2   1114754 2013-02-28         NaN            3\n",
       "4979395           1   1458842 2017-01-23       1.000            1\n",
       "3025877           1   1213906 2015-09-09       1.000            1\n",
       "29276298          5    843589 2016-08-07       2.000            1\n",
       "9375331           2   1047395 2013-06-29         NaN            9\n",
       "6644868           1   2061121 2015-06-20         NaN            9\n",
       "29157821          5    223463 2015-09-25         NaN            1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.sample(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Stockout on store level for 4.8 for perishable and non-perishable items\n",
    "\n",
    "•      Perishable good: when there are missing values for two consecutive days for a given item per individual store \n",
    "\n",
    "•      Nonperishable goods: when there are missing values for 7 consecutive days for a given item and per individual store\n",
    "\n",
    "•      Action: Impute with algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perishable good\n",
    "if ['perishable'] == 1 and item_missing_count = 1  #-->  inpute with 0\n",
    "\n",
    "if ['perishable'] == 1 and item_missing_count > 2  #-->  inpute with 0\n",
    "\n",
    "if ['perishable'] == 1 and item_missing_count <= 2 #-->  inpute with mean? or intrapolate?\n",
    "\n",
    "\n",
    "    \n",
    "#non-perishable good  \n",
    "if ['perishable'] == 0 and and item_missing_count > 7 #-->  inpute with 0?\n",
    "\n",
    "if ['perishable'] == 0 and item_missing_count <= 7 #-->  inpute with mean? or intrapolate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First function to impute stockouts, first needed to merge with item for perishable column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = df_sales.merge(df_items, left_on=\"item_nbr\", right_on=\"item_nbr\", how=\"left\")\n",
    "\n",
    "df_sales = df_sales.drop(columns=[\"family\", \"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 367889472 entries, 0 to 367889471\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   store_nbr   uint8         \n",
      " 1   item_nbr    uint32        \n",
      " 2   date        datetime64[ns]\n",
      " 3   unit_sales  float32       \n",
      " 4   perishable  uint8         \n",
      "dtypes: datetime64[ns](1), float32(1), uint32(1), uint8(2)\n",
      "memory usage: 6.2 GB\n"
     ]
    }
   ],
   "source": [
    "df_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_stockouts(df):\n",
    "    # Sort the dataframe by date, store_nbr, and item_nbr\n",
    "    df = df.sort_values([\"store_nbr\", \"item_nbr\", \"date\"])\n",
    "\n",
    "    # Group by store_nbr and item_nbr\n",
    "    df_grouped = df.groupby([\"store_nbr\", \"item_nbr\"])\n",
    "\n",
    "    def impute_group(group):\n",
    "\n",
    "        # Calculate the number of consecutive missing days\n",
    "        group[\"missing_count\"] = (\n",
    "            group[\"unit_sales\"]\n",
    "            .isnull()  # checks for null values in unit_sales\n",
    "            .astype(int)\n",
    "            .groupby(\n",
    "                (group[\"unit_sales\"].notnull().astype(int).diff() != 0).cumsum()\n",
    "            )  # First occurrence, so each streak of consecutive nulls or non-nulls gets a unique number\n",
    "            .cumsum()  # Counts consecutive nulls within each group\n",
    "        )\n",
    "\n",
    "        # Imputation logic for perishable goods if = 1\n",
    "        perishable_mask = group[\"perishable\"] == 1\n",
    "        group.loc[perishable_mask & (group[\"missing_count\"] == 1), \"unit_sales\"] = 0\n",
    "        group.loc[perishable_mask & (group[\"missing_count\"] > 2), \"unit_sales\"] = 0\n",
    "        group.loc[perishable_mask & (group[\"missing_count\"] == 2), \"unit_sales\"] = (\n",
    "            group.loc[perishable_mask, \"unit_sales\"].interpolate()\n",
    "        )\n",
    "\n",
    "        # Imputation logic for non-perishable goods if = 0\n",
    "        non_perishable_mask = group[\"perishable\"] == 0\n",
    "        group.loc[non_perishable_mask & (group[\"missing_count\"] > 7), \"unit_sales\"] = 0\n",
    "        group.loc[non_perishable_mask & (group[\"missing_count\"] <= 7), \"unit_sales\"] = (\n",
    "            group.loc[non_perishable_mask, \"unit_sales\"].interpolate()\n",
    "        )\n",
    "\n",
    "        # Drop temporary missing count column\n",
    "        group = group.drop(\"missing_count\", axis=1)\n",
    "\n",
    "        return\n",
    "\n",
    "    # Apply the imputation to each group\n",
    "    imputed_df = df_grouped.apply(impute_group).reset_index(drop=True)\n",
    "\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_group(group):\n",
    "\n",
    "    # Calculate the number of consecutive missing days\n",
    "    group[\"missing_count\"] = (\n",
    "        group[\"unit_sales\"]\n",
    "        .isnull()  # checks for null values in unit_sales\n",
    "        .astype(int)\n",
    "        .groupby(\n",
    "            (group[\"unit_sales\"].notnull().astype(int).diff() != 0).cumsum()\n",
    "        )  # First occurrence, so each streak of consecutive nulls or non-nulls gets a unique number\n",
    "        .cumsum()  # Counts consecutive nulls within each group\n",
    "    )\n",
    "\n",
    "    # Imputation logic for perishable goods if 1\n",
    "    perishable_mask = group[\"perishable\"] == 1\n",
    "    group.loc[perishable_mask & (group[\"missing_count\"] == 1), \"unit_sales\"] = 0\n",
    "    group.loc[perishable_mask & (group[\"missing_count\"] > 2), \"unit_sales\"] = 0\n",
    "    group.loc[perishable_mask & (group[\"missing_count\"] == 2), \"unit_sales\"] = (\n",
    "        group.loc[perishable_mask, \"unit_sales\"].interpolate()  # to-do: Interpolate ()\n",
    "    )\n",
    "\n",
    "    # Imputation logic for non-perishable goods if 0\n",
    "    non_perishable_mask = group[\"perishable\"] == 0\n",
    "    group.loc[non_perishable_mask & (group[\"missing_count\"] > 7), \"unit_sales\"] = 0\n",
    "    group.loc[non_perishable_mask & (group[\"missing_count\"] <= 7), \"unit_sales\"] = (\n",
    "        group.loc[\n",
    "            non_perishable_mask, \"unit_sales\"\n",
    "        ].interpolate()  # To-do: Interpolate ()\n",
    "    )\n",
    "\n",
    "    # Drop temporary missing count column\n",
    "    group = group.drop(\"missing_count\", axis=1)\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_stockouts(df):\n",
    "    # Sort the dataframe by date, store_nbr, and item_nbr\n",
    "    df = df.sort_values([\"store_nbr\", \"item_nbr\", \"date\"])\n",
    "\n",
    "    # Group by store_nbr and item_nbr\n",
    "    df_grouped = df.groupby([\"store_nbr\", \"item_nbr\"])\n",
    "\n",
    "    # Apply the imputation to each group\n",
    "    imputed_df = df_grouped.apply(impute_group).reset_index(drop=True)\n",
    "\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_test = df_sales[df_sales[\"item_nbr\"].isin([103665, 1398687, 2011451])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexander\\AppData\\Local\\Temp\\ipykernel_20696\\612087311.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  imputed_df = df_grouped.apply(impute_group).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "df_sales_test = impute_stockouts(df_sales_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>date</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>item_status</th>\n",
       "      <th>perishable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-10</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-15</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-17</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-19</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-01-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    store_nbr  item_nbr       date  unit_sales  item_status  perishable\n",
       "0           1    103665 2013-01-01         0.0            3           1\n",
       "1           1    103665 2013-01-02         2.0            1           1\n",
       "2           1    103665 2013-01-03         3.0            1           1\n",
       "3           1    103665 2013-01-04         2.0            1           1\n",
       "4           1    103665 2013-01-05         4.0            1           1\n",
       "5           1    103665 2013-01-06         2.0            1           1\n",
       "6           1    103665 2013-01-07         1.0            1           1\n",
       "7           1    103665 2013-01-08         0.0            1           1\n",
       "8           1    103665 2013-01-09         1.0            1           1\n",
       "9           1    103665 2013-01-10         6.0            1           1\n",
       "10          1    103665 2013-01-11         3.0            1           1\n",
       "11          1    103665 2013-01-12         3.0            1           1\n",
       "12          1    103665 2013-01-13         0.0            1           1\n",
       "13          1    103665 2013-01-14         3.0            1           1\n",
       "14          1    103665 2013-01-15         6.0            1           1\n",
       "15          1    103665 2013-01-16         1.0            1           1\n",
       "16          1    103665 2013-01-17         8.0            1           1\n",
       "17          1    103665 2013-01-18         3.0            1           1\n",
       "18          1    103665 2013-01-19         6.0            1           1\n",
       "19          1    103665 2013-01-20         0.0            1           1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>date</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>item_status</th>\n",
       "      <th>perishable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5044</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5045</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5046</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5047</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5048</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5049</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5050</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5052</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5053</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5054</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5055</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5056</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5059</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5060</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5061</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5062</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5063</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      store_nbr  item_nbr       date  unit_sales  item_status  perishable\n",
       "5044          1   2011451 2017-07-27         0.0            9           0\n",
       "5045          1   2011451 2017-07-28         0.0            9           0\n",
       "5046          1   2011451 2017-07-29         0.0            9           0\n",
       "5047          1   2011451 2017-07-30         0.0            9           0\n",
       "5048          1   2011451 2017-07-31         0.0            9           0\n",
       "5049          1   2011451 2017-08-01         0.0            9           0\n",
       "5050          1   2011451 2017-08-02         0.0            9           0\n",
       "5051          1   2011451 2017-08-03         0.0            9           0\n",
       "5052          1   2011451 2017-08-04         0.0            9           0\n",
       "5053          1   2011451 2017-08-05         0.0            9           0\n",
       "5054          1   2011451 2017-08-06         0.0            9           0\n",
       "5055          1   2011451 2017-08-07         0.0            9           0\n",
       "5056          1   2011451 2017-08-08         0.0            9           0\n",
       "5057          1   2011451 2017-08-09         0.0            9           0\n",
       "5058          1   2011451 2017-08-10         0.0            9           0\n",
       "5059          1   2011451 2017-08-11         0.0            9           0\n",
       "5060          1   2011451 2017-08-12         0.0            9           0\n",
       "5061          1   2011451 2017-08-13         0.0            9           0\n",
       "5062          1   2011451 2017-08-14         0.0            9           0\n",
       "5063          1   2011451 2017-08-15         0.0            9           0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_test.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>date</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>item_status</th>\n",
       "      <th>perishable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2014-10-21</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2816</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2016-02-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2016-05-15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-02-19</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2014-04-28</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2016-02-26</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2013-04-10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4337</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2015-08-20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2014-09-16</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2014-03-24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2014-02-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2013-12-19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5039</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-07-22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2014-04-26</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2014-03-21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2015-04-18</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2016-07-25</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3789</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2014-02-18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3488</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2013-04-23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3585</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2017-05-14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2014-04-14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2015-10-15</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2014-05-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-08-28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2014-09-18</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2014-05-31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4629</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2016-06-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2014-02-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2013-07-27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3115</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2016-11-28</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2016-02-18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2016-02-19</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2016-07-06</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-12-22</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2015-06-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3601</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2014-06-18</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4585</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2016-07-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2013-03-23</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4013</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2015-10-21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2016-10-26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4717</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2016-09-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2015-10-11</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2014-06-27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2014-10-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2013-06-24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2267</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2014-08-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2013-11-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2014-08-28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2015-07-07</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>1</td>\n",
       "      <td>1398687</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4122</th>\n",
       "      <td>1</td>\n",
       "      <td>2011451</td>\n",
       "      <td>2015-01-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      store_nbr  item_nbr       date  unit_sales  item_status  perishable\n",
       "658           1    103665 2014-10-21    1.000000            1           1\n",
       "2816          1   1398687 2016-02-03    0.000000            1           0\n",
       "2918          1   1398687 2016-05-15    1.000000            1           0\n",
       "49            1    103665 2013-02-19    3.000000            1           1\n",
       "482           1    103665 2014-04-28    1.000000            1           1\n",
       "2839          1   1398687 2016-02-26    4.000000            1           0\n",
       "1787          1   1398687 2013-04-10    0.000000            3           0\n",
       "4337          1   2011451 2015-08-20    0.000000            9           0\n",
       "623           1    103665 2014-09-16    8.000000            1           1\n",
       "2135          1   1398687 2014-03-24    0.000000            3           0\n",
       "2091          1   1398687 2014-02-08    0.000000            3           0\n",
       "3728          1   2011451 2013-12-19    0.000000            9           0\n",
       "5039          1   2011451 2017-07-22    0.000000            9           0\n",
       "480           1    103665 2014-04-26    7.000000            1           1\n",
       "2132          1   1398687 2014-03-21    0.000000            3           0\n",
       "2525          1   1398687 2015-04-18    4.000000            1           0\n",
       "1301          1    103665 2016-07-25    2.000000            1           1\n",
       "3789          1   2011451 2014-02-18    0.000000            9           0\n",
       "3488          1   2011451 2013-04-23    0.000000            9           0\n",
       "3585          1   2011451 2013-07-29    0.000000            9           0\n",
       "4970          1   2011451 2017-05-14    0.000000            9           0\n",
       "2156          1   1398687 2014-04-14    0.000000            3           0\n",
       "1017          1    103665 2015-10-15   13.000000            1           1\n",
       "488           1    103665 2014-05-04    0.000000            1           1\n",
       "239           1    103665 2013-08-28    0.000000            1           1\n",
       "3247          1   1398687 2017-04-09    0.000000            7           0\n",
       "625           1    103665 2014-09-18    3.000000            1           1\n",
       "2203          1   1398687 2014-05-31    0.000000            3           0\n",
       "4629          1   2011451 2016-06-07    0.000000            9           0\n",
       "3026          1   1398687 2016-08-31    6.000000            1           0\n",
       "2099          1   1398687 2014-02-16    0.000000            3           0\n",
       "1895          1   1398687 2013-07-27    0.000000            3           0\n",
       "3115          1   1398687 2016-11-28   10.000000            1           0\n",
       "4519          1   2011451 2016-02-18    0.000000            9           0\n",
       "1144          1    103665 2016-02-19    6.000000            1           1\n",
       "1282          1    103665 2016-07-06    6.000000            1           1\n",
       "355           1    103665 2013-12-22    3.000000            1           1\n",
       "4262          1   2011451 2015-06-06    0.000000            9           0\n",
       "3601          1   2011451 2013-08-14    0.000000            9           0\n",
       "533           1    103665 2014-06-18    3.000000            1           1\n",
       "4585          1   2011451 2016-04-24    0.000000            9           0\n",
       "4669          1   2011451 2016-07-17    0.000000            9           0\n",
       "81            1    103665 2013-03-23    4.000000            1           1\n",
       "4013          1   2011451 2014-09-30    0.000000            9           0\n",
       "4399          1   2011451 2015-10-21    0.000000            9           0\n",
       "4770          1   2011451 2016-10-26    0.000000            9           0\n",
       "4717          1   2011451 2016-09-03    0.000000            9           0\n",
       "2701          1   1398687 2015-10-11    2.333333            1           0\n",
       "2230          1   1398687 2014-06-27    0.000000            3           0\n",
       "1225          1    103665 2016-05-10    6.000000            1           1\n",
       "2329          1   1398687 2014-10-04    0.000000            3           0\n",
       "1589          1    103665 2017-05-09    4.000000            1           1\n",
       "1862          1   1398687 2013-06-24    0.000000            3           0\n",
       "2267          1   1398687 2014-08-03    0.000000            3           0\n",
       "2008          1   1398687 2013-11-17    0.000000            3           0\n",
       "1278          1    103665 2016-07-02    4.000000            1           1\n",
       "3980          1   2011451 2014-08-28    0.000000            9           0\n",
       "917           1    103665 2015-07-07    0.500000            1           1\n",
       "3245          1   1398687 2017-04-07    0.000000            7           0\n",
       "4122          1   2011451 2015-01-17    0.000000            9           0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_test.sample(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sales_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_sales_test\u001b[49m\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_sales_test' is not defined"
     ]
    }
   ],
   "source": [
    "df_sales_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate between missing datapoints --> sales\n",
    "\n",
    "fillna(method=\"mean\")\n",
    "\n",
    "df[\"column_name\"].interpolate(method=\"linear\", inplace=True)\n",
    "\n",
    "df[\"column_name\"].interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "df[\"column_name\"].interpolate(method=\"polynomial\", order=2, inplace=True)\n",
    "\n",
    "# Interpolate missing values for the 'unit_sales' column\n",
    "df[\"unit_sales\"] = df.groupby([\"store_nbr\", \"item_nbr\"])[\"unit_sales\"].apply(\n",
    "    lambda group: group.interpolate(method=\"linear\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP1XEh6OeJMgPK4tWBSiOU8",
   "collapsed_sections": [],
   "mount_file_id": "1tas4gpn15avV6RH91pDuPfl4mfnjqven",
   "name": "Copy of 2020 12 13 - EyeOn Supermarket - v1.ipynb",
   "provenance": [
    {
     "file_id": "1tas4gpn15avV6RH91pDuPfl4mfnjqven",
     "timestamp": 1608148229653
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
