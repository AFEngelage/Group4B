{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporacion Favorita - New Superb Forecasting Model - \n",
    "\n",
    "## Data Preperation Pipeline\n",
    "\n",
    "Made by 4B Consultancy (Janne Heuvelmans, Georgi Duev, Alexander Engelage, Sebastiaan de Bruin) - 2024\n",
    "\n",
    "In this data pipeline, the data used for forecasting item unit_sales will be processed and finalized before being imported in the machine learning model.   \n",
    "The following steps are made within this notebook:  \n",
    "\n",
    ">-0. Import Packages \n",
    "     \n",
    ">-1. Load and optimize raw data  \n",
    "    -1.1. Functions - Creation of downcast and normalize functions for initial data load  \n",
    "    -1.2. Functions - Import raw data from local path  \n",
    "    -1.3. Importing raw data  \n",
    "       \n",
    ">-2. Cleaning data (functions)  \n",
    "    -2.1. Return list containing stores with less then 1670 operational days with sales  \n",
    "    -2.2. Return list containing stores with cluster=10 in stores df  \n",
    "    -2.3. Function to exclude stores with less then 1670 sales days and related to cluster 10 \n",
    " \n",
    ">-3. Excluding data based on exploratory data analyses (functions)  \n",
    "    -3.1. Function (partly optional) - Excluding stores based on sales units and on cluster type 10    \n",
    "    -3.2. Function - Exclude holiday event related to the \"Terromoto\" volcano event  \n",
    "\n",
    ">-4. Enriching datasets for further analysis (functions)  \n",
    "    -4.1. Function - Determining holidays per store     \n",
    "    -4.2. Function - Determining a count per type of holiday per store  \n",
    "    -4.3. Function - Constructing a cartesian sales dataset for each store based on the maximum sales daterange  \n",
    "\n",
    ">-5. Constructing final dataset\n",
    "\n",
    "The structure of this notebook was inspired by:\n",
    "https://hamilton.dagworks.io/en/latest/how-tos/use-in-jupyter-notebook/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import os\n",
    "import sys\n",
    "import altair as alt\n",
    "import vegafusion as vf\n",
    "import sklearn\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and optimize raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Functions - Creation of downcast and normalize functions for initial data load\n",
    "Update formatting of features to optimize memory and standardize column names.  \n",
    "Furthermore, get basic information on loaded data and print back to user.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.1. Optimize memory by:  \n",
    "- a) Remove spaces from column names.    \n",
    "- b) Downcasting objects, integers and floats.  \n",
    "- c) Standardize date columns to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data memory optimization function 1 - Removing spaces from the column names\n",
    "def standardize_column_names(s):\n",
    "    \"\"\"Removes spaces from the column names.\"\"\"\n",
    "\n",
    "    return s.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "# Data memory optimization function 2 - Changing datatypes to smaller ones (downcasting)\n",
    "def optimize_memory(df):\n",
    "    \"\"\"Optimize memory usage of a DataFrame by converting object columns to categorical\n",
    "    and downcasting numeric columns to smaller types.\"\"\"\n",
    "\n",
    "    # Change: Objects to Categorical.\n",
    "    object_cols = df.select_dtypes(include=\"object\").columns\n",
    "    if not object_cols.empty:\n",
    "        print(\"Change: Objects to Categorical\")\n",
    "        df[object_cols] = df[object_cols].astype(\"category\")\n",
    "\n",
    "    # Change: Convert integers to smallest signed or unsigned integer and floats to smallest.\n",
    "    for col in df.select_dtypes(include=[\"int\"]).columns:\n",
    "        if (df[col] >= 0).all():  # Check if all values are non-negative\n",
    "            df[col] = pd.to_numeric(\n",
    "                df[col], downcast=\"unsigned\"\n",
    "            )  # Downcast to unsigned\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"integer\")  # Downcast to signed\n",
    "\n",
    "    # Downcast float columns\n",
    "    for col in df.select_dtypes(include=[\"float\"]).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Data memory optimization function 4 - Transform date-related columns to datetime format.\n",
    "def transform_date_to_datetime(df, i):\n",
    "    \"\"\"Transform date-related columns to datetime format.\"\"\"\n",
    "    if i != 0:\n",
    "        if \"date\" in df.columns:\n",
    "            print(\"Change: Transformed 'date' column to Datetime Dtype\")\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None).dt.floor(\"D\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2. Return basic information on each dataframe:  \n",
    "- a) Information on the number of observation and features.  \n",
    "- b) Information on the optimized size of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the basic information of the dataframe (number of observations and features, optimized size)\n",
    "def df_basic_info(df, dataframe_name):\n",
    "    print(\n",
    "        f\"The '{dataframe_name}' dataframe contains: {df.shape[0]:,}\".replace(\",\", \".\")\n",
    "        + f\" observations and {df.shape[1]} features.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"After optimizing by downcasting and normalizing it has optimized size of    {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Functions - Import raw data from local PATH\n",
    "Create import data function and apply downcast, normalize functions and give basic information function within the importing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_data(i=0):\n",
    "\n",
    "    # Define path.\n",
    "    c_path = \"C:/Users/sebas/OneDrive/Documenten/GitHub/Supermarketcasegroupproject/Group4B/data/raw/\"\n",
    "\n",
    "    # c_path = \"C:/Users/alexander/Documents/0. Data Science and AI for Experts/EAISI_4B_Supermarket/data/raw/\"\n",
    "\n",
    "    # c_path = 'https://www.dropbox.com/scl/fo/4f5xcrzfqlyv3qjzm0kgc/AAJkdVC_Wa8NjoTBMwG4gx4?rlkey=gyi9pc4rcmghkzk2wgqyb7y4o&dl=0' Checking if possible to use c_path of dropbox\n",
    "\n",
    "    # Identify file.\n",
    "    v_file = (\n",
    "        \"history-per-year\",  # 0\n",
    "        \"holidays_events\",  # 1\n",
    "        \"items\",  # 2\n",
    "        \"stores\",  # 3\n",
    "    )\n",
    "\n",
    "    print(f\"\\nReading file {i}\\n\")\n",
    "\n",
    "    # Load data.\n",
    "    df = (\n",
    "        pd.read_parquet(c_path + v_file[i] + \".parquet\")\n",
    "        .rename(columns=standardize_column_names)\n",
    "        .pipe(optimize_memory)\n",
    "        .pipe(transform_date_to_datetime, i)\n",
    "    )\n",
    "\n",
    "    # Return data.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Importing raw data\n",
    "Importing parquet files with importing function (downcasting, normalizing and giving basic information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading file 0\n",
      "\n",
      "\n",
      "Reading file 1\n",
      "\n",
      "Change: Objects to Categorical\n",
      "Change: Transformed 'date' column to Datetime Dtype\n",
      "\n",
      "Reading file 2\n",
      "\n",
      "Change: Objects to Categorical\n",
      "\n",
      "Reading file 3\n",
      "\n",
      "Change: Objects to Categorical\n"
     ]
    }
   ],
   "source": [
    "# Sales History per year\n",
    "df_sales = f_get_data(0)\n",
    "\n",
    "# Holidays\n",
    "df_holidays = f_get_data(1)\n",
    "\n",
    "# Items\n",
    "df_items = f_get_data(2)\n",
    "\n",
    "# Stores\n",
    "df_stores = f_get_data(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning data (functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Prepare and clean df_sales\n",
    "Drop of columns \"id\", \"year\", \"month\", \"day\" and create a date column based on the columns \"year\" , \"month\" and \"day\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_sales by cleaning up df for merging with holidays by dropping unneeded columns\n",
    "def sales_cleaned(df_sales):\n",
    "    df_sales[\"item_nbr\"].astype(int)\n",
    "    df_sales[\"date\"] = pd.to_datetime(df_sales[[\"year\", \"month\", \"day\"]])\n",
    "    df_sales = df_sales.drop(columns=[\"id\", \"year\", \"month\", \"day\"])\n",
    "\n",
    "    return df_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Prepare, clean and rename df_items\n",
    "Renaming columns: \"family\" to \"item_family\" and  \"class\" to \"item_class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_items by cleaning up df by renaming columns for clearity in final df\n",
    "def items_cleaned_renamed(df_items):\n",
    "    df_items[\"item_nbr\"] = df_items[\"item_nbr\"].astype(int)\n",
    "    df_items = df_items.rename(columns={\"family\": \"item_family\", \"class\": \"item_class\"})\n",
    "\n",
    "    return df_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Prepare, clean and rename df_stores\n",
    "Drop of columns \"state\"  \n",
    "Rename of columns \"city\" to \"store_city\", \"cluster\" to \"store_cluster\" and \"type\" to \"store_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_stores by cleaning up df by dropping unneeded columns and rename columns for clearity in final df\n",
    "def stores_cleaned_renamed(df_stores):\n",
    "\n",
    "    df_stores = df_stores.drop(columns=[\"state\", \"city\"])\n",
    "\n",
    "    df_stores = df_stores.rename(\n",
    "        columns={\n",
    "            \"cluster\": \"store_cluster\",\n",
    "            \"type\": \"store_type\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Excluding data based on exploratory data analyses (functions)\n",
    "Excluding sales data based on store sales availability  \n",
    "Excluding holiday events related to the \"Terromoto\" volcano event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sales_funnel_construction(df_sales):\n",
    "\n",
    "    start_date_dataset = df_sales[\"date\"].min()\n",
    "    scoping_end_date = df_sales[\"date\"].max()\n",
    "\n",
    "    df_sales_funnel = df_sales.copy()\n",
    "\n",
    "    # Add the item family to all the rows, make sure that items have been cleaned before running this function!\n",
    "    df_sales_funnel = df_sales_funnel.merge(df_items, how= 'inner', on ='item_nbr')\n",
    "\n",
    "    # Calculate the total sum of unit sales and the unit percentage per row\n",
    "    sales_funnel_total_unit_sales = df_sales_funnel['unit_sales'].sum()\n",
    "    df_sales_funnel['unit_sales_percentage'] = (df_sales_funnel['unit_sales'] / sales_funnel_total_unit_sales) * 100\n",
    "\n",
    "    # Normalize percentages\n",
    "    df_sales_funnel['unit_sales_percentage'] = (df_sales_funnel['unit_sales_percentage'] / \n",
    "                                                df_sales_funnel['unit_sales_percentage'].sum()) * 100\n",
    "\n",
    "    return start_date_dataset, scoping_end_date, df_sales_funnel, sales_funnel_total_unit_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Function (partly optional) - Excluding stores based on sales units and on cluster type 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.1. Function (optional) - Return list containing stores with less then 1670 operational days with sales  \n",
    "default parameter: store_exclusion_cutoff_number = 1670 days. Based on Exploratory data analysis, 17 stores do not have 1670 days of date present in the sales dataset and either are new stores are were closed for a significant number of days during the timeframe within the sales dataset. It might be functional to make the model only for stores that had sales for all dates (and not new) as that might influence model behavior. This function gives the flexibility as so the user can choose him/herself the cutoff point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stores_exclude_sales_days(df_sales, param=None):\n",
    "    \"\"\"\n",
    "    Filters out stores that do not meet sales conditions required for forecasting based on historical and \n",
    "    in-scope sales data.\n",
    "\n",
    "    This function processes a sales dataset to identify and exclude stores that either:\n",
    "    1. Do not have any sales prior to the scoping start date (i.e., missing history).\n",
    "    2. Have insufficient sales within the last two years leading up to the scoping end date.\n",
    "\n",
    "    It applies a default or user-defined threshold to determine which stores are missing significant sales \n",
    "    within the in-scope period.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_sales : pandas.DataFrame\n",
    "        The input DataFrame containing sales data. It must have at least the following columns:\n",
    "        - 'store_nbr': The store identifier.\n",
    "        - 'date': The date of the sales record.\n",
    "        - 'unit_sales': The number of units sold on that date.\n",
    "    \n",
    "    param : int, optional\n",
    "        A threshold value for determining whether a store is missing too many sales dates within the scoping period.\n",
    "        If no value is provided, the function will use the maximum number of valid sales dates within the scoping \n",
    "        period as the threshold.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list_scoping_filtered : list\n",
    "        A list of `store_nbr` identifiers for stores that either have no sales history before the scoping start date \n",
    "        or are missing too many sales records within the last two years (based on the threshold).\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    filtered_stores = stores_exclude_sales_days(df_sales, param=50)\n",
    "    \"\"\"\n",
    "\n",
    "    start_date_dataset = df_sales[\"date\"].min()\n",
    "    scoping_start_date = df_sales[\"date\"].max() - pd.DateOffset(years=2)\n",
    "    scoping_end_date = df_sales[\"date\"].max()\n",
    "\n",
    "    df_scoping = df_sales.copy()\n",
    "\n",
    "    df_scoping = df_scoping.groupby(['store_nbr','date']).agg(\n",
    "            {'unit_sales':'sum'}).reset_index()\n",
    "\n",
    "    # STEP 1 - Make a filter based on the sales prior to the scoping start date! We assume that stores that don't have any sales prior to the scoping start date are not \n",
    "    # candidates for the selection for forecasting.\n",
    "\n",
    "    # Filter the dataframe within the date range # Initialize a column with 0s\n",
    "    df_scoping['label_history'] = 0\n",
    "\n",
    "    # Make a mask that determines if there are any sales on each date and store combination prior to the scoping_start_date. Having no sales before the scoping start date \n",
    "    # Gives us an idea that there are no sales on the first date for scoping, making the store not a good candidate for further analysis as we need 2 years.\n",
    "    mask_historyscoping = (df_scoping['date'] >= start_date_dataset) & (df_scoping['date'] <= scoping_start_date) & (df_scoping['unit_sales'].notna())\n",
    "\n",
    "    maskwithinset = (df_scoping['date'] >= scoping_start_date) & (df_scoping['date'] <= scoping_end_date) & (df_scoping['unit_sales'].notna())\n",
    "\n",
    "    # Assign label 1 where conditions are met\n",
    "    df_scoping.loc[mask_historyscoping, 'label_history'] = 1\n",
    "    df_scoping.loc[maskwithinset, 'label_withinsetcheck'] = 1\n",
    "\n",
    "    # Sum the labels for the period before scoping\n",
    "    df_scoping = df_scoping.groupby(['store_nbr']).agg(\n",
    "        unit_sales_count_history=('label_history', 'sum'),\n",
    "        unit_sales_count_check= ('label_withinsetcheck', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Set default_setrangemax based on the data if the param is not provided\n",
    "    if param is None:\n",
    "        default_setrangemax = df_scoping['unit_sales_count_check'].max()\n",
    "    else:\n",
    "        default_setrangemax = param\n",
    "\n",
    "    # Make a flag for stores that don't have any sales before the start date of scoping\n",
    "    df_scoping['flag_nohistory'] = df_scoping['unit_sales_count_history'].apply(lambda x: 1 if x == 0 else 0)\n",
    "    df_scoping['flag_withinsetcheck'] = df_scoping['unit_sales_count_check'].apply(lambda x: 1 if x < default_setrangemax else 0)\n",
    "\n",
    "    # Select only the stores that do not have any sales prior to the scoping start date\n",
    "    # Apply filter with OR condition: either flag_scopingtwoyears == 1 or flag_withinsetcheck == 1\n",
    "\n",
    "    df_scoping_nohistory_dropout = df_scoping[(df_scoping['flag_nohistory'] == 1)]['store_nbr'].tolist()\n",
    "    df_datasetcheck_dropout = df_scoping[(df_scoping['flag_withinsetcheck'] == 1)]['store_nbr'].tolist()\n",
    "\n",
    "    list_scoping_filtered = df_scoping[\n",
    "        (df_scoping['flag_nohistory'] == 1) | (df_scoping['flag_withinsetcheck'] == 1)\n",
    "    ]['store_nbr'].tolist()\n",
    "\n",
    "    # STEP 2 - Yet, some sales do have sales prior the scoping start date but do miss a significant amount of sales within our two year time window. We need to exclude\n",
    "\n",
    "    print(f'The first date in the sales dataset found is: {start_date_dataset}')\n",
    "    print(f'The last date in the sales dataset found is:  {scoping_end_date}')\n",
    "    print(f'Subtracting two years for train/test/split we now search for stores that did not have any sales before:  {scoping_start_date}')\n",
    "    print(f'We assume these stores do not meet the condition in that they do not have sales for the full last two years, they will atleast at the beginning of this period miss data and thus are not good candidates')\n",
    "    print(f'Stores {df_scoping_nohistory_dropout} were excluded from our dataset based on no sales prior to the used range within the dataset for forecasting')\n",
    "    print(f\"The maximum number of datapoints found within the 2 years set used for forecasting is {df_scoping['unit_sales_count_check'].max()}, the parameter in the function is set to {param}, if filled, this will be used as the cut-off value for our 2 year dataset\")\n",
    "    print(f'Stores {df_datasetcheck_dropout} were excluded from our dataset based on missing to much dates with sales within the dataset needed for forecasting')\n",
    "\n",
    "    return list_scoping_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.2. Function - Return list containing stores with cluster=10 in stores df  \n",
    "From our exploratory data analysis we found that cluster 10 had data issues as it was the only cluster that could was assigned to multiple storetypes. Therefore and because these stores are not part of the top 10 in terms of unit sales, we excluded all stores assigned to cluster 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stores_exclude_cluster(df_stores, cluster_number=10):\n",
    "\n",
    "    # Get the list of store numbers that belong to cluster 10\n",
    "\n",
    "    list_stores_cluster_10 = df_stores[df_stores[\"cluster\"] == cluster_number][\n",
    "        \"store_nbr\"\n",
    "    ].tolist()\n",
    "\n",
    "    print(f'Stores {list_stores_cluster_10} were excluded from our dataset based on beloning to cluster 10')\n",
    "    return list_stores_cluster_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.3. Function - Exclude stores with less then X sales days and stores related to cluster 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sales_cleaned_stores(df_sales, df_stores, df_sales_funnel, store_exclusion_cutoff_number=None):\n",
    "\n",
    "    # Excluded less then 1670 salesdays\n",
    "    list_scoping_filtered = stores_exclude_sales_days(\n",
    "        df_sales, store_exclusion_cutoff_number\n",
    "    )\n",
    "\n",
    "    # Funneling information step 2\n",
    "    df_sales_funnel['Exclude_stores_days'] = df_sales_funnel['store_nbr'].isin(list_scoping_filtered).astype(int)\n",
    "\n",
    "    df_sales = df_sales.drop(\n",
    "        df_sales[df_sales[\"store_nbr\"].isin(list_scoping_filtered)].index\n",
    "    )\n",
    "\n",
    "    # Cluster 10\n",
    "    list_stores_cluster_10 = stores_exclude_cluster(df_stores, cluster_number=10)\n",
    "\n",
    "    # Funneling information step 3\n",
    "    df_sales_funnel['Exclude_stores_cluster'] = df_sales_funnel['store_nbr'].isin(list_stores_cluster_10).astype(int)\n",
    "\n",
    "    df_sales = df_sales.drop(\n",
    "        df_sales[df_sales[\"store_nbr\"].isin(list_stores_cluster_10)].index\n",
    "    )\n",
    "\n",
    "    return df_sales,df_sales_funnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Function - Excluding items based on item family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_exclude_family(df_items, families=None):\n",
    "    \"\"\"\n",
    "    Exclude items based on a list of family categories, using a default list of families \n",
    "    unless additional families are provided by the user.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df_items : pandas.DataFrame\n",
    "        A DataFrame containing items. It should have at least two columns:\n",
    "        - 'item_nbr': The unique identifier for the item.\n",
    "        - 'family': The family or category the item belongs to.\n",
    "    \n",
    "    families : list of str, optional\n",
    "        A list of additional families to exclude. If provided, these families will be \n",
    "        added to the default list. If not provided, only the default list will be used.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    list\n",
    "        A list of item numbers (item_nbr) from the DataFrame that belong to the families \n",
    "        specified either by the default list or by the combined default and user-provided families.\n",
    "    \n",
    "    Notes:\n",
    "    -----\n",
    "    - The default families are:\n",
    "      ['BEVERAGES', 'PRODUCE', 'CELEBRATION', 'HOME AND KITCHEN I', 'HOME AND KITCHEN II',\n",
    "       'HOME CARE', 'LADIESWARE', 'PETS SUPPLIES', 'PLAYERS AND ELECTRONICS', 'SCHOOL AND OFFICE SUPPLIES']\n",
    "    \n",
    "    - If the `families` parameter is provided, the user-specified families will be appended \n",
    "      to the default families list before filtering items.\n",
    "    \n",
    "    Examples:\n",
    "    --------\n",
    "    1. Using only the default families:\n",
    "    \n",
    "        excluded_items = items_exclude_family(df_items)\n",
    "    \n",
    "    2. Adding new families to the exclusion list:\n",
    "    \n",
    "        excluded_items = items_exclude_family(df_items, families=['SNACKS', 'CLOTHING'])\n",
    "    \"\"\"\n",
    "\n",
    "    # Default families list\n",
    "    default_families = ['BEVERAGES', \n",
    "                        'PRODUCE', \n",
    "                        'CELEBRATION', \n",
    "                        'HOME AND KITCHEN I', \n",
    "                        'HOME AND KITCHEN II',\n",
    "                        'HOME CARE', 'LADIESWARE',\n",
    "                        'PETS SUPPLIES', \n",
    "                        'PLAYERS AND ELECTRONICS',\n",
    "                        'SCHOOL AND OFFICE SUPPLIES'\n",
    "                        ]\n",
    "    \n",
    "    # If the user provides additional families, append them to the default list\n",
    "    if families is not None:\n",
    "        default_families.extend(families)\n",
    "    \n",
    "    # Get the list of item numbers that belong to the combined list of families\n",
    "    list_items_in_families = df_items[df_items[\"item_family\"].isin(default_families)][\"item_nbr\"].tolist()\n",
    "\n",
    "    return list_items_in_families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sales_cleaned_items(df_sales, df_items, df_sales_funnel, families = None):\n",
    " \n",
    "    list_items_in_families = items_exclude_family(df_items, families)\n",
    "\n",
    "    # Funneling information step 3\n",
    "    df_sales_funnel['Exclude_items_family'] = df_sales_funnel['item_nbr'].isin(list_items_in_families).astype(int)   \n",
    "\n",
    "    # Drop observations from df_sales where item_nbr is in list_items_in_families\n",
    "    df_sales = df_sales[~df_sales[\"item_nbr\"].isin(list_items_in_families)]\n",
    "\n",
    "    return df_sales, df_sales_funnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sales_funnel_examineexclusions(df_sales_funnel, start_date_dataset, scoping_end_date, sales_funnel_total_unit_sales):  \n",
    "\n",
    "    # PART 1 CALCULATE THE STEPS FOR THE FULL TIMERANGE\n",
    "\n",
    "    # Step 1: Create dataframes per exclusion step\n",
    "    df_sales_funnel_exclusion_stores_days = df_sales_funnel[df_sales_funnel['Exclude_stores_days'] == 1]\n",
    "    df_sales_funnel_exclusion_stores_cluster = df_sales_funnel[df_sales_funnel['Exclude_stores_cluster'] == 1]\n",
    "    df_sales_funnel_exclusion_items_family = df_sales_funnel[df_sales_funnel['Exclude_items_family'] == 1]\n",
    "\n",
    "    #  Step 2: Exclusion of stores_days and of stores_cluster\n",
    "    df_sales_funnel_exclusion_step2 = df_sales_funnel[\n",
    "        (df_sales_funnel['Exclude_stores_days'] == 1)|\n",
    "        (df_sales_funnel['Exclude_stores_cluster'] == 1)\n",
    "    ]\n",
    "\n",
    "    # Step 3: Exclusion of all 3 steps, we calculate this via one notebook (not adding up) as there stores that might be in cluster 10 and are also excluded in step 1\n",
    "    df_sales_funnel_exclusion_step3 = df_sales_funnel[\n",
    "        (df_sales_funnel['Exclude_stores_days'] == 1)|\n",
    "        (df_sales_funnel['Exclude_stores_cluster'] == 1)|\n",
    "        df_sales_funnel['Exclude_items_family'] == 1\n",
    "    ]\n",
    "\n",
    "    # Step 4: Make scalars for each individual exclusion step as well as cumulative steps and return the unit_sales\n",
    "    df_sales_funnel_exclusion_stores_days_total_unit_sales = df_sales_funnel_exclusion_stores_days['unit_sales'].sum()\n",
    "    df_sales_funnel_exclusion_stores_cluster_total_unit_sales = df_sales_funnel_exclusion_stores_cluster['unit_sales'].sum()\n",
    "    df_sales_funnel_exclusion_items_family_total_unit_sales = df_sales_funnel_exclusion_items_family['unit_sales'].sum()\n",
    "    df_sales_funnel_exclusion_step2_total_unit_sales = df_sales_funnel_exclusion_step2['unit_sales'].sum()\n",
    "    df_sales_funnel_exclusion_step3_total_unit_sales = df_sales_funnel_exclusion_step3['unit_sales'].sum()\n",
    "\n",
    "    # Step 5: Make scalars for each individual exclusion step as well as cumulative steps and return the unit_sales_percentage\n",
    "    df_sales_funnel_total_exclusion_stores_days_percentage = df_sales_funnel_exclusion_stores_days['unit_sales_percentage'].sum()\n",
    "    df_sales_funnel_total_exclusion_stores_cluster_percentage = df_sales_funnel_exclusion_stores_cluster['unit_sales_percentage'].sum()\n",
    "    df_sales_funnel_total_exclusion_items_family_percentage = df_sales_funnel_exclusion_items_family['unit_sales_percentage'].sum()\n",
    "    df_sales_funnel_total_exclusion_step2_percentage = df_sales_funnel_exclusion_step2['unit_sales_percentage'].sum()\n",
    "    df_sales_funnel_total_exclusion_step3_percentage = df_sales_funnel_exclusion_step3['unit_sales_percentage'].sum()\n",
    "\n",
    "    # PART 2 CALCULATE THE STEPS FOR THE LAST FULL MONTH IN THE DATASET\n",
    "\n",
    "    # Step 1: Find the last date in the dataset\n",
    "    last_date = df_sales_funnel['date'].max()\n",
    "\n",
    "    # Step 2: Calculate the start and end of the last full month\n",
    "    last_full_month_end = last_date.replace(day=1) - pd.Timedelta(days=1)  # Last day of the previous month\n",
    "    last_full_month_start = last_full_month_end.replace(day=1)  # First day of the previous month\n",
    "\n",
    "    # Step 3: Filter the dataset for the last full month and adjust percentages\n",
    "    df_sales_funnel_last_full_month = df_sales_funnel[\n",
    "        (df_sales_funnel['date'] >= last_full_month_start) &\n",
    "        (df_sales_funnel['date'] <= last_full_month_end)\n",
    "    ]\n",
    "\n",
    "    df_sales_funnel_last_full_month_total_unit_sales = df_sales_funnel_last_full_month['unit_sales'].sum()\n",
    "    df_sales_funnel_last_full_month['unit_sales_percentage'] = (df_sales_funnel_last_full_month['unit_sales'] / df_sales_funnel_last_full_month_total_unit_sales) * 100\n",
    "\n",
    "    # Step 4: Create dataframes per exclusion step\n",
    "    df_sales_funnel_last_full_month_exclusion_stores_days = df_sales_funnel_last_full_month[df_sales_funnel_last_full_month['Exclude_stores_days'] == 1]\n",
    "    df_sales_funnel_last_full_month_exclusion_stores_cluster = df_sales_funnel_last_full_month[df_sales_funnel_last_full_month['Exclude_stores_cluster'] == 1]\n",
    "    df_sales_funnel_last_full_month_exclusion_items_family = df_sales_funnel_last_full_month[df_sales_funnel_last_full_month['Exclude_items_family'] == 1]\n",
    "\n",
    "    # Step 5:  Exclusion of stores_days and of stores_cluster\n",
    "    df_sales_funnel_last_full_month_exclusion_step2 = df_sales_funnel_last_full_month[\n",
    "        (df_sales_funnel_last_full_month['Exclude_stores_days'] == 1)|\n",
    "        (df_sales_funnel_last_full_month['Exclude_stores_cluster'] == 1)\n",
    "    ]\n",
    "\n",
    "    # Step 6: Exclusion of all 3 steps, we calculate this via one notebook (not adding up) as there stores that might be in cluster 10 and are also excluded in step 1\n",
    "    df_sales_funnel_last_full_month_exclusion_step3 = df_sales_funnel_last_full_month[\n",
    "        (df_sales_funnel_last_full_month['Exclude_stores_days'] == 1)|\n",
    "        (df_sales_funnel_last_full_month['Exclude_stores_cluster'] == 1)|\n",
    "        df_sales_funnel_last_full_month['Exclude_items_family'] == 1\n",
    "    ]\n",
    "\n",
    "    # Step 4: Make scalars for each individual exclusion step as well as cumulative steps and return the unit_sales\n",
    "    df_sales_funnel_last_full_month_exclusion_stores_days_total_unit_sales = df_sales_funnel_last_full_month_exclusion_stores_days['unit_sales'].sum()\n",
    "    df_sales_funnel_last_full_month_exclusion_stores_cluster_total_unit_sales = df_sales_funnel_last_full_month_exclusion_stores_cluster['unit_sales'].sum()\n",
    "    df_sales_funnel_last_full_month_exclusion_items_family_total_unit_sales = df_sales_funnel_last_full_month_exclusion_items_family['unit_sales'].sum()\n",
    "    df_sales_funnel_last_full_month_exclusion_step2_total_unit_sales = df_sales_funnel_last_full_month_exclusion_step2['unit_sales'].sum()\n",
    "    df_sales_funnel_last_full_month_exclusion_step3_total_unit_sales = df_sales_funnel_last_full_month_exclusion_step3['unit_sales'].sum()\n",
    "\n",
    "    # Step 5: Make scalars for each individual exclusion step as well as cumulative steps and return the unit_sales_percentage\n",
    "    df_sales_funnel_last_full_month_exclusion_stores_days_percentage = df_sales_funnel_last_full_month_exclusion_stores_days['unit_sales_percentage'].sum()\n",
    "    df_sales_funnel_last_full_month_exclusion_stores_cluster_percentage = df_sales_funnel_last_full_month_exclusion_stores_cluster['unit_sales_percentage'].sum()\n",
    "    df_sales_funnel_last_full_month_exclusion_items_family_percentage = df_sales_funnel_last_full_month_exclusion_items_family['unit_sales_percentage'].sum()\n",
    "    df_sales_funnel_last_full_month_exclusion_step2_percentage = df_sales_funnel_last_full_month_exclusion_step2['unit_sales_percentage'].sum()\n",
    "    df_sales_funnel_last_full_month_exclusion_step3_percentage = df_sales_funnel_last_full_month_exclusion_step3['unit_sales_percentage'].sum()\n",
    "\n",
    "\n",
    "    df_sales_funnel_step1effect = df_sales_funnel_exclusion_stores_days.groupby('store_nbr')[['unit_sales','unit_sales_percentage']].sum()\n",
    "    df_sales_funnel_step1effect[['unit_sales','unit_sales_percentage']] = df_sales_funnel_step1effect[['unit_sales','unit_sales_percentage']].round(2)\n",
    "    df_sales_funnel_step1effect = df_sales_funnel_step1effect.reset_index()\n",
    "\n",
    "    df_sales_funnel_step2effect = df_sales_funnel_exclusion_stores_cluster.groupby('store_nbr')[['unit_sales','unit_sales_percentage']].sum()\n",
    "    df_sales_funnel_step2effect[['unit_sales','unit_sales_percentage']] = df_sales_funnel_step2effect[['unit_sales','unit_sales_percentage']].round(2)\n",
    "    df_sales_funnel_step2effect  = df_sales_funnel_step2effect.reset_index()\n",
    "\n",
    "    df_sales_funnel_step3effect = df_sales_funnel_exclusion_items_family.groupby('family', observed= True)[['unit_sales', 'unit_sales_percentage']].sum()\n",
    "    df_sales_funnel_step3effect[['unit_sales','unit_sales_percentage']] = df_sales_funnel_step3effect[['unit_sales','unit_sales_percentage']].round(2)\n",
    "    df_sales_funnel_step3effect = df_sales_funnel_step3effect.reset_index()\n",
    "\n",
    "    df_sales_funnel_last_full_month_step1effect = df_sales_funnel_last_full_month_exclusion_stores_days.groupby('store_nbr')[['unit_sales','unit_sales_percentage']].sum()\n",
    "    df_sales_funnel_last_full_month_step1effect[['unit_sales','unit_sales_percentage']] = df_sales_funnel_last_full_month_step1effect[['unit_sales','unit_sales_percentage']].round(2)\n",
    "    df_sales_funnel_last_full_month_step1effect = df_sales_funnel_last_full_month_step1effect.reset_index()\n",
    "\n",
    "    df_sales_funnel_last_full_month_step2effect = df_sales_funnel_last_full_month_exclusion_stores_cluster.groupby('store_nbr')[['unit_sales','unit_sales_percentage']].sum()\n",
    "    df_sales_funnel_last_full_month_step2effect[['unit_sales','unit_sales_percentage']] = df_sales_funnel_last_full_month_step2effect[['unit_sales','unit_sales_percentage']].round(2)\n",
    "    df_sales_funnel_last_full_month_step2effect  = df_sales_funnel_last_full_month_step2effect.reset_index()\n",
    "\n",
    "    df_sales_funnel_last_full_month_step3effect = df_sales_funnel_last_full_month_exclusion_items_family.groupby('family', observed= True)[['unit_sales', 'unit_sales_percentage']].sum()\n",
    "    df_sales_funnel_last_full_month_step3effect[['unit_sales','unit_sales_percentage']] = df_sales_funnel_last_full_month_step3effect[['unit_sales','unit_sales_percentage']].round(2)\n",
    "    df_sales_funnel_last_full_month_step3effect = df_sales_funnel_last_full_month_step3effect.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "    # PRINT OUT FOR PART 1 FULL TIMERANGE\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print('We will now check for the impact of the exclusion functions on our initial dataset, we will do this both the whole timerange as well as the last full month')\n",
    "    print('Check 1-Effect on full timerange')\n",
    "    print()\n",
    "    print(f\"Total timerange  - The unit sales from {start_date_dataset} to {scoping_end_date} is {int(sales_funnel_total_unit_sales):,}. This is 100% without excluding data\".replace(',','.'))\n",
    "    print(f\"                   The total number of rows in the df_sales dataset without any manipulation is {df_sales_funnel.shape[0]:,}\".replace(',','.'))\n",
    "    print()\n",
    "    print(f'Exclusion step 1 - Excluding stores that do not have sales at the start of the last two years selected for the train, test and split')\n",
    "    print(f'                   The effect of exclusion 1 is a reduction in rows of {df_sales_funnel_exclusion_stores_days.shape[0]:,} leaving us with {df_sales_funnel.shape[0] - df_sales_funnel_exclusion_stores_days.shape[0]:,} rows'.replace(',','.'))\n",
    "    print(f'                   The effect of exclusion 1 is a reduction in unit sales of {int(df_sales_funnel_exclusion_stores_days_total_unit_sales):,} leaving us with {int(sales_funnel_total_unit_sales - df_sales_funnel_exclusion_stores_days_total_unit_sales):,} '.replace(',','.'))\n",
    "    print(f'                   The effect of exclusion 1 is a reduction of unit sales of {df_sales_funnel_total_exclusion_stores_days_percentage:.2f}% leaving us with {100-df_sales_funnel_total_exclusion_stores_days_percentage:.2f}%.')\n",
    "    print()\n",
    "    print(f'Exclusion step 2 - Excluding stores that belong to cluster 10')\n",
    "    print(f'                   The effect of exclusion 2 is a reduction in rows of {df_sales_funnel_exclusion_step2.shape[0] - df_sales_funnel_exclusion_stores_days.shape[0]:,}. The individual reduction without overlapping effects from step 2 would be {df_sales_funnel_exclusion_stores_cluster.shape[0]:,},  leaving us with {df_sales_funnel.shape[0] - df_sales_funnel_exclusion_step2.shape[0]:,} rows'.replace(',','.'))\n",
    "    print(f'                   The effect of exclusion 2 is a reduction in unit sales of {int(df_sales_funnel_exclusion_step2_total_unit_sales - df_sales_funnel_exclusion_stores_days_total_unit_sales):,}. The individual reduction without overlapping effects from step 1 would be {int(df_sales_funnel_exclusion_stores_cluster_total_unit_sales):,} leaving us with {int(sales_funnel_total_unit_sales - df_sales_funnel_exclusion_step2_total_unit_sales):,} '.replace(',','.'))\n",
    "    print(f'                   The effect of exclusion 2 is a reduction of unit sales of {df_sales_funnel_total_exclusion_step2_percentage - df_sales_funnel_total_exclusion_stores_days_percentage:.2f}%, the individual reduction without overlapping effects from step 1 would be {df_sales_funnel_total_exclusion_stores_cluster_percentage:.2f}%, leaving us with {100-df_sales_funnel_total_exclusion_step2_percentage:.2f}%.')\n",
    "    print()\n",
    "    print(f'Exclusion step 3 - Excluding items that are including in the families selected for exclusion')\n",
    "    print(f'                   The effect of exclusion 3 is a reduction in rows of {df_sales_funnel_exclusion_step3.shape[0] - df_sales_funnel_exclusion_step2.shape[0]:,}. The individual reduction without overlapping effects from step 3 would be {df_sales_funnel_exclusion_items_family.shape[0]:,} leaving us with {df_sales_funnel.shape[0] - df_sales_funnel_exclusion_step3.shape[0]:,} rows'.replace(',','.'))\n",
    "    print(f'                   The effect of exclusion 3 is a reduction in unit sales of {int(df_sales_funnel_exclusion_step3_total_unit_sales - df_sales_funnel_exclusion_step2_total_unit_sales):,}. The individual reduction without overlapping effects from step 1 would be {int(df_sales_funnel_exclusion_items_family_total_unit_sales):,} leaving us with {int(sales_funnel_total_unit_sales - df_sales_funnel_exclusion_step3_total_unit_sales):,} '.replace(',','.'))\n",
    "    print(f'                   The effect of exclusion 3 is a reduction of unit sales of {df_sales_funnel_total_exclusion_step3_percentage - df_sales_funnel_total_exclusion_step2_percentage:.2f}%, the individual reduction without overlapping effects from step 1 would be {df_sales_funnel_total_exclusion_items_family_percentage:.2f}%, leaving us with {100-df_sales_funnel_total_exclusion_step3_percentage:.2f}%.')\n",
    "    print()\n",
    "    print('Printing out the individual effects within each exclusion step')\n",
    "    print(df_sales_funnel_step1effect)\n",
    "    print(df_sales_funnel_step2effect)\n",
    "    print(df_sales_funnel_step3effect)\n",
    "    print(\"-\" * 50)\n",
    "    print('Check 2-Effect on last full month in the dataset')\n",
    "    print(f\"                   The last full month in the dataset is {last_full_month_start.strftime('%B %Y')}\")\n",
    "    print()\n",
    "    print(f\"Total timerange  - The unit sales from {last_full_month_start} to {last_full_month_end} is {int(df_sales_funnel_last_full_month_total_unit_sales):,}. This is 100% without excluding data\".replace(',','.'))\n",
    "    print(f\"                   The total number of rows in the df_sales dataset without any manipulation is {df_sales_funnel_last_full_month.shape[0]:,}\".replace(',','.'))\n",
    "    print()\n",
    "    print(f'Exclusion step 1 - Excluding stores that do not have sales at the start of the last two years selected for the train, test and split')\n",
    "    print(f'                   The effect of exclusion 1 is a reduction in rows of {df_sales_funnel_last_full_month_exclusion_stores_days.shape[0]:,} leaving us with {df_sales_funnel_last_full_month.shape[0] - df_sales_funnel_last_full_month_exclusion_stores_days.shape[0]:,} rows'.replace(',','.'))\n",
    "    print(f'                   The effect of exclusion 1 is a reduction in unit sales of {int(df_sales_funnel_last_full_month_exclusion_stores_days_total_unit_sales):,} leaving us with {int(df_sales_funnel_last_full_month_total_unit_sales - df_sales_funnel_last_full_month_exclusion_stores_days_total_unit_sales):,} '.replace(',','.'))\n",
    "    print(f'                   The effect of exclusion 1 is a reduction of unit sales of {df_sales_funnel_last_full_month_exclusion_stores_days_percentage:.2f}% leaving us with {100-df_sales_funnel_last_full_month_exclusion_stores_days_percentage:.2f}%.')\n",
    "    print()\n",
    "    print(f'Exclusion step 2 - Excluding stores that belong to cluster 10')\n",
    "    print(f'                   The effect of exclusion 2 is a reduction in rows of {df_sales_funnel_last_full_month_exclusion_step2.shape[0] - df_sales_funnel_last_full_month_exclusion_stores_days.shape[0]:,}. The individual reduction without overlapping effects from step 2 would be {df_sales_funnel_last_full_month_exclusion_stores_cluster.shape[0]:,},  leaving us with {df_sales_funnel_last_full_month.shape[0] - df_sales_funnel_last_full_month_exclusion_step2.shape[0]:,} rows'.replace(',','.'))\n",
    "    print(f'                   The effect of exclusion 2 is a reduction in unit sales of {int(df_sales_funnel_last_full_month_exclusion_step2_total_unit_sales - df_sales_funnel_last_full_month_exclusion_stores_days_total_unit_sales):,}. The individual reduction without overlapping effects from step 1 would be {int(df_sales_funnel_last_full_month_exclusion_stores_cluster_total_unit_sales):,} leaving us with {int(df_sales_funnel_last_full_month_total_unit_sales - df_sales_funnel_last_full_month_exclusion_step2_total_unit_sales):,} '.replace(',','.'))\n",
    "    print(f'                   The effect of exclusion 2 is a reduction of unit sales of {df_sales_funnel_last_full_month_exclusion_step2_percentage - df_sales_funnel_last_full_month_exclusion_stores_days_percentage:.2f}%, the individual reduction without overlapping effects from step 1 would be {df_sales_funnel_last_full_month_exclusion_stores_cluster_percentage:.2f}%, leaving us with {100-df_sales_funnel_last_full_month_exclusion_step2_percentage:.2f}%.')\n",
    "    print()\n",
    "    print(f'Exclusion step 3 - Excluding items that are including in the families selected for exclusion')\n",
    "    print(f'                   The effect of exclusion 3 is a reduction in rows of {df_sales_funnel_last_full_month_exclusion_step3.shape[0] - df_sales_funnel_last_full_month_exclusion_step2.shape[0]:,}. The individual reduction without overlapping effects from step 3 would be {df_sales_funnel_last_full_month_exclusion_items_family.shape[0]:,} leaving us with {df_sales_funnel_last_full_month.shape[0] - df_sales_funnel_last_full_month_exclusion_step3.shape[0]:,} rows'.replace(',','.'))\n",
    "    print(f'                   The effect of exclusion 3 is a reduction in unit sales of {int(df_sales_funnel_last_full_month_exclusion_step3_total_unit_sales - df_sales_funnel_last_full_month_exclusion_step2_total_unit_sales):,}. The individual reduction without overlapping effects from step 1 would be {int(df_sales_funnel_last_full_month_exclusion_items_family_total_unit_sales):,} leaving us with {int(df_sales_funnel_last_full_month_total_unit_sales - df_sales_funnel_last_full_month_exclusion_step3_total_unit_sales):,} '.replace(',','.'))\n",
    "    print(f'                   The effect of exclusion 3 is a reduction of unit sales of {df_sales_funnel_last_full_month_exclusion_step3_percentage - df_sales_funnel_last_full_month_exclusion_step2_percentage:.2f}%, the individual reduction without overlapping effects from step 1 would be {df_sales_funnel_last_full_month_exclusion_items_family_percentage:.2f}%, leaving us with {100-df_sales_funnel_last_full_month_exclusion_step3_percentage:.2f}%.')\n",
    "    print()\n",
    "    print('Printing out the individual effects within each exclusion step')\n",
    "    print(df_sales_funnel_last_full_month_step1effect)\n",
    "    print(df_sales_funnel_last_full_month_step2effect)\n",
    "    print(df_sales_funnel_last_full_month_step3effect)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Function - Exclude holiday event related to the \"Terromoto\" volcano event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.1. Function - Create dataframe based on df_holidays with only events containing \"Terremoto Manabi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holiday_filter_vulcano_event(df_holidays, event_substring=\"Terremoto Manabi\"):\n",
    "\n",
    "    # Filter the DataFrame where 'description' contains the event_substring\n",
    "    df_vulcano_event_filtered = df_holidays[\n",
    "        df_holidays[\"description\"].str.contains(event_substring)\n",
    "    ]\n",
    "\n",
    "    return df_vulcano_event_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.2. Function - Exclude the \"Terremoto Manabi\" from the df_holidays dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_holidays_cleaned(df_holidays):\n",
    "\n",
    "    # Exclude holiday_filter_vulcano_event function to return filtered df\n",
    "    df_vulcano_event_filtered = holiday_filter_vulcano_event(df_holidays)\n",
    "\n",
    "    # Filter the specific holiday events from the holiday DataFrame\n",
    "    df_holidays = df_holidays.loc[\n",
    "        ~df_holidays.index.isin(df_vulcano_event_filtered.index)\n",
    "    ]\n",
    "\n",
    "    return df_holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enriching datasets for further analysis (functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Function - Determining holidays per store\n",
    "The holidays dataset contains information on local, regional and national holidays. For each of these types, there is a different key/identifier that corresponds with the stores data found in df_stores (the raw data). To overcome this issue, three separate dataframes are made for each type of holiday where the data is merged (joined) with the stores dataframe. Thereafter, these dataframes are combined as to construct one big dataframe containing all the holidays per store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.1. Function - Make cleaned versions of the holidays and stores dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_holiday and df_stores by cleaning up df for merging with holidays by dropping unneeded columns\n",
    "def clean_holidays_stores_prep(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned = df_holidays.drop(\n",
    "        columns=[\n",
    "            \"description\",\n",
    "            \"transferred\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_stores_cleaned = df_stores.drop(columns=[\"cluster\", \"type\"])\n",
    "\n",
    "    return df_holidays_cleaned, df_stores_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.2. Function - Create a dataframe with all the local holidays per store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_local(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # select locale 'Local' from holiday df and merge with city stores df\n",
    "    df_holidays_local = df_holidays_cleaned[df_holidays_cleaned[\"locale\"] == \"Local\"]\n",
    "\n",
    "    df_holidays_prep_local = df_holidays_local.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"city\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.3. Function - Create a dataframe with all the regional holidays per store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_regional(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # select locale 'Regional' from holiday df and merge with state stores df\n",
    "    df_holidays_regional = df_holidays_cleaned[\n",
    "        df_holidays_cleaned[\"locale\"] == \"Regional\"\n",
    "    ]\n",
    "\n",
    "    df_holidays_prep_regional = df_holidays_regional.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"state\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_regional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.4. Function - Create a dataframe with all the national holidays per store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_national(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # Select locale 'Regional' from holiday df and merge with national stores df\n",
    "    df_holidays_national = df_holidays_cleaned[\n",
    "        df_holidays_cleaned[\"locale\"] == \"National\"\n",
    "    ]\n",
    "\n",
    "    # Create extra column for merge on \"Ecuador\"\n",
    "    df_stores_cleaned[\"national_merge\"] = \"Ecuador\"\n",
    "\n",
    "    df_holidays_prep_national = df_holidays_national.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"national_merge\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Drop newly created column national_merge, not needed further\n",
    "    df_holidays_prep_national = df_holidays_prep_national.drop(\n",
    "        columns=[\"national_merge\"]\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_national"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.5. Function - Create a dataframe that merges all the separate dataframe for each type of holiday and store combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_merged(df_holidays, df_stores):\n",
    "\n",
    "    # Load prep functions from local, Regional and National df's\n",
    "    df_holidays_prep_local = holidays_prep_local(df_holidays, df_stores)\n",
    "\n",
    "    df_holidays_prep_regional = holidays_prep_regional(df_holidays, df_stores)\n",
    "\n",
    "    df_holidays_prep_national = holidays_prep_national(df_holidays, df_stores)\n",
    "\n",
    "    # Combine local, regional and national dataframes into 1 merged dataframe\n",
    "    df_holidays_merged = pd.concat(\n",
    "        [df_holidays_prep_local, df_holidays_prep_regional, df_holidays_prep_national]\n",
    "    )\n",
    "\n",
    "    # Clean df_holidays_merged by dropping \"locale_name\", \"city\", \"state\"\n",
    "    df_holidays_merged = df_holidays_merged.drop(\n",
    "        columns=[\"locale_name\", \"city\", \"state\"]\n",
    "    )\n",
    "\n",
    "    # Rename 'type' of holiday to 'holiday_type'\n",
    "    df_holidays_merged = df_holidays_merged.rename(\n",
    "        columns={\"type\": \"holiday_type\", \"locale\": \"holiday_locale\"}\n",
    "    )\n",
    "\n",
    "    return df_holidays_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Function - Determining a count per type of holiday per store\n",
    "The dataframe resulting from the function described in 4.1. gives duplicate values because there sometimes are multiple holidays on one date. Duplicate values per date would result in multiple sales rows for each date, making it not workable. Therfore, we transform the holiday and stores combination to contain 3 columns (for each type of holiday, namely, local, regional and national) that count the amount of holidays found for a specific date. Thereby we create a unique list of date and store combinations for all the holidays within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1. Function - Creating unique combination of store and date with three count columns for each type of holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_merged_grouped(df_holidays, df_stores):\n",
    "\n",
    "    # Merge the holiday dataframes and clean the merged dataframe\n",
    "    df_holidays_merged = holidays_prep_merged(df_holidays, df_stores)\n",
    "\n",
    "    # Group by date and store_nbr and count the number of holidays per date per store\n",
    "    df_holidays_merged_grouped = df_holidays_merged.pivot_table(\n",
    "        index=[\"date\", \"store_nbr\"],\n",
    "        columns=\"holiday_locale\",\n",
    "        values=\"holiday_type\",\n",
    "        aggfunc=\"count\",\n",
    "        observed=True,\n",
    "    ).reset_index()\n",
    "\n",
    "    # Remove the name of the columns\n",
    "    df_holidays_merged_grouped.columns.name = None\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    df_holidays_merged_grouped = df_holidays_merged_grouped.fillna(0)\n",
    "\n",
    "    # Convert the count columns to Int8-dtype (note the capital 'I'). This dtype can handle null values, needed to prevent float64 from the merge in Step 6\n",
    "    # Rename the columns to holiday_local_count,  holiday_regional_count, holiday_national_count\n",
    "    df_holidays_merged_grouped = df_holidays_merged_grouped.astype(\n",
    "        {\"Local\": \"Int8\", \"Regional\": \"Int8\", \"National\": \"Int8\"}\n",
    "    ).rename(\n",
    "        columns={\n",
    "            \"Local\": \"holiday_local_count\",\n",
    "            \"Regional\": \"holiday_regional_count\",\n",
    "            \"National\": \"holiday_national_count\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Let's do an inner join with the original data to get the original date and store_nbr combinations back. Therefore we need to make another dataframe.\n",
    "    df_holidays_merged_grouped_inner = holidays_prep_merged(df_holidays, df_stores)\n",
    "    df_holidays_merged_grouped_inner = (\n",
    "        df_holidays_merged_grouped_inner.groupby([\"date\", \"store_nbr\"])\n",
    "        .size()\n",
    "        .reset_index()\n",
    "        .drop(columns=0)\n",
    "    )\n",
    "\n",
    "    df_holidays_merged_grouped = df_holidays_merged_grouped.merge(\n",
    "        df_holidays_merged_grouped_inner, on=[\"date\", \"store_nbr\"], how=\"inner\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"In the orignal unioned holiday dataframe, df_holidays_merged we found (including duplicates) {df_holidays_merged.shape[0]} rows\"\n",
    "    )\n",
    "    print(\n",
    "        f\"In our new adjusted dataframe we have {df_holidays_merged_grouped.shape[0]} rows\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Thus, we have removed {df_holidays_merged.shape[0] - df_holidays_merged_grouped.shape[0]} rows\"\n",
    "    )\n",
    "\n",
    "    # Might want to filter out the holiday dates that will never be in de salesdate range. However, they will be left out anyway when joining with the sales data.\n",
    "    return df_holidays_merged_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2. Function - Filling in NA values for each count column whenever no holiday could be found for a specific holiday date and store combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fill newly created NaN columns, due to holiday join, with 'no' on thates where there are now holidays\n",
    "def holidays_fill_zero_normal(df):\n",
    "    \"\"\"\n",
    "    Fills the NaN values with 0 for all columns \"holiday_local_count\", \"holiday_regional_count\", \"holiday_national_count\", in the combined dataframe.\n",
    "    It will only fill the columns that are in the original dataframe and not in the holiday dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    columns_to_fill = [\n",
    "        \"holiday_local_count\",\n",
    "        \"holiday_regional_count\",\n",
    "        \"holiday_national_count\",\n",
    "    ]\n",
    "\n",
    "    df[columns_to_fill] = df[columns_to_fill].fillna(0).astype(\"int8\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Function - Constructing a cartesian sales dataset for each store based on the maximum sales daterange\n",
    "The df_sales dataset contains unit sales data for each store but not all stores have data for each date. To overcome this and make sure each date is present for each store we construct a new dataframe based on the minimum- and maximum date found within the sales dataframe. The result is thus a sales dataframe with each date, store and item combination for the whole timerange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filling_dates_cartesian(df):\n",
    "\n",
    "    # Print first and last date of df\n",
    "    print(f'First date in df: {df[\"date\"].min()}')\n",
    "    print(f'Last date in df:  {df[\"date\"].max()}')\n",
    "\n",
    "    # Calculate memory size and shape size of start df\n",
    "    df_mem_start = sys.getsizeof(df)\n",
    "    df_shape_start = df.shape[0] / 1e6\n",
    "    print(\n",
    "        f\"Start size of df_sales:     {round(df_mem_start/1024/1024/1024, 2)} GB and start observations:     {round(df_shape_start, 1)} million.\"\n",
    "    )\n",
    "\n",
    "    # Create a complete date range for the entire dataset, it's a datetimeindex object\n",
    "    all_dates = pd.date_range(start=df[\"date\"].min(), end=df[\"date\"].max(), freq=\"D\")\n",
    "\n",
    "    # Create a multi-index from all possible combinations of 'item_nbr' and 'date'\n",
    "    all_combinations = pd.MultiIndex.from_product(\n",
    "        [df[\"store_nbr\"].unique(), df[\"item_nbr\"].unique(), all_dates],\n",
    "        names=[\"store_nbr\", \"item_nbr\", \"date\"],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"The multi-index (all_combinations of store, date and item) for the minimum and maximum dates found result in {round(all_combinations.shape[0]/1e6,1)} million rows, this is the amount of rows we expect in the final dataframe.\"\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "    # Check for duplicates in the combination of 'store_nbr', 'item_nbr', and 'date'\n",
    "    # This method is based on boolean indexing, when there's a true value for the duplicated method, it will return those rows to the duplicate_rows variable\n",
    "    duplicate_rows = df[\n",
    "        df.duplicated(subset=[\"store_nbr\", \"item_nbr\", \"date\"], keep=False)\n",
    "    ]\n",
    "    if not duplicate_rows.empty:\n",
    "        print(\n",
    "            \"Warning: Duplicate entries found in the combination of 'store_nbr', 'item_nbr', and 'date'.\"\n",
    "        )\n",
    "        print(f\"Total dublicate rows {duplicate_rows.shape[0]}\")\n",
    "        print(\"-\" * 71)\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Reindex the original DataFrame to include all combinations of 'store_nbr', 'item_nbr', and 'date'\n",
    "    df_reindexed = df.set_index([\"store_nbr\", \"item_nbr\", \"date\"]).reindex(\n",
    "        all_combinations\n",
    "    )\n",
    "\n",
    "    # Reset the index to turn the multi-index back into regular columns\n",
    "    df_sales_cartesian = df_reindexed.reset_index()\n",
    "\n",
    "    # Calculate memory size and shape size of final end df\n",
    "    df_mem_end = sys.getsizeof(df_sales_cartesian)\n",
    "    df_mem_change_perc = ((df_mem_end - df_mem_start) / df_mem_start) * 100\n",
    "    df_mem_change = df_mem_end - df_mem_start\n",
    "\n",
    "    df_shape_end = df_sales_cartesian.shape[0] / 1e6\n",
    "    df_shape_change_perc = ((df_shape_end - df_shape_start) / df_shape_start) * 100\n",
    "    df_shape_change = df_shape_end - df_shape_start\n",
    "\n",
    "    print(\n",
    "        f\"Final size of the dataframe is:     {round(df_mem_end/1024/1024/1024, 2)} GB and end observations:       {round(df_shape_end, 1)} million.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Change in size of the dataframe is: {round(df_mem_change_perc, 2)} % and observations:           {round(df_shape_change_perc, 2)}     %.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Increased size of the dataframe is: {round(df_mem_change/1024/1024/1024, 2)} GB and increased observations: {round(df_shape_change, 1)} million.\"\n",
    "    )\n",
    "\n",
    "    return df_sales_cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Constructing final dataset\n",
    "In this step all the datasets will be merged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "def merge_datasets(df_sales, df_items, df_stores, df_holidays):\n",
    "\n",
    "    # Basic information of loaded data\n",
    "    print(\n",
    "        \"Step 1 - Importing, downcasting and normalizing data and optimizing memory, the following data has been imported.\"\n",
    "    )\n",
    "    df_basic_info(df_sales, \"df_sales\")\n",
    "    print(\"\"\"\"\"\")\n",
    "    df_basic_info(df_items, \"df_items\")\n",
    "    print(\"\"\"\"\"\")\n",
    "    df_basic_info(df_stores, \"df_stores\")\n",
    "    print(\"\"\"\"\"\")\n",
    "    df_basic_info(df_holidays, \"df_holidays\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Sales prep\n",
    "    print(\n",
    "        \"Step 2 - Cleaning sales data and making a cartesian product of the sales data and the minimum and maximum dates found in the data.\"\n",
    "    )\n",
    "    df_sales = sales_cleaned(df_sales)\n",
    "    df_items = items_cleaned_renamed(df_items)\n",
    "    start_date_dataset, scoping_end_date, df_sales_funnel,sales_funnel_total_unit_sales = sales_funnel_construction(df_sales)\n",
    "    df_sales, df_sales_funnel = df_sales_cleaned_stores(df_sales, df_stores, df_sales_funnel)\n",
    "    df_sales, df_sales_funnel = df_sales_cleaned_items(df_sales, df_items, df_sales_funnel)\n",
    "    sales_funnel_examineexclusions(df_sales_funnel, start_date_dataset, scoping_end_date,sales_funnel_total_unit_sales) \n",
    "    df_sales_cartesian = filling_dates_cartesian(df_sales)\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Holidays prep\n",
    "    print(\n",
    "        \"Step 3 - Cleaning holiday data and counting the number of holidays per date per store for each type of holiday (national, regional, local).\"\n",
    "    )\n",
    "    df_holidays = df_holidays_cleaned(df_holidays)\n",
    "    df_holidays_merged_grouped = holidays_prep_merged_grouped(df_holidays, df_stores)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Stores prep\n",
    "    print(\n",
    "        \"Step 4 - Cleaning stores data (read: dropping unnecessary columns and renaming columns for clarity).\"\n",
    "    )\n",
    "    df_stores = stores_cleaned_renamed(df_stores)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Items prep\n",
    "    print(\n",
    "        \"Step 5 - Cleaning items data  (read: dropping unnecessary columns and renaming columns for clarity).\"\n",
    "    )\n",
    "    # df_items = items_cleaned_renamed(df_items)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Holidays merge on sales\n",
    "    print(\n",
    "        \"Step 6 - Adding holiday data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of three holiday columns.\"\n",
    "    )\n",
    "\n",
    "    df_merged = df_sales_cartesian.merge(\n",
    "        df_holidays_merged_grouped, on=[\"date\", \"store_nbr\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    df_merged = holidays_fill_zero_normal(df_merged)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Stores merged with sales+holidays\n",
    "    print(\n",
    "        \"Step 7 - Adding holiday data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of holiday columns.\"\n",
    "    )\n",
    "    df_merged = df_merged.merge(df_stores, on=\"store_nbr\", how=\"left\")\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Change the dtype for item_nbr from uint32 to int32, during testing we found that the merge was not working properly with uint32\n",
    "    # df_merged[\"item_nbr\"] = df_merged[\"item_nbr\"].astype(int)\n",
    "    # df_items[\"item_nbr\"] = df_items[\"item_nbr\"].astype(int)\n",
    "\n",
    "    # Items merged with sales+holidays+stores\n",
    "    print(\n",
    "        \"Step 8 - Adding items data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of holiday columns. Remember, in our last step we added a lot of store information as well\"\n",
    "    )\n",
    "    df_final = df_merged.merge(df_items, on=\"item_nbr\", how=\"left\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Print some referential integrity checks to make sure we have the same amount of rows\n",
    "    print(\n",
    "        f\"The amount of rows in the sales dataframe was {df_sales.shape[0] /1_000_000:.2f} million.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"After making a cartesian product with date, store and item we had a total of {df_sales_cartesian.shape[0]/1_000_000:.2f} million rows.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"After mergin with the holidays, stores, and items we have {df_final.shape[0]/1_000_000:.2f} million rows\"\n",
    "    )\n",
    "    print(\n",
    "        f\"The difference between the incoming and outgoing data from this function is {df_sales.shape[0] - df_final.shape[0]} rows\"\n",
    "    )\n",
    "    print(\n",
    "        f'If we compare the outgoing dataframe called \"df_final\" with the cartesian product of sales data and dates we see that the difference is {df_sales_cartesian.shape[0] - df_final.shape[0]} rows'\n",
    "    )\n",
    "    print(\n",
    "        f\"If the difference is 0, we have a perfect match and we can continue with the next steps.\"\n",
    "    )\n",
    "\n",
    "    # f\"Final size of the dataframe is:     {round(df_mem_end/1024/1024/1024, 2)} GB and end observations:       {round(df_shape_end, 1)} million.\"\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sales = df_sales[(df_sales[\"store_nbr\"] == 1)]\n",
    "\n",
    "# df_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Importing, downcasting and normalizing data and optimizing memory, the following data has been imported.\n",
      "The 'df_sales' dataframe contains: 125.497.040 observations and 8 features.\n",
      "After optimizing by downcasting and normalizing it has optimized size of    2.1 GB.\n",
      "\n",
      "The 'df_items' dataframe contains: 4.100 observations and 4 features.\n",
      "After optimizing by downcasting and normalizing it has optimized size of    0.0 GB.\n",
      "\n",
      "The 'df_stores' dataframe contains: 54 observations and 5 features.\n",
      "After optimizing by downcasting and normalizing it has optimized size of    0.0 GB.\n",
      "\n",
      "The 'df_holidays' dataframe contains: 350 observations and 6 features.\n",
      "After optimizing by downcasting and normalizing it has optimized size of    0.0 GB.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 2 - Cleaning sales data and making a cartesian product of the sales data and the minimum and maximum dates found in the data.\n",
      "The first date in the sales dataset found is: 2013-01-01 00:00:00\n",
      "The last date in the sales dataset found is:  2017-08-15 00:00:00\n",
      "Subtracting two years for train/test/split we now search for stores that did not have any sales before:  2015-08-15 00:00:00\n",
      "We assume these stores do not meet the condition in that they do not have sales for the full last two years, they will atleast at the beginning of this period miss data and thus are not good candidates\n",
      "Stores [22, 42, 52] were excluded from our dataset based on no sales prior to the used range within the dataset for forecasting\n",
      "The maximum number of datapoints found within the 2 years set used for forecasting is 728.0, the parameter in the function is set to None, if filled, this will be used as the cut-off value for our 2 year dataset\n",
      "Stores [17, 18, 22, 25, 42, 43, 52] were excluded from our dataset based on missing to much dates with sales within the dataset needed for forecasting\n",
      "Stores [26, 28, 29, 31, 36, 43] were excluded from our dataset based on beloning to cluster 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_6100\\460683071.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sales_funnel_last_full_month['unit_sales_percentage'] = (df_sales_funnel_last_full_month['unit_sales'] / df_sales_funnel_last_full_month_total_unit_sales) * 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "We will now check for the impact of the exclusion functions on our initial dataset, we will do this both the whole timerange as well as the last full month\n",
      "Check 1-Effect on full timerange\n",
      "\n",
      "Total timerange  - The unit sales from 2013-01-01 00:00:00 to 2017-08-15 00:00:00 is 1.073.612.032. This is 100% without excluding data\n",
      "                   The total number of rows in the df_sales dataset without any manipulation is 125.497.040\n",
      "\n",
      "Exclusion step 1 - Excluding stores that do not have sales at the start of the last two years selected for the train, test and split\n",
      "                   The effect of exclusion 1 is a reduction in rows of 11.033.163 leaving us with 114.463.877 rows\n",
      "                   The effect of exclusion 1 is a reduction in unit sales of 74.822.848 leaving us with 998.789.184 \n",
      "                   The effect of exclusion 1 is a reduction of unit sales of 6.97% leaving us with 93.03%.\n",
      "\n",
      "Exclusion step 2 - Excluding stores that belong to cluster 10\n",
      "                   The effect of exclusion 2 is a reduction in rows of 10.293.743. The individual reduction without overlapping effects from step 2 would be 12.233.437.  leaving us with 104.170.134 rows\n",
      "                   The effect of exclusion 2 is a reduction in unit sales of 68.930.736. The individual reduction without overlapping effects from step 1 would be 85.322.416 leaving us with 929.858.432 \n",
      "                   The effect of exclusion 2 is a reduction of unit sales of 6.42%, the individual reduction without overlapping effects from step 1 would be 7.95%, leaving us with 86.61%.\n",
      "\n",
      "Exclusion step 3 - Excluding items that are including in the families selected for exclusion\n",
      "                   The effect of exclusion 3 is a reduction in rows of 24.252.778. The individual reduction without overlapping effects from step 3 would be 29.316.069 leaving us with 79.917.356 rows\n",
      "                   The effect of exclusion 3 is a reduction in unit sales of 312.248.448. The individual reduction without overlapping effects from step 1 would be 360.643.712 leaving us with 617.609.984 \n",
      "                   The effect of exclusion 3 is a reduction of unit sales of 29.08%, the individual reduction without overlapping effects from step 1 would be 33.59%, leaving us with 57.53%.\n",
      "\n",
      "Printing out the individual effects within each exclusion step\n",
      "   store_nbr  unit_sales  unit_sales_percentage\n",
      "0         17  18030968.0                   1.68\n",
      "1         18  13247101.0                   1.23\n",
      "2         22   4090201.5                   0.38\n",
      "3         25  11420996.0                   1.06\n",
      "4         42   8945768.0                   0.83\n",
      "5         43  16391689.0                   1.53\n",
      "6         52   2696169.5                   0.25\n",
      "   store_nbr  unit_sales  unit_sales_percentage\n",
      "0         26   7755104.0                   0.72\n",
      "1         28  18383154.0                   1.71\n",
      "2         29   9723158.0                   0.91\n",
      "3         31  17746828.0                   1.65\n",
      "4         36  15322424.0                   1.43\n",
      "5         43  16391689.0                   1.53\n",
      "                       family    unit_sales  unit_sales_percentage\n",
      "0                   BEVERAGES  2.169422e+08              20.209999\n",
      "1                 CELEBRATION  7.611329e+05               0.070000\n",
      "2          HOME AND KITCHEN I  1.861229e+06               0.170000\n",
      "3         HOME AND KITCHEN II  1.520557e+06               0.140000\n",
      "4                   HOME CARE  1.602214e+07               1.490000\n",
      "5     PLAYERS AND ELECTRONICS  5.625790e+05               0.050000\n",
      "6                     PRODUCE  1.227047e+08              11.430000\n",
      "7  SCHOOL AND OFFICE SUPPLIES  2.693140e+05               0.030000\n",
      "--------------------------------------------------\n",
      "Check 2-Effect on last full month in the dataset\n",
      "                   The last full month in the dataset is July 2017\n",
      "\n",
      "Total timerange  - The unit sales from 2017-07-01 00:00:00 to 2017-07-31 00:00:00 is 27.011.472. This is 100% without excluding data\n",
      "                   The total number of rows in the df_sales dataset without any manipulation is 3.283.161\n",
      "\n",
      "Exclusion step 1 - Excluding stores that do not have sales at the start of the last two years selected for the train, test and split\n",
      "                   The effect of exclusion 1 is a reduction in rows of 411.984 leaving us with 2.871.177 rows\n",
      "                   The effect of exclusion 1 is a reduction in unit sales of 2.889.866 leaving us with 24.121.606 \n",
      "                   The effect of exclusion 1 is a reduction of unit sales of 10.70% leaving us with 89.30%.\n",
      "\n",
      "Exclusion step 2 - Excluding stores that belong to cluster 10\n",
      "                   The effect of exclusion 2 is a reduction in rows of 274.198. The individual reduction without overlapping effects from step 2 would be 332.344.  leaving us with 2.596.979 rows\n",
      "                   The effect of exclusion 2 is a reduction in unit sales of 1.841.925. The individual reduction without overlapping effects from step 1 would be 2.346.902 leaving us with 22.279.680 \n",
      "                   The effect of exclusion 2 is a reduction of unit sales of 6.82%, the individual reduction without overlapping effects from step 1 would be 8.69%, leaving us with 82.48%.\n",
      "\n",
      "Exclusion step 3 - Excluding items that are including in the families selected for exclusion\n",
      "                   The effect of exclusion 3 is a reduction in rows of 741.356. The individual reduction without overlapping effects from step 3 would be 934.283 leaving us with 1.855.623 rows\n",
      "                   The effect of exclusion 3 is a reduction in unit sales of 9.016.456. The individual reduction without overlapping effects from step 1 would be 10.928.399 leaving us with 13.263.224 \n",
      "                   The effect of exclusion 3 is a reduction of unit sales of 33.38%, the individual reduction without overlapping effects from step 1 would be 40.46%, leaving us with 49.10%.\n",
      "\n",
      "Printing out the individual effects within each exclusion step\n",
      "   store_nbr    unit_sales  unit_sales_percentage\n",
      "0         17  444951.25000                   1.65\n",
      "1         18  332888.46875                   1.23\n",
      "2         22  222510.81250                   0.82\n",
      "3         25  246307.71875                   0.91\n",
      "4         42  418542.62500                   1.55\n",
      "5         43  504976.43750                   1.87\n",
      "6         52  719688.93750                   2.66\n",
      "   store_nbr     unit_sales  unit_sales_percentage\n",
      "0         26  164361.765625                   0.61\n",
      "1         28  473918.625000                   1.75\n",
      "2         29  396443.375000                   1.47\n",
      "3         31  447265.187500                   1.66\n",
      "4         36  359936.968750                   1.33\n",
      "5         43  504976.437500                   1.87\n",
      "                       family  unit_sales  unit_sales_percentage\n",
      "0                   BEVERAGES  6204055.00              22.969999\n",
      "1                 CELEBRATION    22720.00               0.080000\n",
      "2          HOME AND KITCHEN I    52719.00               0.200000\n",
      "3         HOME AND KITCHEN II    51493.00               0.190000\n",
      "4                   HOME CARE   563416.00               2.090000\n",
      "5     PLAYERS AND ELECTRONICS    19805.00               0.070000\n",
      "6                     PRODUCE  4005391.25              14.830000\n",
      "7  SCHOOL AND OFFICE SUPPLIES     8797.00               0.030000\n",
      "--------------------------------------------------\n",
      "First date in df: 2013-01-02 00:00:00\n",
      "Last date in df:  2017-08-15 00:00:00\n",
      "Start size of df_sales:     2.01 GB and start observations:     79.9 million.\n",
      "The multi-index (all_combinations of store, date and item) for the minimum and maximum dates found result in 200.7 million rows, this is the amount of rows we expect in the final dataframe.\n",
      "Final size of the dataframe is:     3.55 GB and end observations:       200.7 million.\n",
      "Change in size of the dataframe is: 76.75 % and observations:           151.17     %.\n",
      "Increased size of the dataframe is: 1.54 GB and increased observations: 120.8 million.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 3 - Cleaning holiday data and counting the number of holidays per date per store for each type of holiday (national, regional, local).\n",
      "In the orignal unioned holiday dataframe, df_holidays_merged we found (including duplicates) 8276 rows\n",
      "In our new adjusted dataframe we have 8091 rows\n",
      "Thus, we have removed 185 rows\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 4 - Cleaning stores data (read: dropping unnecessary columns and renaming columns for clarity).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 5 - Cleaning items data  (read: dropping unnecessary columns and renaming columns for clarity).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 6 - Adding holiday data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of three holiday columns.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 7 - Adding holiday data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of holiday columns.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 8 - Adding items data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of holiday columns. Remember, in our last step we added a lot of store information as well\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The amount of rows in the sales dataframe was 79.92 million.\n",
      "After making a cartesian product with date, store and item we had a total of 200.73 million rows.\n",
      "After mergin with the holidays, stores, and items we have 200.73 million rows\n",
      "The difference between the incoming and outgoing data from this function is -120812026 rows\n",
      "If we compare the outgoing dataframe called \"df_final\" with the cartesian product of sales data and dates we see that the difference is 0 rows\n",
      "If the difference is 0, we have a perfect match and we can continue with the next steps.\n"
     ]
    }
   ],
   "source": [
    "df_final = merge_datasets(df_sales, df_items, df_stores, df_holidays)  # --> 2.44 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200729382"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan code -\n",
    "# What about the \"onpromotion\" column, seems that it has a lot of NaN values. Are these quality issues or is just that there's no promotion.\n",
    "# This issue didn't arrive after merging, it was there from the beginning (in the df_sales dataframe).\n",
    "# You would expect that if there's no promotion going on the value to be \"False\"\n",
    "\n",
    "# df_sales1 = sales_cleaned(df_sales)\n",
    "\n",
    "# df_sales1_unique = df_sales1[\"onpromotion\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X.X. Count nulls per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_counts = df_final.isnull().sum()\n",
    "\n",
    "type(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200729382 entries, 0 to 200729381\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Dtype         \n",
      "---  ------                  -----         \n",
      " 0   store_nbr               uint8         \n",
      " 1   item_nbr                uint32        \n",
      " 2   date                    datetime64[ns]\n",
      " 3   unit_sales              float32       \n",
      " 4   onpromotion             boolean       \n",
      " 5   holiday_local_count     int8          \n",
      " 6   holiday_national_count  int8          \n",
      " 7   holiday_regional_count  int8          \n",
      " 8   store_type              category      \n",
      " 9   store_cluster           uint8         \n",
      " 10  item_family             category      \n",
      " 11  item_class              uint16        \n",
      " 12  perishable              uint8         \n",
      "dtypes: boolean(1), category(2), datetime64[ns](1), float32(1), int8(3), uint16(1), uint32(1), uint8(3)\n",
      "memory usage: 5.2 GB\n",
      "Column 'store_nbr' has 0 null values.\n",
      "Column 'item_nbr' has 0 null values.\n",
      "Column 'date' has 0 null values.\n",
      "Column 'unit_sales' has 120812026 null values.\n",
      "Column 'onpromotion' has 136877122 null values.\n",
      "Column 'holiday_local_count' has 0 null values.\n",
      "Column 'holiday_national_count' has 0 null values.\n",
      "Column 'holiday_regional_count' has 0 null values.\n",
      "Column 'store_type' has 0 null values.\n",
      "Column 'store_cluster' has 0 null values.\n",
      "Column 'item_family' has 0 null values.\n",
      "Column 'item_class' has 0 null values.\n",
      "Column 'perishable' has 0 null values.\n"
     ]
    }
   ],
   "source": [
    "df_final.info()\n",
    "# Count nulls per column\n",
    "null_counts = df_final.isnull().sum()\n",
    "\n",
    "# Print results\n",
    "for column, count in null_counts.items():\n",
    "    print(f\"Column '{column}' has {count} null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2: Detect negative values\n",
    "\n",
    "•\tAction: Delete unit_sales if values are lower than zero --> N/A\n",
    "\n",
    "To-do: do we want do make negative --> 0 or delete values --> Inpute later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sales_cleaned(df):\n",
    "\n",
    "    # Check the number of negative values before replacement\n",
    "    before_replacement = (df[\"unit_sales\"] < 0).sum()\n",
    "    print(f\"Number of negative values before replacement: {before_replacement}\")\n",
    "\n",
    "    # Create a boolean mask for the negative sales rows to create a 'boolean flag-list' containing all negative rows, used to filter full df_sales df\n",
    "    negative_sales_mask = df[\"unit_sales\"] < 0\n",
    "\n",
    "    # Use the mask to update the flagged 'unit_sales' column in the original DataFrame\n",
    "    df.loc[negative_sales_mask, \"unit_sales\"] = df.loc[\n",
    "        negative_sales_mask, \"unit_sales\"\n",
    "    ].where(df.loc[negative_sales_mask, \"unit_sales\"] >= 0, np.nan)\n",
    "\n",
    "    # Check the number of negative values after replacement\n",
    "    after_replacement = (df[\"unit_sales\"] < 0).sum()\n",
    "    print(f\"Number of negative values after replacement: {after_replacement}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Define new, old and closed stores\n",
    "\n",
    "•\tCondition: sales for all items a given store and date are NA\n",
    "\n",
    "•\tAction: Impute with 0\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "Label Variable for atributing numbers to store status:\n",
    "\n",
    "-     OPEN = 0\n",
    "-     NEW = 2\n",
    "-     CLOSED = 4\n",
    "-     OLD = 6\n",
    "-     NEVER_OPENED = 8\n",
    "\n",
    "----------\n",
    "\n",
    "To-do: Write in polars??\n",
    "\n",
    "To-do: Can the ML model run with NaN values? Or need the new / old stores also need to inputed with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_store_status(df):\n",
    "\n",
    "    # Label Variable for atributing numbers to store status, to save memory in df\n",
    "    OPEN = 0\n",
    "    NEW = 2\n",
    "    CLOSED = 4\n",
    "    OLD = 6\n",
    "    NEVER_OPENED = 8\n",
    "\n",
    "    # Group by store and date, then sum sales\n",
    "    df_grouped = (\n",
    "        df.groupby([\"store_nbr\", \"date\"]).agg({\"unit_sales\": \"sum\"}).reset_index()\n",
    "    ).reset_index()\n",
    "\n",
    "    # Sort by store and date\n",
    "    df_grouped = df_grouped.sort_values([\"store_nbr\", \"date\"])\n",
    "\n",
    "    # Create a new column for store status, label al stores as 'open' by default and make dtype in8\n",
    "    df_grouped[\"store_status\"] = np.int8(OPEN)\n",
    "\n",
    "    # Find the first and last day with sales for each store\n",
    "    first_sale_date = (\n",
    "        df_grouped[df_grouped[\"unit_sales\"] > 0].groupby(\"store_nbr\")[\"date\"].min()\n",
    "    )\n",
    "\n",
    "    last_sale_date = (\n",
    "        df_grouped[df_grouped[\"unit_sales\"] > 0].groupby(\"store_nbr\")[\"date\"].max()\n",
    "    )\n",
    "\n",
    "    # Loop trhough stores by lapeling them as 'NEW', 'CLOSED', 'OLD' or 'NEVER_OPENED' based on first sale date and last sale date\n",
    "    for store in df_grouped[\"store_nbr\"].unique():\n",
    "        store_data = df_grouped[df_grouped[\"store_nbr\"] == store]\n",
    "\n",
    "        if store in first_sale_date.index:\n",
    "            first_date = first_sale_date[store]\n",
    "            last_date = last_sale_date[store]\n",
    "\n",
    "            # Mark as 'NEW' before first sale date\n",
    "            df_grouped.loc[\n",
    "                (df_grouped[\"store_nbr\"] == store) & (df_grouped[\"date\"] < first_date),\n",
    "                \"store_status\",\n",
    "            ] = NEW\n",
    "\n",
    "            # Mark as 'closed' after first sale date if sales are 0\n",
    "            df_grouped.loc[\n",
    "                (df_grouped[\"store_nbr\"] == store)\n",
    "                & (df_grouped[\"date\"] > first_date)\n",
    "                & (df_grouped[\"unit_sales\"] == 0),\n",
    "                \"store_status\",\n",
    "            ] = CLOSED\n",
    "\n",
    "            # Mark as 'OLD' after last sale date\n",
    "            df_grouped.loc[\n",
    "                (df_grouped[\"store_nbr\"] == store) & (df_grouped[\"date\"] > last_date),\n",
    "                \"store_status\",\n",
    "            ] = OLD\n",
    "\n",
    "        else:\n",
    "            # If a store never had any sales, mark all dates as 'NEVER_OPENED' --> no records?\n",
    "            df_grouped.loc[df_grouped[\"store_nbr\"] == store, \"store_status\"] = (\n",
    "                NEVER_OPENED\n",
    "            )\n",
    "\n",
    "    # Merging store_status on df_sales\n",
    "    df = df.merge(\n",
    "        df_grouped[[\"store_nbr\", \"date\", \"store_status\"]],\n",
    "        left_on=[\"store_nbr\", \"date\"],\n",
    "        right_on=[\"store_nbr\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Get list of NEW stores at 01-01-2013 and OPEN stores at 02-01-2013\n",
    "    mask_new = (df[\"store_status\"] == NEW) & (df[\"date\"] == \"2013-01-01\")\n",
    "    mask_open = (df[\"store_status\"] == OPEN) & (df[\"date\"] == \"2013-01-02\")\n",
    "\n",
    "    # Get list of thores that meet both the coditions of NEW AT 01-01-2013 and OPEN at 02-01-2013\n",
    "    stores_new = set(df[mask_new][\"store_nbr\"].unique())\n",
    "    stores_open = set(df[mask_open][\"store_nbr\"].unique())\n",
    "    stores_status_change = stores_new.intersection(stores_open)\n",
    "\n",
    "    # Change status of stores that are NEW on 01-01-2013 but OPEN on 02-01-2013 to CLOSED on 01-01-2013\n",
    "    df.loc[\n",
    "        (df[\"store_nbr\"].isin(stores_status_change)) & (df[\"date\"] == \"2013-01-01\"),\n",
    "        [\"store_status\"],\n",
    "    ] = [CLOSED]\n",
    "\n",
    "    # Using a mask to flag al 'CLOSED' or (|) 'NEW' stores and impute 'closed' and 'new' stores with 0\n",
    "    mask = (df[\"store_status\"] == CLOSED) | (df[\"store_status\"] == NEW)\n",
    "    df.loc[mask, \"unit_sales\"] = 0\n",
    "\n",
    "    print(\"-\" * 72)\n",
    "    print(\n",
    "        f\"Size of df:     {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB and end observations:       {round(df.shape[0] / 1e6, 1)} million.\"\n",
    "    )\n",
    "    print(\"- \" * 36)\n",
    "    print(\"df_grouped store_status value counts:\")\n",
    "    print(df_grouped[\"store_status\"].value_counts())\n",
    "\n",
    "    print(\"-\" * 72)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 New product --> !Polars function!\n",
    "\n",
    "•\tBefore the very first sale of an item, all observations are kept as NA\n",
    "\n",
    "•\tAfter the very first sale of an item, we go to step 3: \n",
    "\n",
    " -----------------------------------\n",
    "\n",
    "Label Variable for atributing numbers to store status, to save memory in df\n",
    "-     EXISTING = 1\n",
    "-     NEW = 3\n",
    "-     OLD = 7\n",
    "-     NEVER_SOLD = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO: Add polars to requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <PATH>.\\venv_case_project\\Scripts\\activate\n",
    "# pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl  # later to import packages step at 0\n",
    "\n",
    "\n",
    "def merge_item_status_polars(df_pandas):\n",
    "\n",
    "    # Record the start time of the function\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Label variables\n",
    "\n",
    "    EXISTING = 1\n",
    "    NEW = 3\n",
    "    OLD = 7\n",
    "    NEVER_SOLD = 9\n",
    "\n",
    "    # Convert the Pandas df to Polars df\n",
    "    df = pl.from_pandas(df_pandas)\n",
    "\n",
    "    # Sort by store, item, and date\n",
    "    df = df.sort([\"store_nbr\", \"item_nbr\", \"date\"])\n",
    "\n",
    "    print(f\"Elapsed time: {time.time() - start_time:.2f} seconds | LINE | df sorted |\")\n",
    "\n",
    "    # Create a new column for item status, initialise to EXISTING\n",
    "    df = df.with_columns(pl.lit(EXISTING).cast(pl.Int8).alias(\"item_status\"))\n",
    "\n",
    "    print(\n",
    "        f\"Elapsed time: {time.time() - start_time:.2f} seconds | LINE | item_status added |\"\n",
    "    )\n",
    "\n",
    "    # Filter for rows with unit_sales > 0 and calculate first/last sale dates\n",
    "    first_sale_date = (\n",
    "        df.filter(pl.col(\"unit_sales\") > 0)\n",
    "        .group_by([\"store_nbr\", \"item_nbr\"])\n",
    "        .agg([pl.col(\"date\").min().alias(\"first_sale_date\")])\n",
    "    )\n",
    "\n",
    "    last_sale_date = (\n",
    "        df.filter(pl.col(\"unit_sales\") > 0)\n",
    "        .group_by([\"store_nbr\", \"item_nbr\"])\n",
    "        .agg([pl.col(\"date\").max().alias(\"last_sale_date\")])\n",
    "    )\n",
    "    print(\n",
    "        f\"Elapsed time: {time.time() - start_time:.2f} seconds | LINE | first and last sale dates |\"\n",
    "    )\n",
    "\n",
    "    # Join first and last sale dates to the original dataframe\n",
    "    df = df.join(first_sale_date, on=[\"store_nbr\", \"item_nbr\"], how=\"left\")\n",
    "\n",
    "    df = df.join(last_sale_date, on=[\"store_nbr\", \"item_nbr\"], how=\"left\")\n",
    "\n",
    "    print(\n",
    "        f\"Elapsed time: {time.time() - start_time:.2f} seconds | LINE | joined sale dates |\"\n",
    "    )\n",
    "\n",
    "    # Update the item_status column based on first and last sale dates\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"date\") < pl.col(\"first_sale_date\"))\n",
    "        .then(pl.lit(NEW))\n",
    "        .when(pl.col(\"date\") > pl.col(\"last_sale_date\"))\n",
    "        .then(pl.lit(OLD))\n",
    "        .otherwise(pl.col(\"item_status\"))\n",
    "        .alias(\"item_status\")\n",
    "    )\n",
    "\n",
    "    # Handle NEVER_SOLD case where first_sale_date is null\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"first_sale_date\").is_null())\n",
    "        .then(pl.lit(NEVER_SOLD))\n",
    "        .otherwise(pl.col(\"item_status\"))\n",
    "        .alias(\"item_status\")\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Elapsed time: {time.time() - start_time:.2f} seconds | LINE | updated item status |\"\n",
    "    )\n",
    "\n",
    "    # Drop columns first_sale_date\" and \"last_sale_date\" as these are not longer needed\n",
    "    df = df.drop([\"first_sale_date\", \"last_sale_date\"])\n",
    "\n",
    "    # Convert Polars df back to Pandas df\n",
    "    df = df.to_pandas()\n",
    "\n",
    "    print(\"-\" * 72)\n",
    "    print(f\"Total execution time: {(time.time() - start_time) / 60:.2f} minutes\")\n",
    "    print(\"- \" * 36)\n",
    "    print(\"df_grouped item_status value counts:\")\n",
    "    print(df[\"item_status\"].value_counts())\n",
    "\n",
    "    print(\"-\" * 72)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: Add print function to keep track of type of inputations\n",
    "\n",
    "To-do: .interpolate() --> ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 Promotional Data \n",
    "\n",
    "•   All missing values are interpreted a day with no promotion\n",
    "\n",
    "•   Action: Inpute onpromotion N/A with False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing N/A values in onpromotion column with False\n",
    "def sales_fill_onpromotion(df):\n",
    "\n",
    "    df[\"onpromotion\"] = df[\"onpromotion\"].fillna(False).astype(bool)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Feature construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.X Extracting datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_features(df):\n",
    "    # Ensure the date column is sorted\n",
    "    df = df.sort_values(\"date\")\n",
    "\n",
    "    # Add column with ISO year\n",
    "    df[\"year\"] = df[\"date\"].dt.isocalendar().year.astype(\"int16\")\n",
    "\n",
    "    # Add column with weekday (1-7, where 1 is Monday)\n",
    "    df[\"weekday\"] = df[\"date\"].dt.dayofweek.add(1).astype(\"int8\")\n",
    "\n",
    "    # Add column with ISO week number (1-53)\n",
    "    df[\"week_nbr\"] = df[\"date\"].dt.isocalendar().week.astype(\"int8\")\n",
    "\n",
    "    # Calculate the date of the Monday of the first week\n",
    "    first_date = df[\"date\"].iloc[0]\n",
    "    days_to_last_monday = (first_date.weekday() - 0 + 7) % 7\n",
    "    monday_first_week = first_date - pd.Timedelta(days=days_to_last_monday)\n",
    "\n",
    "    # Calculate cumulative week numbers starting from the first Monday\n",
    "    df[\"week_number_cum\"] = (\n",
    "        ((df[\"date\"] - monday_first_week).dt.days // 7) + 1\n",
    "    ).astype(\"int16\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data Manipulation and Feature construction --> Final-Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate_final_dataset(df):\n",
    "\n",
    "    df = negative_sales_cleaned(df)\n",
    "\n",
    "    df = merge_store_status(df)\n",
    "\n",
    "    df = merge_item_status_polars(df)\n",
    "\n",
    "    df = sales_fill_onpromotion(df)\n",
    "\n",
    "    df = datetime_features(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative values before replacement: 4438\n",
      "Number of negative values after replacement: 0\n",
      "------------------------------------------------------------------------\n",
      "Size of df:     5.42 GB and end observations:       200.7 million.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "df_grouped store_status value counts:\n",
      "store_status\n",
      "0    68088\n",
      "2     2217\n",
      "4      549\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------------------------------------\n",
      "Elapsed time: 8.73 seconds | LINE | df sorted |\n",
      "Elapsed time: 8.80 seconds | LINE | item_status added |\n",
      "Elapsed time: 20.34 seconds | LINE | first and last sale dates |\n",
      "Elapsed time: 37.87 seconds | LINE | joined sale dates |\n",
      "Elapsed time: 39.48 seconds | LINE | updated item status |\n",
      "------------------------------------------------------------------------\n",
      "Total execution time: 0.81 minutes\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "df_grouped item_status value counts:\n",
      "item_status\n",
      "1    113662714\n",
      "3     42177480\n",
      "9     41687457\n",
      "7      3201731\n",
      "Name: count, dtype: int64\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_6100\\1347920879.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"onpromotion\"] = df[\"onpromotion\"].fillna(False).astype(bool)\n"
     ]
    }
   ],
   "source": [
    "df_final = manipulate_final_dataset(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 200729382 entries, 0 to 200729381\n",
      "Data columns (total 19 columns):\n",
      " #   Column                  Dtype         \n",
      "---  ------                  -----         \n",
      " 0   store_nbr               uint8         \n",
      " 1   item_nbr                uint32        \n",
      " 2   date                    datetime64[ns]\n",
      " 3   unit_sales              float32       \n",
      " 4   onpromotion             bool          \n",
      " 5   holiday_local_count     int8          \n",
      " 6   holiday_national_count  int8          \n",
      " 7   holiday_regional_count  int8          \n",
      " 8   store_type              category      \n",
      " 9   store_cluster           uint8         \n",
      " 10  item_family             category      \n",
      " 11  item_class              uint16        \n",
      " 12  perishable              uint8         \n",
      " 13  store_status            int8          \n",
      " 14  item_status             int8          \n",
      " 15  year                    int16         \n",
      " 16  weekday                 int8          \n",
      " 17  week_nbr                int8          \n",
      " 18  week_number_cum         int16         \n",
      "dtypes: bool(1), category(2), datetime64[ns](1), float32(1), int16(2), int8(7), uint16(1), uint32(1), uint8(3)\n",
      "memory usage: 8.0 GB\n",
      "Column 'store_nbr' has 0 null values.\n",
      "Column 'item_nbr' has 0 null values.\n",
      "Column 'date' has 0 null values.\n",
      "Column 'unit_sales' has 112980386 null values.\n",
      "Column 'onpromotion' has 0 null values.\n",
      "Column 'holiday_local_count' has 0 null values.\n",
      "Column 'holiday_national_count' has 0 null values.\n",
      "Column 'holiday_regional_count' has 0 null values.\n",
      "Column 'store_type' has 0 null values.\n",
      "Column 'store_cluster' has 0 null values.\n",
      "Column 'item_family' has 0 null values.\n",
      "Column 'item_class' has 0 null values.\n",
      "Column 'perishable' has 0 null values.\n",
      "Column 'store_status' has 0 null values.\n",
      "Column 'item_status' has 0 null values.\n",
      "Column 'year' has 0 null values.\n",
      "Column 'weekday' has 0 null values.\n",
      "Column 'week_nbr' has 0 null values.\n",
      "Column 'week_number_cum' has 0 null values.\n"
     ]
    }
   ],
   "source": [
    "df_final.info()\n",
    "# Count nulls per column\n",
    "null_counts = df_final.isnull().sum()\n",
    "\n",
    "# Print results\n",
    "for column, count in null_counts.items():\n",
    "    print(f\"Column '{column}' has {count} null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76491"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_finalcountholidaysum = df_final['holiday_regional_count'].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Parquet fil and saves it in output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_parquet(df, output_path, file_prefix=\"Prepped_data\"):\n",
    "    try:\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Generate today's date for the filename\n",
    "        today = date.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "        # Create the full filename with path\n",
    "        filename = f\"{file_prefix}_{today}.parquet\"\n",
    "        full_path = os.path.join(output_path, filename)\n",
    "\n",
    "        # Save the DataFrame to a Parquet file\n",
    "        df.to_parquet(full_path)\n",
    "\n",
    "        print(f\"DataFrame successfully saved to {full_path}\")\n",
    "\n",
    "        return full_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame to Parquet file: {e}\")\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully saved to C:/Users/sebas/OneDrive/Documenten/GitHub/Supermarketcasegroupproject/Group4B/data/interim\\Prepped_data_20241016.parquet\n"
     ]
    }
   ],
   "source": [
    "# output_path = \"C:/Users/alexander/Documents/0. Data Science and AI for Experts/TEST\"\n",
    "\n",
    "output_path = \"C:/Users/sebas/OneDrive/Documenten/GitHub/Supermarketcasegroupproject/Group4B/data/interim\"\n",
    "\n",
    "saved_path = save_dataframe_to_parquet(df_final, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X # Function to print memory usage of DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: _\n",
      "Index                       80\n",
      "store_nbr                   10\n",
      "item_nbr                    40\n",
      "date                        80\n",
      "unit_sales                  40\n",
      "onpromotion                 10\n",
      "holiday_local_count         10\n",
      "holiday_national_count      10\n",
      "holiday_regional_count      10\n",
      "store_type                 472\n",
      "store_cluster               10\n",
      "item_family               3318\n",
      "item_class                  20\n",
      "perishable                  10\n",
      "store_status                10\n",
      "item_status                 10\n",
      "year                        20\n",
      "weekday                     10\n",
      "week_nbr                    10\n",
      "week_number_cum             20\n",
      "dtype: int64\n",
      "Total Memory Usage: 4200 bytes\n",
      "\n",
      "DataFrame: df_sales\n",
      "Index                 128\n",
      "id              501988160\n",
      "store_nbr       125497040\n",
      "item_nbr        501988160\n",
      "unit_sales      501988160\n",
      "onpromotion     250994080\n",
      "day             125497040\n",
      "year            125497200\n",
      "month           125497324\n",
      "date           1003976320\n",
      "dtype: int64\n",
      "Total Memory Usage: 3262923612 bytes\n",
      "\n",
      "DataFrame: df_holidays\n",
      "Index            128\n",
      "date            2800\n",
      "type             908\n",
      "locale           650\n",
      "locale_name     2476\n",
      "description    12694\n",
      "transferred      350\n",
      "dtype: int64\n",
      "Total Memory Usage: 20006 bytes\n",
      "\n",
      "DataFrame: df_items\n",
      "Index           128\n",
      "item_nbr      16400\n",
      "family         7408\n",
      "class          8200\n",
      "perishable     4100\n",
      "dtype: int64\n",
      "Total Memory Usage: 36236 bytes\n",
      "\n",
      "DataFrame: df_stores\n",
      "Index         128\n",
      "store_nbr      54\n",
      "city         2021\n",
      "state        1667\n",
      "type          516\n",
      "cluster        54\n",
      "dtype: int64\n",
      "Total Memory Usage: 4440 bytes\n",
      "\n",
      "DataFrame: df_final\n",
      "Index                     2561600768\n",
      "store_nbr                  320200096\n",
      "item_nbr                  1280800384\n",
      "date                      2561600768\n",
      "unit_sales                1280800384\n",
      "onpromotion                320200096\n",
      "holiday_local_count        320200096\n",
      "holiday_national_count     320200096\n",
      "holiday_regional_count     320200096\n",
      "store_type                 320200558\n",
      "store_cluster              320200096\n",
      "item_family                320203404\n",
      "item_class                 640400192\n",
      "perishable                 320200096\n",
      "store_status               320200096\n",
      "item_status                320200096\n",
      "year                       640400192\n",
      "weekday                    320200096\n",
      "week_nbr                   320200096\n",
      "week_number_cum            640400192\n",
      "dtype: int64\n",
      "Total Memory Usage: 13768607898 bytes\n",
      "\n",
      "DataFrame: _38\n",
      "Index                       80\n",
      "store_nbr                   10\n",
      "item_nbr                    40\n",
      "date                        80\n",
      "unit_sales                  40\n",
      "onpromotion                 10\n",
      "holiday_local_count         10\n",
      "holiday_national_count      10\n",
      "holiday_regional_count      10\n",
      "store_type                 472\n",
      "store_cluster               10\n",
      "item_family               3318\n",
      "item_class                  20\n",
      "perishable                  10\n",
      "store_status                10\n",
      "item_status                 10\n",
      "year                        20\n",
      "weekday                     10\n",
      "week_nbr                    10\n",
      "week_number_cum             20\n",
      "dtype: int64\n",
      "Total Memory Usage: 4200 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to print memory usage of DataFrames\n",
    "def print_memory_usage(dataframes):\n",
    "    for name, df in dataframes.items():\n",
    "        mem_usage = df.memory_usage(deep=True)\n",
    "        total_mem = mem_usage.sum()\n",
    "\n",
    "        print(f\"DataFrame: {name}\")\n",
    "        print(mem_usage)\n",
    "        print(f\"Total Memory Usage: {total_mem} bytes\\n\")\n",
    "\n",
    "\n",
    "# Check for DataFrames\n",
    "dataframes = {\n",
    "    name: obj for name, obj in globals().items() if isinstance(obj, pd.DataFrame)\n",
    "}\n",
    "print_memory_usage(dataframes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
