{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation Pipeline\n",
    "\n",
    "### Hamilton Framework\n",
    "https://hamilton.dagworks.io/en/latest/how-tos/use-in-jupyter-notebook/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import altair as alt\n",
    "\n",
    "import vegafusion as vf\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Downcast and transform data\n",
    "Update formatting of features to optimize memory and standardize column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_column_names(s):\n",
    "    \"\"\"Removes spaces from the column names.\"\"\"\n",
    "    return s.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "def optimize_memory(df):\n",
    "    \"\"\"Optimize memory usage of a DataFrame by converting object columns to categorical\n",
    "    and downcasting numeric columns to smaller types.\"\"\"\n",
    "\n",
    "    # Change: Objects to Categorical.\n",
    "    object_cols = df.select_dtypes(include=\"object\").columns\n",
    "    if not object_cols.empty:\n",
    "        print(\"Change: Objects to Categorical\")\n",
    "        df[object_cols] = df[object_cols].astype(\"category\")\n",
    "\n",
    "    # Change: Convert integers to smallest signed or unsigned integer and floats to smallest.\n",
    "    for col in df.select_dtypes(include=[\"int\"]).columns:\n",
    "        if (df[col] >= 0).all():  # Check if all values are non-negative\n",
    "            df[col] = pd.to_numeric(\n",
    "                df[col], downcast=\"unsigned\"\n",
    "            )  # Downcast to unsigned\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"integer\")  # Downcast to signed\n",
    "\n",
    "    # Downcast float columns\n",
    "    for col in df.select_dtypes(include=[\"float\"]).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def month_year_to_int(df, i):\n",
    "\n",
    "    # Change: Month and Year to integer.\n",
    "\n",
    "    if i == 0:\n",
    "\n",
    "        print(\"Change: Month and Year to integer\")\n",
    "\n",
    "        df = df.astype({\"month\": int, \"year\": int})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Transform date-related columns to datetime format.\n",
    "\n",
    "\n",
    "def transform_date_to_datetime(df, i):\n",
    "\n",
    "    if i == 0:\n",
    "\n",
    "        print(\"Change: Transformed 'year', 'month', 'day' columns to Datetime feature\")\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]], unit=\"us\")\n",
    "\n",
    "    else:\n",
    "        if \"date\" in df.columns:\n",
    "\n",
    "            print(\"Change: Transformed 'date' column to Datetime Dtype\")\n",
    "\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_basic_info_before(df):\n",
    "    print(\n",
    "        f\"-> Contains:                {df.shape[0]} observations and {df.shape[1]} features.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-> Has original size of    {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def df_basic_info_after(df):\n",
    "    print(\n",
    "        f\"-> Contains:                {df.shape[0]} observations and {df.shape[1]} features.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-> Has optimized size of    {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import data from local PATH\n",
    "Import data trough pipeline to downcast the data and transformation to datetime dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_data(i=0):\n",
    "\n",
    "    # Define path.\n",
    "    c_path = \"C:/Users/sebas/OneDrive/Documenten/GitHub/Supermarketcasegroupproject/Group4B/data/raw/\"\n",
    "\n",
    "    # Identify file.\n",
    "    v_file = (\n",
    "        \"history-per-year\",  # 0\n",
    "        \"holidays_events\",  # 1\n",
    "        \"items\",  # 2\n",
    "        \"stores\",  # 3\n",
    "    )\n",
    "\n",
    "    print(f\"\\nReading file {i}\\n\")\n",
    "\n",
    "    # Load data.\n",
    "    df = (\n",
    "        pd.read_parquet(c_path + v_file[i] + \".parquet\")\n",
    "        .rename(columns=standardize_column_names)\n",
    "        .pipe(optimize_memory)\n",
    "        .pipe(month_year_to_int, i)\n",
    "        .pipe(transform_date_to_datetime, i)\n",
    "    )\n",
    "\n",
    "    # Return data.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-do: write this in function. But where executed? In the end ?\n",
    "\n",
    "# Sales History per year\n",
    "\n",
    "df_sales = f_get_data(0)\n",
    "\n",
    "df_basic_info_after(df_sales)\n",
    "\n",
    "\n",
    "# Holidays\n",
    "\n",
    "df_holidays = f_get_data(1)\n",
    "\n",
    "df_basic_info_after(df_holidays)\n",
    "\n",
    "\n",
    "# Items\n",
    "\n",
    "df_items = f_get_data(2)\n",
    "\n",
    "df_basic_info_after(df_items)\n",
    "\n",
    "\n",
    "# Stores\n",
    "\n",
    "df_stores = f_get_data(3)\n",
    "\n",
    "df_basic_info_after(df_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Exclude Stores + Vulcano Eruption holiday + Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Return list containing stores with less then 1670 operational days with sales\n",
    "\n",
    "parameter: store_exclusion_cutoff_number = 1670 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stores_exclude_sales_days(df_sales, df_stores, store_exclusion_cutoff_number=1670):\n",
    "\n",
    "    # Group the sales date by store and item\n",
    "    df_sales_grouped = (\n",
    "        df_sales.groupby([\"store_nbr\", \"date\"]).agg({\"unit_sales\": \"sum\"}).reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge the grouped sales data with the store data\n",
    "    df_sales_stores_merged = df_sales_grouped.merge(\n",
    "        df_stores, left_on=\"store_nbr\", right_on=\"store_nbr\", how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Count the number of daily sale records per store\n",
    "    store_count = df_sales_stores_merged[\"store_nbr\"].value_counts()\n",
    "\n",
    "    # Get stores with counts less than the exclusion cutoff\n",
    "    store_count_exclusion = store_count[store_count < store_exclusion_cutoff_number]\n",
    "\n",
    "    # Get the list of store numbers to be excluded\n",
    "    list_excluded_stores_sales_days = store_count_exclusion.index.tolist()\n",
    "\n",
    "    return list_excluded_stores_sales_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_exclude_sales_days(\n",
    "    df_sales, df_stores, store_exclusion_cutoff_number=1670\n",
    ")  # --> [30, 14, 12, 25, 24, 18, 36, 53, 20, 29, 21, 42, 22, 52]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Return list containing stores with cluster=10 in stores df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stores_exclude_cluster(df_stores, cluster_number=10):\n",
    "\n",
    "    # Get the list of store numbers that belong to cluster 10\n",
    "\n",
    "    list_stores_cluster_10 = df_stores[df_stores[\"cluster\"] == cluster_number][\n",
    "        \"store_nbr\"\n",
    "    ].tolist()\n",
    "\n",
    "    return list_stores_cluster_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_exclude_cluster(df_stores, cluster_number=10)  # --> [26, 28, 29, 31, 36, 43]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 Function to exclude stores with less then 1670 sales days and related to cluster 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sales_cleaned_stores(df_sales, store_exclusion_cutoff_number=1670):\n",
    "\n",
    "    # Excluded less then 1670 salesdays\n",
    "    list_excluded_stores_sales_days = stores_exclude_sales_days(\n",
    "        df_sales, df_stores, store_exclusion_cutoff_number\n",
    "    )\n",
    "\n",
    "    df_sales = df_sales.drop(\n",
    "        df_sales[df_sales[\"store_nbr\"].isin(list_excluded_stores_sales_days)].index\n",
    "    )\n",
    "\n",
    "    # Cluster 10\n",
    "    list_stores_cluster_10 = stores_exclude_cluster(df_stores, cluster_number=10)\n",
    "\n",
    "    df_sales = df_sales.drop(\n",
    "        df_sales[df_sales[\"store_nbr\"].isin(list_stores_cluster_10)].index\n",
    "    )\n",
    "\n",
    "    return df_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan -> eigen stukje code\n",
    "\n",
    "list_stores_cluster_10e = stores_exclude_cluster(df_stores, cluster_number=10)\n",
    "\n",
    "type(list_stores_cluster_10e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution of final function --> In pipeline\n",
    "df_sales = df_sales_cleaned_stores(df_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4 Filter Vulcano Eruption from holiday df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan code\n",
    "\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     display(df_holidays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holiday_filter_vulcano_event(df_holidays, event_substring=\"Terremoto Manabi\"):\n",
    "\n",
    "    # Filter the DataFrame where 'description' contains the event_substring\n",
    "    df_vulcano_event_filtered = df_holidays[\n",
    "        df_holidays[\"description\"].str.contains(event_substring)\n",
    "    ]\n",
    "\n",
    "    return df_vulcano_event_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_holidays_cleaned(df_holidays):\n",
    "\n",
    "    # Exclude holiday_filter_vulcano_event function to return filtered df\n",
    "    df_vulcano_event_filtered = holiday_filter_vulcano_event(df_holidays)\n",
    "\n",
    "    # Filter the specific holiday events from the holiday DataFrame\n",
    "    df_holidays = df_holidays.loc[\n",
    "        ~df_holidays.index.isin(df_vulcano_event_filtered.index)\n",
    "    ]\n",
    "\n",
    "    return df_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution of final function --> In pipeline?\n",
    "df_holidays = df_holidays_cleaned(df_holidays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Filter and exclude of Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Orginal, first try and check on item level\n",
    "\n",
    "\n",
    "def item_check(start_date, x_days):\n",
    "\n",
    "    # start_date = \"2013-02-01\"\n",
    "    # x_days = 31\n",
    "\n",
    "    # Convert start_date to datetime\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    print(start_date)\n",
    "\n",
    "    # Calculate end_date\n",
    "    end_date = start_date + timedelta(days=x_days)\n",
    "    print(x_days)\n",
    "    print(end_date)\n",
    "\n",
    "    # Filter the DataFrame based on the date range\n",
    "    df_sales_filtered = df_sales[\n",
    "        (df_sales[\"date\"] >= start_date) & (df_sales[\"date\"] <= end_date)\n",
    "    ]\n",
    "\n",
    "    # Group by item_nbr and sum unit_sales, so this will be the same criteria for all stores.\n",
    "    df_sales_item = (\n",
    "        df_sales_filtered.groupby(\"item_nbr\").agg({\"unit_sales\": \"sum\"}).reset_index()\n",
    "    )\n",
    "\n",
    "    # Get the list of store numbers to stay included, as they have sales within the first 28 days\n",
    "    list_sales_items = df_sales_item[\"item_nbr\"].tolist()\n",
    "    unique_values_count = df_sales_item[\"item_nbr\"].nunique()\n",
    "    print(unique_values_count)\n",
    "\n",
    "    # If sum_sales = 0 --> drop item\n",
    "\n",
    "    # first 4 weeks\n",
    "    # last 4 weeks\n",
    "\n",
    "    # sum_total per item for these two months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_check(\"2016-01-01\", 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO- WRITE ITEM EXLUSION PART\n",
    "\n",
    "If no sum_sales of unique item_nbr per unique store_nbr after 30-07-2017 --> exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_date = \"30-07-2017\"\n",
    "\n",
    "# # Convert start_date to datetime\n",
    "# start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "# print(start_date)\n",
    "\n",
    "# # Calculate end_date\n",
    "# end_date = start_date + timedelta(days=x_days)\n",
    "# print(x_days)\n",
    "# print(end_date)\n",
    "\n",
    "# # Filter the DataFrame based on the date range\n",
    "# df_sales_filtered = df_sales[\n",
    "#     (df_sales[\"date\"] >= start_date) & (df_sales[\"date\"] <= end_date)\n",
    "# ]\n",
    "\n",
    "# # Change the dtype for item_nbr from uint32 to int32\n",
    "# df_sales[\"item_nbr\"] = df_sales[\"item_nbr\"].astype(int)\n",
    "# df_items[\"item_nbr\"] = df_items[\"item_nbr\"].astype(int)\n",
    "\n",
    "# # Merge the filtered sales data with the items data\n",
    "# df_sales_items_merged = df_sales.merge(df_items, on=\"item_nbr\", how=\"left\")\n",
    "\n",
    "# # print(df_sales_items_merged.info())\n",
    "# # print(df_sales_items_merged.sample(5))\n",
    "\n",
    "# df_sales_items_merged[\"class\"] = df_sales_items_merged[\"class\"].astype(str)\n",
    "\n",
    "# # Group by item_nbr and sum unit_sales, so this will be the same criteria for all stores.\n",
    "# df_sales_item = (\n",
    "#     df_sales_filtered.groupby(\"class\").agg({\"unit_sales\": \"sum\"}).reset_index()\n",
    "# )\n",
    "\n",
    "# # Get the list of store numbers to stay included, as they have sales within the first 28 days\n",
    "# list_sales_items = df_sales_item[\"class\"].tolist()\n",
    "# unique_values_count = df_sales_item[\"class\"].nunique()\n",
    "# print(unique_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function for items\n",
    "def get_unique(df, column_name):\n",
    "    \"\"\"Get the all values and the count for specific column\"\"\"\n",
    "\n",
    "    unique_values_count = df[column_name].nunique()\n",
    "\n",
    "    unique_values = df[column_name].unique()\n",
    "\n",
    "\n",
    "    # Convert unique values to a single string to print\n",
    "\n",
    "    unique_values_str = \", \".join(map(str, unique_values))\n",
    "\n",
    "\n",
    "    print(f\"Number of unique values in {column_name}: {unique_values_count}\")\n",
    "\n",
    "    print(\"Unique values:\")\n",
    "\n",
    "    print(unique_values_str)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Prepare and Merge df_sales + df_items + df_stores + df_holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Prepare and clean df_sales\n",
    "\n",
    "Drop of columns \"id\", \"year\", \"month\", \"day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_sales by cleaning up df for merging with holidays by dropping unneeded columns\n",
    "def sales_cleaned(df_sales):\n",
    "\n",
    "    df_sales = df_sales.drop(columns=[\"id\", \"year\", \"month\", \"day\"])\n",
    "\n",
    "    return df_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Prepare, clean and rename df_items\n",
    "\n",
    "Rename of columns: \"family\" to \"item_family\" and  \"class\" to \"item_class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_items by cleaning up df by dropping unneeded columns and rename columns for clearity in final df\n",
    "def items_cleaned_renamed(df_items):\n",
    "\n",
    "    df_items = df_items.rename(columns={\"family\": \"item_family\", \"class\": \"item_class\"})\n",
    "\n",
    "    return df_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Prepare, clean and rename df_stores\n",
    "\n",
    "Drop of columns \"state\"\n",
    "\n",
    "Rename of columns \"city\" to \"store_city\", \"cluster\" to \"store_cluster\" and \"type\" to \"store_type\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_stores by cleaning up df by dropping unneeded columns and rename columns for clearity in final df\n",
    "def stores_cleaned_renamed(df_stores):\n",
    "\n",
    "    df_stores = df_stores.drop(columns=[\"state\"])\n",
    "\n",
    "    df_stores = df_stores.rename(\n",
    "        columns={\"city\": \"store_city\", \"cluster\": \"store_cluster\", \"type\": \"store_type\"}\n",
    "    )\n",
    "\n",
    "    return df_stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Prepare df_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_holiday and df_stores by cleaning up df for merging with holidays by dropping unneeded columns\n",
    "def clean_holidays_stores_prep(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned = df_holidays.drop(\n",
    "        columns=[\n",
    "            \"description\",\n",
    "            \"transferred\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_stores_cleaned = df_stores.drop(columns=[\"cluster\", \"type\"])\n",
    "\n",
    "    return df_holidays_cleaned, df_stores_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_local(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # select locale 'Local' from holiday df and merge with city stores df\n",
    "    df_holidays_local = df_holidays_cleaned[df_holidays_cleaned[\"locale\"] == \"Local\"]\n",
    "\n",
    "    df_holidays_prep_local = df_holidays_local.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"city\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sebastiaan code\n",
    "df_holidays_prep_local = holidays_prep_local(df_holidays, df_stores)\n",
    "\n",
    "df_holidays_prep_local_filtered = df_holidays_prep_local[df_holidays_prep_local[\"store_nbr\"] == 51]\n",
    "\n",
    "df_holidays_prep_local_filtered.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_types = df_holidays_prep_local['type'].unique().tolist()\n",
    "\n",
    "unique_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_regional(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # select locale 'Regional' from holiday df and merge with state stores df\n",
    "    df_holidays_regional = df_holidays_cleaned[\n",
    "        df_holidays_cleaned[\"locale\"] == \"Regional\"\n",
    "    ]\n",
    "\n",
    "    df_holidays_prep_regional = df_holidays_regional.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"state\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_regional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sebastiaan code\n",
    "df_holidays_prep_regional = holidays_prep_regional(df_holidays, df_stores)\n",
    "\n",
    "df_holidays_prep_regional_filtered = df_holidays_prep_regional[df_holidays_prep_regional[\"store_nbr\"] == 27]\n",
    "\n",
    "df_holidays_prep_regional_filtered.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_national(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # Select locale 'Regional' from holiday df and merge with national stores df\n",
    "    df_holidays_national = df_holidays_cleaned[\n",
    "        df_holidays_cleaned[\"locale\"] == \"National\"\n",
    "    ]\n",
    "\n",
    "    # Create extra column for merge on \"Ecuador\"\n",
    "    df_stores_cleaned[\"national_merge\"] = \"Ecuador\"\n",
    "\n",
    "    df_holidays_prep_national = df_holidays_national.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"national_merge\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Drop newly created column national_merge, not needed further\n",
    "    df_holidays_prep_national = df_holidays_prep_national.drop(\n",
    "        columns=[\"national_merge\"]\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_national"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan code\n",
    "\n",
    "df_holidays_prep_national = holidays_prep_national(df_holidays, df_stores)\n",
    "\n",
    "df_holidays_prep_national_filtered = df_holidays_prep_national[df_holidays_prep_national[\"store_nbr\"] == 54]\n",
    "\n",
    "df_holidays_prep_national_filtered.sort_values(by=\"date\")\n",
    "\n",
    "df_holidays_prep_national_filtered.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan code\n",
    "df_holidays_prep_local = holidays_prep_local(df_holidays, df_stores)\n",
    "df_holidays_prep_regional = holidays_prep_regional(df_holidays, df_stores)\n",
    "df_holidays_prep_national = holidays_prep_national(df_holidays, df_stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_merged(df_holidays, df_stores):\n",
    "\n",
    "    # Load prep functions from local, Regional and National df's\n",
    "    df_holidays_prep_local = holidays_prep_local(df_holidays, df_stores)\n",
    "\n",
    "    df_holidays_prep_regional = holidays_prep_regional(df_holidays, df_stores)\n",
    "\n",
    "    df_holidays_prep_national = holidays_prep_national(df_holidays, df_stores)\n",
    "\n",
    "    # Combine local, regional and national dataframes into 1 merged dataframe\n",
    "    df_holidays_merged = pd.concat(\n",
    "        [df_holidays_prep_local, df_holidays_prep_regional, df_holidays_prep_national]\n",
    "    )\n",
    "\n",
    "    # Clean df_holidays_merged by dropping locale_name\", \"city\", \"state\"\n",
    "    df_holidays_merged = df_holidays_merged.drop(\n",
    "        columns=[\"locale_name\", \"city\", \"state\"]\n",
    "    )\n",
    "\n",
    "    # Rename 'type' of holiday to 'holiday_type'\n",
    "    df_holidays_merged = df_holidays_merged.rename(\n",
    "        columns={\"type\": \"holiday_type\", \"locale\": \"holiday_locale\"}\n",
    "    )\n",
    "\n",
    "    return df_holidays_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3X FIX FOR DUPLICATE ROWS FROM DF_HOLIDAYS_MERGED OUTPUT - BY SEBASTIAAN 16082024\n",
    "\n",
    "Fix contains 2 possible options:  \n",
    "Option 1: makes from df_holidays_merged a pivot with 3 columns counting for the amount of holidays per locale of holiday (local, regional or national).  \n",
    "Option 2: makes from df_holidays_merged a pivot following the columns of option 1 but additionally adds the type of holiday (but therefore increasing the amount of columns).  \n",
    "\n",
    "I was first thinking about adjusting the processing from local, regional en national dataframes and then doing the union by grouping them first but that would still make it possible to have duplicates over multiple 'locales' (you could have a regional and national holiday on the same day). Thus, the way it's been done now is the way to go. After the union we do the trick to fix the duplicate rows for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan code - Option 1 to group all dates in df_holidays_merged after the union of the three dataframes and count the number of holidays per date per store\n",
    "\n",
    "# Merge the holiday dataframes and clean the merged dataframe\n",
    "df_holidays_merged = holidays_prep_merged(df_holidays, df_stores)\n",
    "\n",
    "# Group by date and store_nbr and count the number of holidays per date per store\n",
    "df_holidays_merged_grouped = df_holidays_merged.pivot_table(\n",
    "    index=['date', 'store_nbr'],\n",
    "    columns='holiday_locale',\n",
    "    values='holiday_type', \n",
    "    aggfunc='count'\n",
    ").reset_index()\n",
    "\n",
    "# The nature of the pivot function causes it to append date and store_nbrs for all possible combinations and thus not only the date and store combinations that we originally had in our data, we will conduct an inner join with the original data to get the original date and store_nbr combinations back\n",
    "\n",
    "# Remove the name of the columns\n",
    "df_holidays_merged_grouped.columns.name = None\n",
    "\n",
    "# Rename the columns to countoflocalholidays, countofregionalholidays, countofnationalholidays\n",
    "df_holidays_merged_grouped = df_holidays_merged_grouped.rename(columns={\n",
    "    'Local': 'countoflocalholidays', \n",
    "    'Regional': 'countofregionalholidays', \n",
    "    'National': 'countofnationalholidays'\n",
    "})\n",
    "\n",
    "# Fill NaN values with 0\n",
    "df_holidays_merged_grouped = df_holidays_merged_grouped.fillna(0)\n",
    "\n",
    "# Let's do an inner join with the original data to get the original date and store_nbr combinations back. Therefore we need to make another dataframe.\n",
    "\n",
    "df_holidays_merged_grouped_inner = holidays_prep_merged(df_holidays, df_stores)\n",
    "df_holidays_merged_grouped_inner = df_holidays_merged_grouped_inner.groupby(['date', 'store_nbr']).size().reset_index().drop(columns=0)\n",
    "\n",
    "df_holidays_merged_grouped = df_holidays_merged_grouped.merge(df_holidays_merged_grouped_inner, on=['date', 'store_nbr'], how='inner')\n",
    "\n",
    "# Convert the count columns to integer\n",
    "df_holidays_merged_grouped = df_holidays_merged_grouped.astype({'countoflocalholidays': int, 'countofregionalholidays': int, 'countofnationalholidays': int})\n",
    "\n",
    "print(f'In the orignal unioned holiday dataframe, df_holidays_merged we found (including duplicates) {df_holidays_merged.shape[0]} rows')\n",
    "print(f'In our new adjusted dataframe we have {df_holidays_merged_grouped.shape[0]} rows')\n",
    "print(f'Thus, we have removed {df_holidays_merged.shape[0] - df_holidays_merged_grouped.shape[0]} rows')\n",
    "\n",
    "# Might want to filter out the holiday dates that will never be in de salesdate range. However, they will be left out anyway when joining with the sales data.\n",
    "df_holidays_merged_grouped.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan code # To see if the code works as designed we will check it with store 54 and only for the year 2012 (as i know we have holidays on the same day that year and don't want to bother you with to much data)\n",
    "\n",
    "df_holidays_prep_national54 = holidays_prep_national(df_holidays, df_stores)\n",
    "df_holidays_prep_national_filtered54 = df_holidays_prep_national54[df_holidays_prep_national54[\"store_nbr\"] == 54]\n",
    "df_holidays_prep_national_filtered54 = df_holidays_prep_national_filtered54[df_holidays_prep_national_filtered54[\"date\"].dt.year == 2012]\n",
    "df_holidays_prep_national_filtered54.sort_values(by=\"date\")\n",
    "\n",
    "print('In the original holiday dataframe for store 54 we found 2 duplicate holidays in 2012, on the 24th and the 31st of December')\n",
    "df_holidays_prep_national_filtered54.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan code # This should be fixed in our new dataframe, let's see\n",
    "\n",
    "df_holidays_merged_grouped54 = df_holidays_merged_grouped[df_holidays_merged_grouped[\"store_nbr\"] == 54]\n",
    "df_holidays_merged_grouped54 = df_holidays_merged_grouped54[df_holidays_merged_grouped54[\"date\"].dt.year == 2012]\n",
    "\n",
    "print('In our new holiday dataframe for store 54 we nicely removed the duplicate rows but do account for multiple holidays on the same day by making use of the count columns')\n",
    "df_holidays_merged_grouped54.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan code # Option 2 Option 2: makes from df_holidays_merged a pivot following the columns of option 1 but additionally adds the type of holiday (but therefore increasing the amount of columns).  \n",
    "\n",
    "# Merge the holiday dataframes and clean the merged dataframe\n",
    "df_holidays_merged2 = holidays_prep_merged(df_holidays, df_stores)\n",
    "df_holidays_merged2['holiday_localeandtype'] = df_holidays_merged2['holiday_locale'].astype(str) + '_' + df_holidays_merged['holiday_type'].astype(str)\n",
    "\n",
    "\n",
    "# Group by date and store_nbr and count the number of holidays per date per store\n",
    "df_holidays_merged_grouped2 = df_holidays_merged2.pivot_table(\n",
    "    index=['date', 'store_nbr'],\n",
    "    columns='holiday_localeandtype',\n",
    "    values='holiday_type', \n",
    "    aggfunc='count'\n",
    ").reset_index()\n",
    "\n",
    "# The nature of the pivot function causes it to append date and store_nbrs for all possible combinations and thus not only the date and store combinations that we originally had in our data, we will conduct an inner join with the original data to get the original date and store_nbr combinations back\n",
    "\n",
    "# Remove the name of the columns\n",
    "df_holidays_merged_grouped2.columns.name = None\n",
    "\n",
    "# Rename the columns to countoflocalholidays, countofregionalholidays, countofnationalholidays\n",
    "df_holidays_merged_grouped2 = df_holidays_merged_grouped2.rename(columns={\n",
    "    'Local_Additional': 'countoflocal_additional_holidays', \n",
    "    'Local_Holiday': 'countoflocal_holiday_holidays', \n",
    "    'Local_Transfer': 'countoflocal_transfer_holidays',\n",
    "    'Regional_Holiday': 'countofregional_holiday_holidays',\n",
    "    'National_Holiday': 'countofnational_holiday_holidays',\n",
    "    'National_Additional': 'countofnational_additional_holidays',\n",
    "    'National_Bridge': 'countofnational_bridge_holidays',\n",
    "    'National_Event': 'countofnational_event_holidays',\n",
    "    'National_Transfer': 'countofnational_transfer_holidays',\n",
    "    'National_Work Day': 'countofnational_work_day_holidays'\n",
    "})\n",
    "\n",
    "# Fill NaN values with 0\n",
    "df_holidays_merged_grouped2 = df_holidays_merged_grouped2.fillna(0)\n",
    "\n",
    "# Let's do an inner join with the original data to get the original date and store_nbr combinations back. Therefore we need to make another dataframe.\n",
    "\n",
    "df_holidays_merged_grouped_inner = holidays_prep_merged(df_holidays, df_stores)\n",
    "df_holidays_merged_grouped_inner = df_holidays_merged_grouped_inner.groupby(['date', 'store_nbr']).size().reset_index().drop(columns=0)\n",
    "df_holidays_merged_grouped2 = df_holidays_merged_grouped2.merge(df_holidays_merged_grouped_inner, on=['date', 'store_nbr'], how='inner')\n",
    "\n",
    "# Convert the count columns to integer\n",
    "df_holidays_merged_grouped2_numerical_columns = df_holidays_merged_grouped2.select_dtypes(include=['float64']).columns\n",
    "df_holidays_merged_grouped2[df_holidays_merged_grouped2_numerical_columns] = df_holidays_merged_grouped2[df_holidays_merged_grouped2_numerical_columns].astype(int)\n",
    "\n",
    "# df_holidays_merged_grouped2 = df_holidays_merged_grouped2.astype({'countoflocalholidays': int, 'countofregionalholidays': int, 'countofnationalholidays': int})\n",
    "\n",
    "print(f'In the orignal unioned holiday dataframe, df_holidays_merged we found (including duplicates) {df_holidays_merged2.shape[0]} rows')\n",
    "print(f'In our new adjusted dataframe we have {df_holidays_merged_grouped2.shape[0]} rows')\n",
    "print(f'Thus, we have removed {df_holidays_merged.shape[0] - df_holidays_merged_grouped2.shape[0]} rows')\n",
    "\n",
    "# Might want to filter out the holiday dates that will never be in de salesdate range. However, they will be left out anyway when joining with the sales data.\n",
    "# df_holidays_merged_grouped2.head(10)\n",
    "\n",
    "df_holidays_merged_grouped2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill newly created NaN columns, due to holiday join, with 'no' on thates where there are now holidays\n",
    "def holidays_fill_no_normal(df):\n",
    "\n",
    "    cat_col = df.select_dtypes(include=[\"category\"]).columns\n",
    "\n",
    "    for col in cat_col:\n",
    "\n",
    "        if \"no\" not in df[col].cat.categories:\n",
    "\n",
    "            df[col] = df[col].cat.add_categories(\"no\")\n",
    "\n",
    "    df[cat_col] = df[cat_col].fillna(\"no\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan code # Old tryout from 9th of August\n",
    "\n",
    "# df_localholidayswithstores = holidays_prep_local(df_holidays, df_stores)\n",
    "# df_localholidayswithstores = df_localholidayswithstores.groupby([\"date\", \"store_nbr\"]).size().reset_index(name=\"countoflocalholidays\")\n",
    "\n",
    "# df_regionalholidayswithstores = holidays_prep_regional(df_holidays, df_stores)\n",
    "# df_regionalholidayswithstores = df_regionalholidayswithstores.groupby([\"date\", \"store_nbr\"]).size().reset_index(name=\"countofregionalholidays\")\n",
    "\n",
    "# df_nationalholidayswithstores = holidays_prep_national(df_holidays, df_stores)\n",
    "# df_nationalholidayswithstores = df_nationalholidayswithstores.groupby([\"date\", \"store_nbr\"]).size().reset_index(name=\"countofnationalholidays\")\n",
    "\n",
    "# df_holidays_merged_duplicatecount = df_holidays_merged.groupby([\"date\", \"store_nbr\"]).size().reset_index(name=\"count\")\n",
    "# df_holidays_merged_duplicatecount = df_holidays_merged_duplicatecount[df_holidays_merged_duplicatecount[\"count\"] > 1]\n",
    "# df_holidays_merged_duplicatecount.sort_values(by=\"store_nbr\", inplace=True)\n",
    "\n",
    "# df_holidays_merged_duplicatecount = df_holidays_merged_duplicatecount.merge(df_localholidayswithstores, on=[\"date\", \"store_nbr\"], how=\"left\")  \n",
    "# df_holidays_merged_duplicatecount = df_holidays_merged_duplicatecount.merge(df_regionalholidayswithstores, on=[\"date\", \"store_nbr\"], how=\"left\")\n",
    "# df_holidays_merged_duplicatecount = df_holidays_merged_duplicatecount.merge(df_nationalholidayswithstores, on=[\"date\", \"store_nbr\"], how=\"left\")\n",
    "\n",
    "# df_holidays_merged_duplicatecount = df_holidays_merged_duplicatecount.fillna('')\n",
    "\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     display(df_holidays_merged_duplicatecount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "def merge_datasets(df_sales, df_items, df_stores, df_holidays):\n",
    "\n",
    "    # Sales prep\n",
    "    df_sales = sales_cleaned(df_sales)\n",
    "\n",
    "    # Holidays prep\n",
    "    df_holidays_merged = holidays_prep_merged(df_holidays, df_stores)\n",
    "\n",
    "    # Stores prep\n",
    "    df_stores = stores_cleaned_renamed(df_stores)\n",
    "\n",
    "    # Items prep\n",
    "    df_items = items_cleaned_renamed(df_items)\n",
    "\n",
    "    # Holidays merge on sales\n",
    "    df_merged = df_sales.merge(df_holidays_merged, on=[\"date\", \"store_nbr\"], how=\"left\")\n",
    "    df_merged = holidays_fill_no_normal(df_merged)\n",
    "\n",
    "    # Stores merged with sales+holidays\n",
    "    df_merged = df_merged.merge(df_stores, on=\"store_nbr\", how=\"left\")\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # To-do: Check if problem is in dtype of item_nbr --> in df_merged or in df_items\n",
    "\n",
    "    print(df_merged[\"item_nbr\"].dtype)\n",
    "    print(df_items[\"item_nbr\"].dtype)\n",
    "\n",
    "    # # Change the dtype for item_nbr from uint32 to int32\n",
    "    df_merged[\"item_nbr\"] = df_merged[\"item_nbr\"].astype(int)\n",
    "    df_items[\"item_nbr\"] = df_items[\"item_nbr\"].astype(int)\n",
    "    print(\"-\" * 30)\n",
    "    print(df_merged[\"item_nbr\"].dtype)\n",
    "    print(df_items[\"item_nbr\"].dtype)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "\n",
    "    # Items merged with sales+holidays+stores\n",
    "    df_merged = df_merged.merge(df_items, on=\"item_nbr\", how=\"left\")\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = merge_datasets(df_sales, df_items, df_stores, df_holidays)  # --> 2.44 GB\n",
    "\n",
    "\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan code\n",
    "# Example of store and item with duplicate rows due to multiple holidays on the same day\n",
    "\n",
    "df_merged_store_25_item_103665 = df_merged[(df_merged[\"store_nbr\"] == 54) & (df_merged[\"item_nbr\"] == 129296)]\n",
    "df_merged_store_25_item_103665 = df_merged_store_25_item_103665[df_merged_store_25_item_103665[\"date\"] == '2014-12-26']\n",
    "\n",
    "df_merged_store_25_item_103665.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan code\n",
    "# Example of store and item with duplicate rows due to multiple holidays on the same day\n",
    "\n",
    "df_merged_store_54_item_129296 = df_merged[(df_merged[\"store_nbr\"] == 54) & (df_merged[\"item_nbr\"] == 129296)]\n",
    "#df_merged_store_54_item_129296 = df_merged_store_25_item_103665[df_merged_store_25_item_103665[\"date\"] == '2014-12-26']\n",
    "\n",
    "df_merged_store_54_item_129296[df_merged_store_54_item_129296[\"unit_sales\"].isnull()]\n",
    "\n",
    "# df_merged_store_54_item_129296.head(50)\n",
    "# Of course this doesn't lead to missing sales, we haven't appended the date range yet to this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.X Brainstorm ideas for imputing missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Forward or Backward fill for NaN values --> items, stores, holidays\n",
    "# df[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "# df[\"sales\"].fillna(method=\"bfill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fill missing values for non-sales columns using forward fill and backward fill --> items, stores, holidays\n",
    "# non_sales_columns = [\n",
    "#     \"holiday_type\",\n",
    "#     \"locale\",\n",
    "#     \"city\",\n",
    "#     \"state\",\n",
    "#     \"type\",\n",
    "#     \"cluster\",\n",
    "#     \"family\",\n",
    "#     \"class\",\n",
    "#     \"perishable\",  # be aware of name changes of columns in step 3.X\n",
    "# ]\n",
    "# df[non_sales_columns] = df.groupby([\"store_nbr\", \"item_nbr\"])[non_sales_columns].apply(\n",
    "#     lambda group: group.ffill().bfill()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.1 Create a date for all missing values/dates and keep the value of sales as NA\n",
    "\n",
    "Action: Create all daily dates in the date range. Date range starts from first available date in df to last available date in df. Then filling missing dates with NaNs for per unique item per unique store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filling_dates_NaN(df):\n",
    "\n",
    "    # Create new df to include all daily dates in the range, filling missing dates with NaNs\n",
    "    df = df.copy()\n",
    "\n",
    "    # Print first and last date of df\n",
    "    print(f'First date in df: {df[\"date\"].min()}')\n",
    "    print(f'Last date in df:  {df[\"date\"].max()}')\n",
    "    print(\"-\" * 71)\n",
    "\n",
    "    # Calculate memory size and shape size of start df\n",
    "    df_mem_start = sys.getsizeof(df)\n",
    "    df_shape_start = df.shape[0] / 1e6\n",
    "    print(\n",
    "        f\"Start size of df:     {round(df_mem_start/1024/1024/1024, 2)} GB and start observations:     {round(df_shape_start, 1)} million.\"\n",
    "    )\n",
    "\n",
    "    # Create a complete date range for the entire dataset\n",
    "    all_dates = pd.date_range(start=df[\"date\"].min(), end=df[\"date\"].max(), freq=\"D\")\n",
    "\n",
    "    # Create a multi-index from all possible combinations of 'item_nbr' and 'date'\n",
    "    all_combinations = pd.MultiIndex.from_product(\n",
    "        [df[\"store_nbr\"].unique(), df[\"item_nbr\"].unique(), all_dates],\n",
    "        names=[\"store_nbr\", \"item_nbr\", \"date\"],\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "    # Check for duplicates in the combination of 'store_nbr', 'item_nbr', and 'date'\n",
    "    duplicate_rows = df[\n",
    "        df.duplicated(subset=[\"store_nbr\", \"item_nbr\", \"date\"], keep=False)\n",
    "    ]\n",
    "    if not duplicate_rows.empty:\n",
    "        print(\n",
    "            \"Warning: Duplicate entries found in the combination of 'store_nbr', 'item_nbr', and 'date'.\"\n",
    "        )\n",
    "        print(f\"Total dublicate rows {duplicate_rows.shape[0]}\")\n",
    "        print(\"-\" * 71)\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Reindex the original DataFrame to include all combinations of 'store_nbr', 'item_nbr', and 'date'\n",
    "    df_reindexed = df.set_index([\"store_nbr\", \"item_nbr\", \"date\"]).reindex(\n",
    "        all_combinations\n",
    "    )\n",
    "\n",
    "    # Reset the index to turn the multi-index back into regular columns\n",
    "    df_final = df_reindexed.reset_index()\n",
    "\n",
    "    # Calculate memory size and shape size of final end df\n",
    "    df_mem_end = sys.getsizeof(df_final)\n",
    "    df_mem_change_perc = ((df_mem_end - df_mem_start) / df_mem_start) * 100\n",
    "    df_mem_change = df_mem_end - df_mem_start\n",
    "\n",
    "    df_shape_end = df_final.shape[0] / 1e6\n",
    "    df_shape_change_perc = ((df_shape_end - df_shape_start) / df_shape_start) * 100\n",
    "    df_shape_change = df_shape_end - df_shape_start\n",
    "\n",
    "    print(\n",
    "        f\"Final size of df:     {round(df_mem_end/1024/1024/1024, 2)} GB and end observations:       {round(df_shape_end, 1)} million.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Change in size of df: {round(df_mem_change_perc, 2)} % and observations:           {round(df_shape_change_perc, 2)}     %.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Increased size of df: {round(df_mem_change/1024/1024/1024, 2)} GB and increased observations: {round(df_shape_change, 1)} million.\"\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 71)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshooting of filling_dates_NaN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filling_dates_NaN_error(df):\n",
    "\n",
    "    # Create new df to include all daily dates in the range, filling missing dates with NaNs\n",
    "    df = df.copy()\n",
    "\n",
    "    # Print first and last date of df\n",
    "    print(f'First date in df: {df[\"date\"].min()}')\n",
    "    print(f'Last date in df:  {df[\"date\"].max()}')\n",
    "    print(\"-\" * 71)\n",
    "\n",
    "    # Calculate memory size and shape size of start df\n",
    "    df_mem_start = sys.getsizeof(df)\n",
    "    df_shape_start = df.shape[0] / 1e6\n",
    "    print(\n",
    "        f\"Start size of df:     {round(df_mem_start/1024/1024/1024, 2)} GB and start observations:     {round(df_shape_start, 1)} million.\"\n",
    "    )\n",
    "\n",
    "    # Create a complete date range for the entire dataset\n",
    "    all_dates = pd.date_range(start=df[\"date\"].min(), end=df[\"date\"].max(), freq=\"D\")\n",
    "\n",
    "    # Create a multi-index from all possible combinations of 'item_nbr' and 'date'\n",
    "    all_combinations = pd.MultiIndex.from_product(\n",
    "        [df[\"store_nbr\"].unique(), df[\"item_nbr\"].unique(), all_dates],\n",
    "        names=[\"store_nbr\", \"item_nbr\", \"date\"],\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "    # Check for duplicates in the combination of 'store_nbr', 'item_nbr', and 'date'\n",
    "    duplicate_rows = df[\n",
    "        df.duplicated(subset=[\"store_nbr\", \"item_nbr\", \"date\"], keep=False)\n",
    "    ]\n",
    "    if not duplicate_rows.empty:\n",
    "        print(\n",
    "            \"Warning: Duplicate entries found in the combination of 'store_nbr', 'item_nbr', and 'date'.\"\n",
    "        )\n",
    "        print(f\"Total dublicate rows {duplicate_rows.shape[0]}\")\n",
    "        print(\"-\" * 71)\n",
    "\n",
    "    return duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = filling_dates_NaN(df_merged)\n",
    "\n",
    "duplicate_rows.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows_store_54_item_129296 = duplicate_rows[(duplicate_rows[\"store_nbr\"] == 54) & (duplicate_rows[\"item_nbr\"] == 129296)]\n",
    "\n",
    "duplicate_rows_store_54_item_129296.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicate_rows_example = filling_dates_NaN_error(df_merged)\n",
    "df_duplicate_rows_example= df_duplicate_rows_example[(df_duplicate_rows_example[\"store_nbr\"] == 22) & (df_duplicate_rows_example[\"item_nbr\"] == 96995)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very memory costly!!! --> will result in huge df\n",
    "df_merged_full_nan = filling_dates_NaN(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.2 Fill newly created dates for non-sales columns using forward fill and backward fill --> items, stores, holidays columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for non-sales columns using forward fill and backward fill --> items, stores, holidays\n",
    "non_sales_columns = [\n",
    "    \"holiday_type\",\n",
    "    \"holiday_locale\",\n",
    "    \"store_city\",\n",
    "    \"store_type\",\n",
    "    \"store_cluster\",\n",
    "    \"item_family\",\n",
    "    \"item_class\",\n",
    "    \"perishable\",\n",
    "]\n",
    "\n",
    "# To-do: test more in individual item level how this works\n",
    "df_merged_full_nan[non_sales_columns] = df_merged_full_nan.groupby(\n",
    "    [\"item_nbr\", \"store_nbr\"]\n",
    ")[non_sales_columns].transform(lambda group: group.ffill().bfill())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2: Detect negative values\n",
    "\n",
    "•\tAction: Delete unit_sales if values are lower than zero --> N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sales_cleaned(df):\n",
    "\n",
    "    # Check the number of negative values before replacement\n",
    "    before_replacement = (df[\"unit_sales\"] < 0).sum()\n",
    "    print(f\"Number of negative values before replacement: {before_replacement}\")\n",
    "\n",
    "    # Create a boolean mask for the negative sales rows to create a 'boolean flag-list' containing all negative rows, used to filter full df_sales df\n",
    "    negative_sales_mask = df[\"unit_sales\"] < 0\n",
    "\n",
    "    # Use the mask to update the flagged 'unit_sales' column in the original DataFrame\n",
    "    df.loc[negative_sales_mask, \"unit_sales\"] = df.loc[\n",
    "        negative_sales_mask, \"unit_sales\"\n",
    "    ].where(df.loc[negative_sales_mask, \"unit_sales\"] >= 0, np.nan)\n",
    "\n",
    "    # Check the number of negative values after replacement\n",
    "    after_replacement = (df[\"unit_sales\"] < 0).sum()\n",
    "    print(f\"Number of negative values after replacement: {after_replacement}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full merged df_merged_full\n",
    "\n",
    "# df_sales_nan = negative_sales_cleaned(df_merged_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check function with df_sales\n",
    "df_sales_nan = negative_sales_cleaned(df_sales)\n",
    "\n",
    "df_sales_nan_check = df_sales_nan[df_sales_nan[\"unit_sales\"].isna()].sort_values(\n",
    "    by=[\"date\", \"store_nbr\", \"item_nbr\"]\n",
    ")\n",
    "\n",
    "df_sales_nan_check.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Define new, old and closed stores\n",
    "\n",
    "•\tCondition: sales for all items a given store and date are NA\n",
    "\n",
    "•\tAction: Impute with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum/Agg all sales group  by store, date\n",
    "# --> Sum_sales > 0 then store_opened\n",
    "# else --> closed --> inputed with 0\n",
    "\n",
    "\n",
    "# TO-do: discuss about closed_store --> inpute with 0 or N/A?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 New product\n",
    "\n",
    "•\tBefore the very first sale of an item, all observations are kept as NA\n",
    "\n",
    "•\tAfter the very first sale of an item, we go to step 3:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum/Agg all sales group  by item, date\n",
    "# --> Sum_sales > 0 then first_sales_day of product\n",
    "# else <first_sales_day of product --> delete unit_sales --> N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8  Stockout on store level\n",
    "\n",
    "•      Perishable good: when there are missing values for two consecutive days for a given item per individual store \n",
    "\n",
    "•      Nonperishable goods: when there are missing values for 7 consecutive days for a given item and per individual store\n",
    "\n",
    "•      Action: Impute with algorithm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perishable good\n",
    "if ['perishable'] == 1 and item_missing_count > 2 #-->  inpute with 0?\n",
    "\n",
    "if ['perishable'] == 1 and item_missing_count <= 2 #-->  inpute with mean? or intrepolate?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#non-perishable good  \n",
    "if ['perishable'] == 0 and and item_missing_count > 7 #-->  inpute with 0?\n",
    "\n",
    "if ['perishable'] == 1 and item_missing_count <= 7 #-->  inpute with mean? or intrepolate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate between missing datapoints --> sales\n",
    "\n",
    "fillna(method=\"mean\")\n",
    "\n",
    "df[\"column_name\"].interpolate(method=\"linear\", inplace=True)\n",
    "\n",
    "df[\"column_name\"].interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "df[\"column_name\"].interpolate(method=\"polynomial\", order=2, inplace=True)\n",
    "\n",
    "# Interpolate missing values for the 'unit_sales' column\n",
    "df[\"unit_sales\"] = df.groupby([\"store_nbr\", \"item_nbr\"])[\"unit_sales\"].apply(\n",
    "    lambda group: group.interpolate(method=\"linear\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 Missing sales data: Zero sales\n",
    "\n",
    "•\tAll other cases\n",
    "\n",
    "•\tAction: Impute with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.X Negative values imputing to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 Promotional Data \n",
    "\n",
    "•   All missing values are interpreted a day with no promotion\n",
    "\n",
    "•   Action: Inpute onpromotion N/A with False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing N/A values in boolean columns with False\n",
    "def sales_fill_onpromotion(df):\n",
    "\n",
    "    df[\"onpromotion\"] = df[\"onpromotion\"].fillna(False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# To-do: when perform this function? Before filling_dates_NaN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged_test = sales_fill_onpromotion(df_merged)\n",
    "\n",
    "# df_merged_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Feature construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.X Extracting datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime_features(df):\n",
    "    \"\"\"\n",
    "    Extracting datetime features\n",
    "    year, month, day of month, weekday (1-7), week number-year, week_year_date\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure the date column is sorted\n",
    "    df = df.sort_values(\"date\")\n",
    "\n",
    "    # df[\"year\"] = df[\"date\"].dt.year\n",
    "    # df[\"month\"] = df[\"date\"].dt.month\n",
    "    # df[\"day\"] = df[\"date\"].dt.day\n",
    "\n",
    "    # Adjusting weekday to start from 1 (Monday) to 7 (Sunday)\n",
    "    df[\"weekday\"] = df[\"date\"].dt.dayofweek + 1\n",
    "\n",
    "    # Adding week number-year feature\n",
    "    df[\"week_number\"] = df[\"date\"].dt.isocalendar().week\n",
    "    df[\"week_year\"] = df[\"week_number\"].astype(str).str.zfill(2) + df[\"year\"].astype(\n",
    "        str\n",
    "    )\n",
    "\n",
    "    # Convert week_year to datetime with monday as startdate of week\n",
    "    df[\"week_year_date\"] = pd.to_datetime(\n",
    "        df[\"year\"].astype(str) + df[\"week_number\"].astype(str).str.zfill(2) + \"1\",\n",
    "        format=\"%Y%W%w\",\n",
    "    )\n",
    "\n",
    "    # Adding trend feature: number of weeks since the start of the dataset\n",
    "    start_date = df[\"date\"].min()\n",
    "    df[\"weeks_since_start\"] = ((df[\"date\"] - start_date).dt.days / 7).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime_features(df):\n",
    "    \"\"\"\n",
    "    Extracting datetime features:\n",
    "    year, month, day of month, weekday (1-7), week number-year, and trend (weeks since start, starting at 1)\n",
    "    \"\"\"\n",
    "    # Ensure the date column is sorted\n",
    "    df = df.copy().sort_values(\"date\")\n",
    "\n",
    "    # Use isocalendar for consistent week-based calculations\n",
    "\n",
    "    iso_calendar = df[\"date\"].dt.isocalendar()\n",
    "\n",
    "    # Year, Month, Day\n",
    "    # df[\"year\"] = iso_calendar.year\n",
    "    # df[\"month\"] = df[\"date\"].dt.month\n",
    "    # df[\"day\"] = df[\"date\"].dt.day\n",
    "\n",
    "    # Weekday (1 = Monday, 7 = Sunday)\n",
    "    df[\"weekday\"] = iso_calendar.day\n",
    "\n",
    "    # Week number\n",
    "    df[\"week_number\"] = iso_calendar.week\n",
    "\n",
    "    # Week-year\n",
    "    df[\"week_year\"] = df[\"week_number\"].astype(str).str.zfill(2) + df[\"year\"].astype(\n",
    "        str\n",
    "    )\n",
    "\n",
    "    # Convert week_year to datetime with monday as startdate of week\n",
    "    df[\"week_year_date\"] = pd.to_datetime(\n",
    "        df[\"year\"].astype(str) + df[\"week_number\"].astype(str).str.zfill(2) + \"1\",\n",
    "        format=\"%Y%W%w\",\n",
    "    )\n",
    "\n",
    "    # First day of the ISO year containing the start date\n",
    "\n",
    "    start_date = df[\"date\"].min()\n",
    "    start_year_first_day = datetime(start_date.year, 1, 1)\n",
    "\n",
    "    # 'search' for first monday of year\n",
    "    while start_year_first_day.isocalendar()[1] != 1:\n",
    "\n",
    "        start_year_first_day = start_year_first_day + pd.Timedelta(days=1)\n",
    "\n",
    "    ##Itemweek number\n",
    "    # Weeks since start (aligned with ISO week numbers)\n",
    "    df[\"weeks_since_start\"] = (\n",
    "        iso_calendar.week + (iso_calendar.year - start_year_first_day.year) * 52\n",
    "    )\n",
    "\n",
    "    # Adjust weeks_since_start to start from 1\n",
    "    df[\"weeks_since_start\"] = (\n",
    "        df[\"weeks_since_start\"] - df[\"weeks_since_start\"].min() + 1\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = extract_datetime_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.X Promotion\n",
    "\n",
    "The number of days a item was on promotion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY FROM OLD NOTEBOOK\n",
    "# TO-DO 1: transform with new df names\n",
    "# TO-DO 2: total promotion days month --> week\n",
    "\n",
    "\n",
    "def onpromotion_month_count(df):\n",
    "\n",
    "    if \"onpromotion\" in df.columns:\n",
    "\n",
    "        df[\"onpromotion_month_count\"] = df.groupby(\n",
    "            [\"item_nbr\", \"store_nbr\", \"day\", \"month\", \"year\"]\n",
    "        )[\"onpromotion\"].transform(\"sum\")\n",
    "\n",
    "        print(\n",
    "            \"Change: 'onpromotion' column transformed to 'onpromotion_month_count' feature.\"\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        print(\"The DataFrame does not contain an 'onpromotion' column.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_agg = (\n",
    "    onpromotion_month_count(df_0)  # Transformation to 'onpromotion_month_count' feature\n",
    "    .drop(\n",
    "        columns=[\"id\", \"date\", \"onpromotion\"]\n",
    "    )  # Drop unnecessary columns \"id\", \"date\", \"onpromotion\"\n",
    "    .groupby([\"month\", \"year\", \"store_nbr\", \"item_nbr\"])\n",
    "    .agg({\"unit_sales\": \"sum\", \"onpromotion_month_count\": \"sum\"})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.X Store closed on 25-12 and 01-01 \n",
    "\n",
    "STore closed in between when inputed with 0\n",
    "\n",
    "--> can we also use this feature to include the excluded stores with >9 days data, due to closing or later openings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_case_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
