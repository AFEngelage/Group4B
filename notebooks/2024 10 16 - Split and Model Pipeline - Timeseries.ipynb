{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporacion Favorita - New Superb Forecasting Model - \n",
    "\n",
    "## Split and Model Pipeline\n",
    "\n",
    "#codi\n",
    "\n",
    "Made by 4B Consultancy (Janne Heuvelmans, Georgi Duev, Alexander Engelage, Sebastiaan de Bruin) - 2024\n",
    "\n",
    "In this data pipeline, \n",
    "\n",
    "The following steps are made within this notebook:  \n",
    "\n",
    ">-0. Import Packages \n",
    "\n",
    ">-1. Load final dataset and aggregate dataset to weekly level\n",
    "    -1.1 Load final dataset made in Data Preperation Pipeline Notebook\n",
    "    -1.2 Aggregate dataset to weekly level\n",
    "\n",
    ">-2. Column transformers and Train, Test, Validation Split\n",
    "\n",
    ">-3. Models\n",
    "\n",
    ">-4. Pick best model one and optimize with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import os\n",
    "import sys\n",
    "import altair as alt\n",
    "import vegafusion as vf\n",
    "import sklearn\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load final dataset, Inpute Stockouts and Aggregate dataset to weekly level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Functions - Import raw data from local PATH\n",
    "Create import data function and give basic information function within the importing function.\n",
    "\n",
    "Return basic information on each dataframe:  \n",
    "- a) Information on the number of observation and features.  \n",
    "- b) Information on the size of the dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO: Import via polars, and use polars dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_data_and_info(import_path, file_name=\"df_final\"):\n",
    "\n",
    "    print(f\"\\nReading file {file_name}\\n\")\n",
    "\n",
    "    # Load data.\n",
    "    df = pd.read_parquet(import_path + file_name + \".parquet\")\n",
    "\n",
    "    # Getting the basic information of the dataframe (number of observations and features, and size)\n",
    "    print(\n",
    "        f\"The '{file_name}' dataframe contains: {df.shape[0]:,}\".replace(\",\", \".\")\n",
    "        + f\" observations and {df.shape[1]} features.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Prepared and transformed dataframe has optimized size of {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB.\"\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Importing raw data\n",
    "Importing parquet files with importing function (giving basic information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading file Prepped_data_20241004\n",
      "\n",
      "The 'Prepped_data_20241004' dataframe contains: 67.834.270 observations and 19 features.\n",
      "Prepared and transformed dataframe has optimized size of 2.72 GB.\n"
     ]
    }
   ],
   "source": [
    "import_path = \"C:/Users/alexander/Documents/0. Data Science and AI for Experts/EAISI_4B_Supermarket/data/processed/\"\n",
    "\n",
    "\n",
    "# import_path = \"C:/Users/sebas/OneDrive/Documenten/GitHub/Supermarketcasegroupproject/Group4B/data/raw/\n",
    "\n",
    "\n",
    "# Importing final df\n",
    "\n",
    "\n",
    "df_final = f_get_data_and_info(import_path, file_name=\"Prepped_data_20241004\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: include null_count print dunction into importing OR make basic descrption function with features, size, null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 67834270 entries, 0 to 67834269\n",
      "Data columns (total 19 columns):\n",
      " #   Column                  Dtype         \n",
      "---  ------                  -----         \n",
      " 0   store_nbr               uint8         \n",
      " 1   item_nbr                int32         \n",
      " 2   date                    datetime64[ns]\n",
      " 3   unit_sales              float32       \n",
      " 4   onpromotion             bool          \n",
      " 5   holiday_local_count     int8          \n",
      " 6   holiday_national_count  int8          \n",
      " 7   holiday_regional_count  int8          \n",
      " 8   store_type              category      \n",
      " 9   store_cluster           uint8         \n",
      " 10  item_family             category      \n",
      " 11  item_class              uint16        \n",
      " 12  perishable              uint8         \n",
      " 13  store_status            int8          \n",
      " 14  item_status             int8          \n",
      " 15  year                    int16         \n",
      " 16  weekday                 int8          \n",
      " 17  week_nbr                int8          \n",
      " 18  week_number_cum         int16         \n",
      "dtypes: bool(1), category(2), datetime64[ns](1), float32(1), int16(2), int32(1), int8(7), uint16(1), uint8(3)\n",
      "memory usage: 2.7 GB\n",
      "Column 'store_nbr' has 0 null values.\n",
      "Column 'item_nbr' has 0 null values.\n",
      "Column 'date' has 0 null values.\n",
      "Column 'unit_sales' has 39275809 null values.\n",
      "Column 'onpromotion' has 0 null values.\n",
      "Column 'holiday_local_count' has 0 null values.\n",
      "Column 'holiday_national_count' has 0 null values.\n",
      "Column 'holiday_regional_count' has 0 null values.\n",
      "Column 'store_type' has 0 null values.\n",
      "Column 'store_cluster' has 0 null values.\n",
      "Column 'item_family' has 0 null values.\n",
      "Column 'item_class' has 0 null values.\n",
      "Column 'perishable' has 0 null values.\n",
      "Column 'store_status' has 0 null values.\n",
      "Column 'item_status' has 0 null values.\n",
      "Column 'year' has 0 null values.\n",
      "Column 'weekday' has 0 null values.\n",
      "Column 'week_nbr' has 0 null values.\n",
      "Column 'week_number_cum' has 0 null values.\n"
     ]
    }
   ],
   "source": [
    "df_final.info()\n",
    "# Count nulls per column\n",
    "null_counts = df_final.isnull().sum()\n",
    "\n",
    "# Print results\n",
    "for column, count in null_counts.items():\n",
    "    print(f\"Column '{column}' has {count} null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Train test val split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKtime\n",
    "\n",
    "ExpandingWindowSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TO-DO: Selecting on weeks or via data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"store_nbr\",\n",
    "    \"item_nbr\",\n",
    "    \"onpromotion\",\n",
    "    \"holiday_local_count\",\n",
    "    \"holiday_national_count\",\n",
    "    \"holiday_regional_count\",\n",
    "    \"store_type\",\n",
    "    \"store_cluster\",\n",
    "    \"item_family\",\n",
    "    \"item_class\",\n",
    "    \"perishable\",\n",
    "    \"store_status\",\n",
    "    \"item_status\",\n",
    "    \"year\",\n",
    "    # \"week_nbr\",\n",
    "    \"week_number_cum\",\n",
    "]\n",
    "\n",
    "target_variable = [\"unit_sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(df, features, target_variable, window_length=26):\n",
    "\n",
    "    # Sort the DataFrame by store number, item number, and date for ordering\n",
    "    df = df.sort_values([\"store_nbr\", \"item_nbr\", \"week_number_cum\"])\n",
    "\n",
    "    # Create X (features) and y (target)\n",
    "    X = df[features]\n",
    "    y = df[target_variable]\n",
    "\n",
    "    # Get the maximum week in the dataset\n",
    "    max_week = df[\"week_number_cum\"].max()\n",
    "\n",
    "    # Calculate start and end weeks for validation and test sets\n",
    "    test_week_start = max_week - window_length + 1\n",
    "\n",
    "    val_week_start = max_week - 2 * window_length + 1\n",
    "\n",
    "    val_week_end = test_week_start - 1\n",
    "\n",
    "    train_week_end = val_week_start - 1\n",
    "\n",
    "    # 1. Train data: All data before the start of the validation period\n",
    "    X_train = X[X[\"week_number_cum\"] <= train_week_end]\n",
    "    y_train = y[df[\"week_number_cum\"] <= train_week_end]\n",
    "\n",
    "    # 2. Val data: From `val_week_start` to `val_week_end`\n",
    "    X_val = X[\n",
    "        (X[\"week_number_cum\"] >= val_week_start)\n",
    "        & (X[\"week_number_cum\"] <= val_week_end)\n",
    "    ]\n",
    "    y_val = y[\n",
    "        (df[\"week_number_cum\"] >= val_week_start)\n",
    "        & (df[\"week_number_cum\"] <= val_week_end)\n",
    "    ]\n",
    "\n",
    "    # 3. Test data: From `test_week_start` to `max_week`\n",
    "    X_test = X[\n",
    "        (X[\"week_number_cum\"] >= test_week_start) & (X[\"week_number_cum\"] <= max_week)\n",
    "    ]\n",
    "    y_test = y[\n",
    "        (df[\"week_number_cum\"] >= test_week_start) & (df[\"week_number_cum\"] <= max_week)\n",
    "    ]\n",
    "\n",
    "    # Function to print split information\n",
    "    def print_split_info(split_name, X_split, y_split):\n",
    "        print(f\"\\n{split_name} set:\")\n",
    "        print(f\"X_{split_name.lower()} shape: {X_split.shape}\")\n",
    "        print(f\"y_{split_name.lower()} shape: {y_split.shape}\")\n",
    "        print(f\"{split_name} Min Week: {X_split['week_number_cum'].min()}\")\n",
    "        print(f\"{split_name} Max Week: {X_split['week_number_cum'].max()}\")\n",
    "        print(f\"{split_name} number of weeks: {X_split['week_number_cum'].nunique()}\")\n",
    "        print(f\"Number of stores: {X_split['store_nbr'].nunique()}\")\n",
    "        print(f\"Number of items: {X_split['item_nbr'].nunique()}\")\n",
    "\n",
    "    # Print information about the splits\n",
    "    print_split_info(\"Train\", X_train, y_train)\n",
    "    print_split_info(\"Validation\", X_val, y_val)\n",
    "    print_split_info(\"Test\", X_test, y_test)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set:\n",
      "X_train shape: (53398880, 15)\n",
      "y_train shape: (53398880, 1)\n",
      "Train Min Week: 1\n",
      "Train Max Week: 190\n",
      "Train number of weeks: 190\n",
      "Number of stores: 10\n",
      "Number of items: 4021\n",
      "\n",
      "Validation set:\n",
      "X_validation shape: (7318220, 15)\n",
      "y_validation shape: (7318220, 1)\n",
      "Validation Min Week: 191\n",
      "Validation Max Week: 216\n",
      "Validation number of weeks: 26\n",
      "Number of stores: 10\n",
      "Number of items: 4021\n",
      "\n",
      "Test set:\n",
      "X_test shape: (7117170, 15)\n",
      "y_test shape: (7117170, 1)\n",
      "Test Min Week: 217\n",
      "Test Max Week: 242\n",
      "Test number of weeks: 26\n",
      "Number of stores: 10\n",
      "Number of items: 4021\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(\n",
    "    df_final, features, target_variable, window_length=26\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: do we split based on dates or based on weeks since start?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_test, y_test, X_val, y_val = train_test_val_split(\n",
    "#     df_final, train_end=\"2016-06-01\", test_end=\"2017-01-01\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Functions - Impute stockouts and Aggregate dataset to weekly level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Impute stockouts\n",
    "\n",
    "Stockout on store level\n",
    "\n",
    "•      Perishable good: when there are missing values for two consecutive days for a given item per individual store \n",
    "\n",
    "•      Nonperishable goods: when there are missing values for 7 consecutive days for a given item and per individual store\n",
    "\n",
    "•      Action: Impute with Rolling Mean with defeault window of 7 days \n",
    "\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_stockouts_polars(df_pandas, window_size=7):\n",
    "\n",
    "    # Convert the input Pandas DataFrame to a Polars DataFrame\n",
    "    df = pl.from_pandas(df_pandas)\n",
    "\n",
    "    # Sort the DataFrame by store number, item number, and date for ordering\n",
    "\n",
    "    df = df.sort([\"store_nbr\", \"item_nbr\", \"date\"])\n",
    "\n",
    "    # Nested function calc_missing_count to calculate the count of consecutive missing values in unit_sales\n",
    "\n",
    "    def calc_missing_count(unit_sales):\n",
    "\n",
    "        return (\n",
    "            unit_sales.is_null()  # Check for null values\n",
    "            .cast(pl.Int32)  # Cast to integer (1 for null, 0 for not null)\n",
    "            .cum_sum()  # Cumulative sum to count sequential nulls\n",
    "            .over([\"store_nbr\", \"item_nbr\"])  # Group by store_nbr and item_nbr\n",
    "        )\n",
    "\n",
    "    # Nested function to Inpute with rolling mean for missing values\n",
    "    def rolling_mean_imputation(unit_sales, window_size):\n",
    "\n",
    "        return (\n",
    "            unit_sales.rolling_mean(\n",
    "                window_size=window_size, min_periods=1\n",
    "            )  # Impute strategy based on rolling mean\n",
    "            .shift(\n",
    "                1\n",
    "            )  # Shift window by one day, to prevent taking the same day into account\n",
    "            .over([\"store_nbr\", \"item_nbr\"])  # Group by store_nbr and item_nbr\n",
    "        )\n",
    "\n",
    "    # Apply the imputation logic based on the perishable status of the items\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.when(pl.col(\"perishable\") == 1)  # Check if the item is perishable = 1\n",
    "            .then(\n",
    "                pl.when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) == 1\n",
    "                )  # 1 missing value\n",
    "                .then(0)  # --> Impute with 0\n",
    "                .when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) > 2\n",
    "                )  # More than 2 missing values\n",
    "                .then(0)  # --> Impute with 0\n",
    "                .when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) == 2\n",
    "                )  # = 2 missing values\n",
    "                .then(\n",
    "                    rolling_mean_imputation(pl.col(\"unit_sales\"), window_size)\n",
    "                )  # --> Inpute with rolling mean for 2 missing days\n",
    "                .otherwise(pl.col(\"unit_sales\"))  # Otherwise keep original value\n",
    "            )\n",
    "            .when(pl.col(\"perishable\") == 0)  # If the item is not perishable = 0\n",
    "            .then(\n",
    "                pl.when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) > 7\n",
    "                )  # More than 7 missing values\n",
    "                .then(0)  # --> Impute with 0\n",
    "                .when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) <= 7\n",
    "                )  # if less 7 missing values\n",
    "                .then(\n",
    "                    rolling_mean_imputation(pl.col(\"unit_sales\"), window_size)\n",
    "                )  # --> Inpute with rolling mean for missing 7 or less days\n",
    "                .otherwise(pl.col(\"unit_sales\"))  # Otherwise keep original value\n",
    "            )\n",
    "            .otherwise(pl.col(\"unit_sales\"))  # For any other case not covered\n",
    "            .alias(\"unit_sales\")  # Alias the new column as 'unit_sales'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Convert Polars df back to Pandas df\n",
    "    df = df.to_pandas()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def impute_stockouts_(df):\n",
    "\n",
    "#     df = impute_stockouts_polars(df, window_size=7)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Aggregate dataset to weekly level\n",
    "\n",
    "- Group the DataFrame by store number, item number, year, and week_cum_number, then aggregate the columns\n",
    "--> \"unit_sales\",\"onpromotion\", \"holiday_local_count\",\"holiday_regional_count\",\"holiday_national_count\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_week(df):\n",
    "\n",
    "    # Sort the DataFrame by store number, item number, and date for ordering\n",
    "    df = df.sort_values([\"store_nbr\", \"item_nbr\", \"year\", \"week_nbr\"])\n",
    "\n",
    "    # Group by the specified columns and aggregate\n",
    "    df = (\n",
    "        df.groupby(\n",
    "            [\n",
    "                \"store_nbr\",\n",
    "                \"item_nbr\",\n",
    "                \"year\",\n",
    "                \"week_number_cum\",  # Aggregating by week_number_cum\n",
    "            ]\n",
    "        )\n",
    "        .agg(\n",
    "            {\n",
    "                \"unit_sales\": \"sum\",\n",
    "                \"onpromotion\": \"sum\",\n",
    "                \"holiday_local_count\": \"sum\",\n",
    "                \"holiday_regional_count\": \"sum\",\n",
    "                \"holiday_national_count\": \"sum\",\n",
    "                \"date\": \"first\",  # Keep the first day of week, needed to run Timeseries models from SKtime\n",
    "                \"store_type\": \"first\",  # Keep the first occurrence of store_type\n",
    "                \"store_cluster\": \"first\",  # Keep the first occurrence of store_cluster\n",
    "                \"item_family\": \"first\",  # Keep the first occurrence of item_family\n",
    "                \"item_class\": \"first\",  # Keep the first occurrence of item_class\n",
    "                \"perishable\": \"first\",  # Keep the first occurrence of perishable\n",
    "                \"store_status\": \"last\",  # Keep the last occurrence of store_status\n",
    "                \"item_status\": \"last\",  # Keep the last occurrence of item_status\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_agg = aggregate_week(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.X Timeseries forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.1. Drop Unneeded columns, only keep ones for timeseries forecasting\n",
    "Not a fancy way, later need to think about how to run this in the same pipeline as the ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unnecessary_columns_timeseries(df):\n",
    "\n",
    "    # List of columns to keep\n",
    "    columns_to_keep = [\"date\", \"week_number_cum\", \"store_nbr\", \"item_nbr\", \"unit_sales\"]\n",
    "\n",
    "    # Drop all other columns and keep only the specified ones\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X.2.1 Pandas: Creating lagged and rolling mean features for each item-store combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lagged_features(df, lag_shift=2, rolling_mean_window=3):\n",
    "\n",
    "    # Sort the DataFrame by store number, item number, and date for ordering\n",
    "    df = df.sort_values([\"store_nbr\", \"item_nbr\", \"week_number_cum\"])\n",
    "\n",
    "    # Lag by the specified number of weeks\n",
    "    df[\"predicted last\"] = df[\"sales\"].shift(lag_shift)\n",
    "\n",
    "    # Rolling mean (t-2, t-3, t-4)\n",
    "    df[\"predicted mean\"] = (\n",
    "        df[\"sales\"].shift(lag_shift).rolling(window=rolling_mean_window).mean()\n",
    "    )\n",
    "\n",
    "    # Group by (item_nbr, store_nbr) and apply the lagging and rolling features\n",
    "    df = (\n",
    "        df.groupby([\"item_nbr\", \"store_nbr\"], group_keys=False)\n",
    "        .apply(lambda x: x)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X.2.1 Polars: Creating lagged and rolling mean features for each item-store combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add lagged features and rolling mean using Polars\n",
    "def add_lagged_features_timeseries_polars(\n",
    "    df_pandas, lag_shift=2, rolling_mean_window=3\n",
    "):\n",
    "\n",
    "    # Convert the input Pandas DataFrame to a Polars DataFrame\n",
    "    df = pl.from_pandas(df_pandas)\n",
    "\n",
    "    # Sort the DataFrame by store number, item number, and date for orderings\n",
    "    df = df.sort([\"store_nbr\", \"item_nbr\", \"week_number_cum\"])\n",
    "\n",
    "    # Apply the lag and rolling features\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            # Lagged sales\n",
    "            pl.col(\"unit_sales\")\n",
    "            .shift(lag_shift)\n",
    "            .alias(\"predicted last\"),  # Shift by 2 weeks for lag\n",
    "            # Rolling mean of lagged sales\n",
    "            pl.col(\"unit_sales\")\n",
    "            .shift(lag_shift)\n",
    "            .rolling_mean(window_size=rolling_mean_window)\n",
    "            .alias(\"predicted mean\"),  # Rolling mean (t-2, t-3, t-4)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Convert Polars df back to Pandas df\n",
    "    df = df.to_pandas()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 67834270 entries, 0 to 67834269\n",
      "Data columns (total 19 columns):\n",
      " #   Column                  Dtype         \n",
      "---  ------                  -----         \n",
      " 0   store_nbr               uint8         \n",
      " 1   item_nbr                int32         \n",
      " 2   date                    datetime64[ns]\n",
      " 3   unit_sales              float32       \n",
      " 4   onpromotion             bool          \n",
      " 5   holiday_local_count     int8          \n",
      " 6   holiday_national_count  int8          \n",
      " 7   holiday_regional_count  int8          \n",
      " 8   store_type              category      \n",
      " 9   store_cluster           uint8         \n",
      " 10  item_family             category      \n",
      " 11  item_class              uint16        \n",
      " 12  perishable              uint8         \n",
      " 13  store_status            int8          \n",
      " 14  item_status             int8          \n",
      " 15  year                    int16         \n",
      " 16  weekday                 int8          \n",
      " 17  week_nbr                int8          \n",
      " 18  week_number_cum         int16         \n",
      "dtypes: bool(1), category(2), datetime64[ns](1), float32(1), int16(2), int32(1), int8(7), uint16(1), uint8(3)\n",
      "memory usage: 2.7 GB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_functions(df):\n",
    "\n",
    "    df = impute_stockouts_polars(df, window_size=7)\n",
    "\n",
    "    df = aggregate_week(df)\n",
    "\n",
    "    df = drop_unnecessary_columns_timeseries(df)\n",
    "\n",
    "    df = add_lagged_features_timeseries_polars(df, lag_shift=2, rolling_mean_window=3)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries = func_functions(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>week_number_cum</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>predicted last</th>\n",
       "      <th>predicted mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>26.666666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>36.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>2013-01-14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>26.666666</td>\n",
       "      <td>8.888890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>2013-01-21</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>38.571426</td>\n",
       "      <td>36.533333</td>\n",
       "      <td>21.066668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>2013-01-28</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>29.571428</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>32.066669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>2013-02-04</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>39.571430</td>\n",
       "      <td>38.571426</td>\n",
       "      <td>36.034924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>2013-02-11</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>29.571428</td>\n",
       "      <td>33.714287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>2013-02-18</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>27.142857</td>\n",
       "      <td>39.571430</td>\n",
       "      <td>35.904766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>2013-02-25</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>30.333334</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>32.047623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>2013-03-04</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>34.428570</td>\n",
       "      <td>27.142857</td>\n",
       "      <td>31.238098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>2013-03-11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>59.142857</td>\n",
       "      <td>30.333334</td>\n",
       "      <td>28.158731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>2013-03-18</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>37.900002</td>\n",
       "      <td>34.428570</td>\n",
       "      <td>30.634924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>2013-03-25</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>31.100000</td>\n",
       "      <td>59.142857</td>\n",
       "      <td>41.301590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.900002</td>\n",
       "      <td>43.823811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>2013-04-08</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.100000</td>\n",
       "      <td>42.714291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>2013-04-15</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>2013-04-22</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.366672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>2013-05-06</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>2013-05-13</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>2013-05-20</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>2013-05-27</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>2013-06-03</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>2013-06-17</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>2013-06-24</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>2013-07-08</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>2013-07-15</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>2013-08-05</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>2013-08-12</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>2013-08-19</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>2013-08-26</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>2013-09-02</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>2013-09-16</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>2013-09-23</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  week_number_cum  store_nbr  item_nbr  unit_sales  \\\n",
       "1210 2013-01-02                1          1    105574   26.666666   \n",
       "1211 2013-01-07                2          1    105574   36.533333   \n",
       "1212 2013-01-14                3          1    105574   33.000000   \n",
       "1213 2013-01-21                4          1    105574   38.571426   \n",
       "1214 2013-01-28                5          1    105574   29.571428   \n",
       "1215 2013-02-04                6          1    105574   39.571430   \n",
       "1216 2013-02-11                7          1    105574   27.000000   \n",
       "1217 2013-02-18                8          1    105574   27.142857   \n",
       "1218 2013-02-25                9          1    105574   30.333334   \n",
       "1219 2013-03-04               10          1    105574   34.428570   \n",
       "1220 2013-03-11               11          1    105574   59.142857   \n",
       "1221 2013-03-18               12          1    105574   37.900002   \n",
       "1222 2013-03-25               13          1    105574   31.100000   \n",
       "1223 2013-04-01               14          1    105574    0.000000   \n",
       "1224 2013-04-08               15          1    105574    0.000000   \n",
       "1225 2013-04-15               16          1    105574    0.000000   \n",
       "1226 2013-04-22               17          1    105574    0.000000   \n",
       "1227 2013-04-29               18          1    105574    0.000000   \n",
       "1228 2013-05-06               19          1    105574    0.000000   \n",
       "1229 2013-05-13               20          1    105574    0.000000   \n",
       "1230 2013-05-20               21          1    105574    0.000000   \n",
       "1231 2013-05-27               22          1    105574    0.000000   \n",
       "1232 2013-06-03               23          1    105574    0.000000   \n",
       "1233 2013-06-10               24          1    105574    0.000000   \n",
       "1234 2013-06-17               25          1    105574    0.000000   \n",
       "1235 2013-06-24               26          1    105574    0.000000   \n",
       "1236 2013-07-01               27          1    105574    0.000000   \n",
       "1237 2013-07-08               28          1    105574    0.000000   \n",
       "1238 2013-07-15               29          1    105574    0.000000   \n",
       "1239 2013-07-22               30          1    105574    0.000000   \n",
       "1240 2013-07-29               31          1    105574    0.000000   \n",
       "1241 2013-08-05               32          1    105574    0.000000   \n",
       "1242 2013-08-12               33          1    105574    0.000000   \n",
       "1243 2013-08-19               34          1    105574    0.000000   \n",
       "1244 2013-08-26               35          1    105574    0.000000   \n",
       "1245 2013-09-02               36          1    105574    0.000000   \n",
       "1246 2013-09-09               37          1    105574    0.000000   \n",
       "1247 2013-09-16               38          1    105574    0.000000   \n",
       "1248 2013-09-23               39          1    105574    0.000000   \n",
       "1249 2013-09-30               40          1    105574    0.000000   \n",
       "\n",
       "      predicted last  predicted mean  \n",
       "1210        0.000000        0.000001  \n",
       "1211        0.000000        0.000001  \n",
       "1212       26.666666        8.888890  \n",
       "1213       36.533333       21.066668  \n",
       "1214       33.000000       32.066669  \n",
       "1215       38.571426       36.034924  \n",
       "1216       29.571428       33.714287  \n",
       "1217       39.571430       35.904766  \n",
       "1218       27.000000       32.047623  \n",
       "1219       27.142857       31.238098  \n",
       "1220       30.333334       28.158731  \n",
       "1221       34.428570       30.634924  \n",
       "1222       59.142857       41.301590  \n",
       "1223       37.900002       43.823811  \n",
       "1224       31.100000       42.714291  \n",
       "1225        0.000000       23.000006  \n",
       "1226        0.000000       10.366672  \n",
       "1227        0.000000        0.000004  \n",
       "1228        0.000000        0.000004  \n",
       "1229        0.000000        0.000004  \n",
       "1230        0.000000        0.000004  \n",
       "1231        0.000000        0.000004  \n",
       "1232        0.000000        0.000004  \n",
       "1233        0.000000        0.000004  \n",
       "1234        0.000000        0.000004  \n",
       "1235        0.000000        0.000004  \n",
       "1236        0.000000        0.000004  \n",
       "1237        0.000000        0.000004  \n",
       "1238        0.000000        0.000004  \n",
       "1239        0.000000        0.000004  \n",
       "1240        0.000000        0.000004  \n",
       "1241        0.000000        0.000004  \n",
       "1242        0.000000        0.000004  \n",
       "1243        0.000000        0.000004  \n",
       "1244        0.000000        0.000004  \n",
       "1245        0.000000        0.000004  \n",
       "1246        0.000000        0.000004  \n",
       "1247        0.000000        0.000004  \n",
       "1248        0.000000        0.000004  \n",
       "1249        0.000000        0.000004  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_timeseries[\n",
    "    (df_timeseries[\"item_nbr\"] == 105574)  # (df_timeseries[\"unit_sales\"] != 0)\n",
    "].head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected '(' (137546471.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[23], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def stop\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected '('\n"
     ]
    }
   ],
   "source": [
    "def stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X.4 Evaulation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MAPE function\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Percentage Error (MAPE).\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    nonzero_mask = y_true != 0  # Avoid division by zero\n",
    "    return (\n",
    "        np.mean(\n",
    "            np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])\n",
    "        )\n",
    "        * 100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAPE for the test last 10 observations for each item-store group\n",
    "mape_results = df.groupby([\"item\", \"store\"]).apply(\n",
    "    lambda group: pd.Series(\n",
    "        {\n",
    "            \"MAPE Mean\": mean_absolute_percentage_error(\n",
    "                group[\"sales\"].tail(10), group[\"Predicted Mean\"].tail(10)\n",
    "            ),\n",
    "            \"MAPE Last\": mean_absolute_percentage_error(\n",
    "                group[\"sales\"].tail(10), group[\"Predicted Last\"].tail(10)\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compute final average MAPE across all item-store combinations\n",
    "final_mape_mean = mape_results[\"MAPE Mean\"].mean()\n",
    "final_mape_last = mape_results[\"MAPE Last\"].mean()\n",
    "\n",
    "print(\"Final Average MAPE (Mean Forecast): {:.2f}%\".format(final_mape_mean))\n",
    "print(\"Final Average MAPE (Last Forecast): {:.2f}%\".format(final_mape_last))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    accuracy = 100 - mape\n",
    "    bias = np.mean(y_pred - y_true)\n",
    "\n",
    "    return mape, accuracy, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_grouped_polars(df):\n",
    "\n",
    "    # Convert the input Pandas DataFrame to a Polars DataFrame\n",
    "    df = pl.from_pandas(df)\n",
    "\n",
    "    def calculate_group_metrics(group):\n",
    "        y_true = group.filter(pl.col(\"unit_sales\") != 0)[\n",
    "            \"unit_sales\"\n",
    "        ]  # Avoid division by zero for MAPE\n",
    "        metrics = []\n",
    "\n",
    "        for col in [\"predicted_last\", \"predicted_mean\"]:\n",
    "            y_pred = group.filter(pl.col(\"unit_sales\") != 0)[\n",
    "                col\n",
    "            ]  # Avoid division by zero for MAPE\n",
    "\n",
    "            # Use evaluate_model function to get metrics\n",
    "            mape, accuracy, bias = evaluate_model(y_true.to_numpy(), y_pred.to_numpy())\n",
    "\n",
    "            metrics.append(\n",
    "                pl.DataFrame(\n",
    "                    {\n",
    "                        \"prediction_type\": [col],\n",
    "                        \"MAPE\": [mape],\n",
    "                        \"Accuracy\": [accuracy],\n",
    "                        \"Bias\": [bias],\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return pl.concat(metrics)\n",
    "\n",
    "    # Group by 'item_nbr' and 'store_nbr' and calculate metrics\n",
    "    metrics_df = (\n",
    "        df.group_by(\"store_nbr\", \"item_nbr\")\n",
    "        .apply(calculate_group_metrics)\n",
    "        .explode([\"prediction_type\", \"MAPE\", \"Accuracy\", \"Bias\"])\n",
    "    )\n",
    "\n",
    "    # Calculate the final average MAPE across all item-store combinations\n",
    "    final_average_mape = metrics_df[\"MAPE\"].mean()\n",
    "\n",
    "    print(\n",
    "        f\"Final average MAPE across all item-store combinations: {final_average_mape:.2f}%\"\n",
    "    )\n",
    "    # print(grouped_metrics)\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate metrics on Polars DataFrame columns grouped by item and store\n",
    "def calculate_metrics_grouped_polars(df):\n",
    "    # Convert the input Pandas DataFrame to a Polars DataFrame\n",
    "    df = pl.from_pandas(df)\n",
    "\n",
    "    grouped_metrics = []\n",
    "\n",
    "    # Group by 'item_nbr' and 'store_nbr' and iterate through each group\n",
    "    for group in df.groupby([\"store_nbr\", \"item_nbr\"]):\n",
    "        store = group[\"store_nbr\"][0]\n",
    "        item = group[\"item_nbr\"][0]\n",
    "\n",
    "        # Filter out rows where 'unit_sales' is zero to avoid division errors in MAPE\n",
    "        filtered_group = group.filter(pl.col(\"unit_sales\") != 0)\n",
    "        y_true = filtered_group[\"unit_sales\"]\n",
    "\n",
    "        for col in [\"predicted_last\", \"predicted_mean\"]:\n",
    "            y_pred = filtered_group[col]\n",
    "\n",
    "            # Use evaluate_model function to get metrics\n",
    "            mape, accuracy, bias = evaluate_model(y_true.to_numpy(), y_pred.to_numpy())\n",
    "\n",
    "            grouped_metrics.append(\n",
    "                {\n",
    "                    \"store_nbr\": store,\n",
    "                    \"item_nbr\": item,\n",
    "                    \"prediction_type\": col,\n",
    "                    \"MAPE\": mape,\n",
    "                    \"Accuracy\": accuracy,\n",
    "                    \"Bias\": bias,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Convert the metrics list to a Polars DataFrame\n",
    "    metrics_df = pl.DataFrame(grouped_metrics)\n",
    "\n",
    "    # Calculate the final average MAPE across all item-store combinations\n",
    "    final_average_mape = metrics_df[\"MAPE\"].mean()\n",
    "\n",
    "    print(\n",
    "        f\"Final average MAPE across all item-store combinations: {final_average_mape:.2f}%\"\n",
    "    )\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'groupby'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics_grouped_polars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_timeseries\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[48], line 9\u001b[0m, in \u001b[0;36mcalculate_metrics_grouped_polars\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      6\u001b[0m grouped_metrics \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Group by 'item_nbr' and 'store_nbr' and iterate through each group\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_nbr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_nbr\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     10\u001b[0m     store \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_nbr\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     11\u001b[0m     item \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_nbr\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'groupby'"
     ]
    }
   ],
   "source": [
    "metrics_df = calculate_metrics_grouped_polars(df_timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Column transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Column transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"store_nbr\",\n",
    "    \"item_nbr\",\n",
    "    \"onpromotion\",\n",
    "    \"holiday_local_count\",\n",
    "    \"holiday_national_count\",\n",
    "    \"holiday_regional_count\",\n",
    "    \"store_type\",\n",
    "    \"store_cluster\",\n",
    "    \"item_family\",\n",
    "    \"item_class\",\n",
    "    \"perishable\",\n",
    "    \"store_status\",\n",
    "    \"item_status\",\n",
    "    \"year\",\n",
    "    \"week_nbr\",\n",
    "    \"week_number_cum\",\n",
    "]\n",
    "\n",
    "target_variable = [\"unit_sales\"]\n",
    "\n",
    "X = df[features]\n",
    "y = df[target_variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: Do we need onehotencoder? --> then needed to seperate between timeseries en ML models\n",
    "\n",
    "To-do: Change catagory dtypes from store_type and item_family just to numbers in prep pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Create your transformers\n",
    "num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "cat_features = X.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# Create a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"ImputeStockouts\",  # Impute stockouts\n",
    "            FunctionTransformer(impute_stockouts_polars),\n",
    "            num_features.tolist() + cat_features.tolist(),\n",
    "        ),\n",
    "        (\n",
    "            \"AggregateWeek\",  # Aggregate dataset to weekly level\n",
    "            FunctionTransformer(aggregate_week_polars),\n",
    "            num_features.tolist() + cat_features.tolist(),\n",
    "        ),\n",
    "        (\"StandardScaler\", StandardScaler(), num_features),  # To-do: --> needed?????\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Models list to compare in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <PATH>.\\venv_case_project\\Scripts\\activate\n",
    "\n",
    "# source venv_macbook/bin/activate\n",
    "\n",
    "# pip install sktime\n",
    "# pip install statsmodels\n",
    "# pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "models = {\n",
    "    \"Naive\": NaiveForecaster(strategy=\"last\"),\n",
    "    \"Simple Moving Average\": NaiveForecaster(strategy=\"mean\", window_length=6),\n",
    "    \"Holt-Winters\": ExponentialSmoothing(trend=\"add\", seasonal=\"add\", sp=52),\n",
    "    \"Random Forest Regressor\": make_reduction(\n",
    "        RandomForestRegressor(), window_length=13, strategy=\"recursive\"\n",
    "    ),\n",
    "    \"XGBoost\": make_reduction(XGBRegressor(), window_length=13, strategy=\"recursive\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Evaulation Metrics and Evaluate Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "\n",
    "def forecast_accuracy(y_true, y_pred):\n",
    "\n",
    "    # Calculate the absolute differences between true and predicted values\n",
    "    absolute_errors = np.abs(y_true - y_pred)\n",
    "\n",
    "    # Calculate 10% of the absolute true values\n",
    "    tolerance = 0.1 * np.abs(y_true)\n",
    "\n",
    "    # Check if the absolute errors are within 10% of the true values\n",
    "    within_tolerance = absolute_errors <= tolerance\n",
    "\n",
    "    # Calculate and return the mean of the boolean array\n",
    "    # (True is treated as 1 and False as 0), which gives the proportion of accurate forecasts\n",
    "    return np.mean(within_tolerance)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    accuracy = forecast_accuracy(y_true, y_pred)\n",
    "    bias = np.mean(y_true - y_pred)\n",
    "\n",
    "    return mape, accuracy, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "\n",
    "\n",
    "def evaluate_forecast(y_true, y_pred, tolerance=0.1):\n",
    "\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    # Calculate absolute errors and tolerance\n",
    "    absolute_errors = np.abs(y_true - y_pred)\n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    bias = np.mean(y_true - y_pred)\n",
    "\n",
    "    # Calculate accuracy (within tolerance)\n",
    "    within_tolerance = absolute_errors <= tolerance_values\n",
    "    accuracy = np.mean(within_tolerance)\n",
    "\n",
    "    # Calculate direction accuracy\n",
    "    direction_correct = np.sign(y_true[1:] - y_true[:-1]) == np.sign(\n",
    "        y_pred[1:] - y_pred[:-1]\n",
    "    )\n",
    "    direction_accuracy = np.mean(direction_correct)\n",
    "\n",
    "    return {\n",
    "        \"MAPE\": mape,\n",
    "        \"Bias\": bias,\n",
    "        \"Accuracy (within {}% tolerance)\".format(tolerance * 100): accuracy,\n",
    "        \"Direction Accuracy\": direction_accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run / Fit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting import (\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_scaled_error,\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_models(models, y_train, y_val, y_test, fh):\n",
    "    results = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(y_train)\n",
    "\n",
    "        y_val_pred = model.predict(fh[: len(y_val)])\n",
    "        y_test_pred = model.predict(fh)\n",
    "\n",
    "        val_mape, val_accuracy, val_bias = evaluate_model(y_val, y_val_pred)\n",
    "        test_mape, test_accuracy, test_bias = evaluate_model(y_test, y_test_pred)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"Validation MAPE\": val_mape,\n",
    "                \"Validation Accuracy\": val_accuracy,\n",
    "                \"Validation Bias\": val_bias,\n",
    "                \"Test MAPE\": test_mape,\n",
    "                \"Test Accuracy\": test_accuracy,\n",
    "                \"Test Bias\": test_bias,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(\"Test MAPE\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "# Assuming you have your data split into y_train, y_val, y_test\n",
    "# and a forecast horizon fh defined\n",
    "\n",
    "# models = create_models()\n",
    "# results = evaluate_models(models, y_train, y_val, y_test, fh)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Pick best one --> Optimize with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tGet\tfeature\timportances\tfrom\tthe\tmodel\n",
    "feature_importances = best_model.get_feature_importance(prettified=False)\n",
    "\n",
    "# \tGet\tfeature\tnames\t(considering\tpotential\ttransformation)\n",
    "feature_names = preprocessor.get_feature_names_out()  # \tAfter\tcolumn\ttransformation\n",
    "\n",
    "# \tSort\tfeature\timportances\tand\tnames\ttogether\tby\timportance\t(descending)\n",
    "sorted_idx = np.argsort(feature_importances)\n",
    "feature_importances = feature_importances[sorted_idx]\n",
    "feature_names = feature_names[sorted_idx]\n",
    "\n",
    "# \tDefine\tplot\tsize\tand\tcreate\ta\tbar\tchart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(feature_names)), feature_importances, align=\"center\")\n",
    "plt.yticks(range(len(feature_names)), feature_names)\n",
    "plt.xlabel(\"Feature\tImportance\")\n",
    "plt.ylabel(\"Feature\tNames\")\n",
    "plt.title(\"Feature\tImportance\tfor\tElectricity\tDemand-Supply\tPrediction\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"depth\": [4, 6, 8],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"iterations\": [50, 100, 200],\n",
    "}\n",
    "\n",
    "best_model = CatBoostRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=best_model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"svm__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    \"svm__gamma\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "pipe = pipeline.Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross-validation\n",
    "https://ml-course.github.io/master/notebooks/Tutorial%203%20-%20Machine%20Learning%20in%20Python.html#evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    GridSearchCV(SVC(), param_grid, cv=5), iris.data, iris.target, cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    GridSearchCV(SVC(), param_grid, cv=5), iris.data, iris.target, cv=5\n",
    ")\n",
    "print(\"Cross-validation scores: \", scores)\n",
    "print(\"Mean cross-validation score: \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda.to_csv(\"final_model.csv\", index=False)\n",
    "# \tSave\tthe\ttrained\tmodel\n",
    "lr_model.save_model(\"catboost_model.cbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: Residual analysis?\n",
    "--> Check if errors are randomly distributed in pointcloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_case_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
