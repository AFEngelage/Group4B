{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporacion Favorita - New Superb Forecasting Model - \n",
    "\n",
    "## Machine Learning Model Pipeline\n",
    "\n",
    "Made by 4B Consultancy (Janne Heuvelmans, Georgi Duev, Alexander Engelage, Sebastiaan de Bruin) - 2024\n",
    "\n",
    "In this data pipeline, \n",
    "\n",
    "The following steps are made within this notebook:  \n",
    "\n",
    ">-0. Import Packages \n",
    "\n",
    ">-1. Load final dataset and aggregate dataset to weekly level\n",
    "    - 1.1 Load final dataset made in Data Preperation Pipeline Notebook\n",
    "    - 1.2 Aggregate dataset to weekly level\n",
    "\n",
    ">-2. Column transformers and Train, Test, Validation Split\n",
    "\n",
    ">-3. Models\n",
    "\n",
    ">-4. Pick best model one and optimize with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load final dataset and aggregate dataset to weekly level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_data(i=0):\n",
    "\n",
    "    # Define path.\n",
    "    # c_path = \"C:/Users/sebas/OneDrive/Documenten/GitHub/Supermarketcasegroupproject/Group4B/data/raw/\"\n",
    "\n",
    "    c_path = \"C:/Users/alexander/Documents/0. Data Science and AI for Experts/EAISI_4B_Supermarket/data/processed/\"\n",
    "\n",
    "    # Identify file.\n",
    "    v_file = (\n",
    "        \"history-per-year\",  # 0\n",
    "        \"holidays_events\",  # 1\n",
    "        \"items\",  # 2\n",
    "        \"stores\",  # 3\n",
    "    )\n",
    "\n",
    "    print(f\"\\nReading file {i}\\n\")\n",
    "\n",
    "    # Load data.\n",
    "    df = (\n",
    "        pd.read_parquet(c_path + v_file[i] + \".parquet\")\n",
    "        .rename(columns=standardize_column_names)\n",
    "        .pipe(optimize_memory)\n",
    "        .pipe(transform_date_to_datetime, i)\n",
    "    )\n",
    "\n",
    "    # Return data.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Importing raw data\n",
    "Importing parquet files with importing function (giving basic information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales History per year\n",
    "df_sales = f_get_data(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Aggregate dataset to weekly level\n",
    "\n",
    "- Group the DataFrame by store number, item number, year, and week number, then aggregate the columns\n",
    "--> \"unit_sales\",\"onpromotion\", \"holiday_local_count\",\"holiday_regional_count\",\"holiday_national_count\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sales_agg_week(df):\n",
    "\n",
    "    # Pre-sort the DataFrame, should be faster? To-do: test this?\n",
    "\n",
    "    df = df.sort_values([\"store_nbr\", \"item_nbr\", \"year\", \"week_number\"])\n",
    "\n",
    "    df = (\n",
    "        df.groupby(\n",
    "            [\n",
    "                \"store_nbr\",\n",
    "                \"item_nbr\",\n",
    "                \"year\",\n",
    "                \"week_number\",\n",
    "            ],\n",
    "            sort=False,  # We've already sorted, so no need to sort again/double\n",
    "        )\n",
    "        .agg(\n",
    "            {\n",
    "                \"unit_sales\": \"sum\",\n",
    "                \"onpromotion\": \"sum\",\n",
    "                \"holiday_local_count\": \"sum\",\n",
    "                \"holiday_regional_count\": \"sum\",\n",
    "                \"holiday_national_count\": \"sum\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Column transformers and Train, Test, Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Column transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"store_nbr\",\n",
    "    \"item_nbr\",\n",
    "    \"date\",  # drop date column?\n",
    "    \"onpromotion\",\n",
    "    \"holiday_local_count\",\n",
    "    \"holiday_national_count\",\n",
    "    \"holiday_regional_count\",\n",
    "    \"store_type\",\n",
    "    \"store_cluster\",\n",
    "    \"item_family\",\n",
    "    \"item_class\",\n",
    "    \"perishable\",\n",
    "    \"store_status\",\n",
    "    \"item_status\",\n",
    "    \"year\",\n",
    "    \"weekday\",\n",
    "    \"week_nbr\",\n",
    "    \"week_number_cum\",\n",
    "]\n",
    "\n",
    "target_variable = [\"unit_sales\"]\n",
    "\n",
    "X = df[features]\n",
    "y = df[target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tCreate\tColumn\tTransformer\twith\t3\ttypes\tof\ttransformers\n",
    "num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "cat_features = X.select_dtypes(include=\"object\").columns\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "oh_transformer = OneHotEncoder()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", oh_transformer, cat_features),\n",
    "        (\"StandardScaler\", numeric_transformer, num_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train, Test, Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(df, train_end, test_end):\n",
    "\n",
    "    # Ensure the df is sorted by date\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Split the df in train, test en validation subsets\n",
    "    train = df[:train_end]\n",
    "    test = df[train_end:test_end]\n",
    "    val = df[test_end:]\n",
    "\n",
    "    # Create X and y for each subset\n",
    "    X_train, y_train = train[features], train[target_variable]\n",
    "    X_test, y_test = test[features], test[target_variable]\n",
    "    X_val, y_val = val[features], val[target_variable]\n",
    "\n",
    "    # Print information about the splits\n",
    "    print(\"Train set:\")\n",
    "    print(\"X_train shape: {}\".format(X_train.shape))\n",
    "    print(\"y_train shape: {}\".format(y_train.shape))\n",
    "    print(\"Training Min Date: {}\".format(X_train.index.min()))\n",
    "    print(\"Training Max Date: {}\".format(X_train.index.max()))\n",
    "\n",
    "    print(\"\\nTest set:\")\n",
    "    print(\"X_test shape: {}\".format(X_test.shape))\n",
    "    print(\"y_test shape: {}\".format(y_test.shape))\n",
    "    print(\"Test Min Date: {}\".format(X_test.index.min()))\n",
    "    print(\"Test Max Date: {}\".format(X_test.index.max()))\n",
    "\n",
    "    print(\"\\nValidation set:\")\n",
    "    print(\"X_val shape: {}\".format(X_val.shape))\n",
    "    print(\"y_val shape: {}\".format(y_val.shape))\n",
    "    print(\"Validation Min Date: {}\".format(X_val.index.min()))\n",
    "    print(\"Validation Max Date: {}\".format(X_val.index.max()))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: do we split based on dates or based on weeks since start?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val = train_test_val_split(\n",
    "    df, train_end=\"2016-06-01\", test_end=\"2017-01-01\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Models list to compare in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "models = {\n",
    "    \"Naive Forecast\": NaiveForecaster(),  # Naive as baseline performance?\n",
    "    \"Linear\tRegression\": LinearRegression(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"K-Neighbor Regressor\": KNeighborsRegressor(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(),\n",
    "    \"Random\tForest\tRegressor\": RandomForestRegressor(),\n",
    "    \"XGBRegressor\": XGBRegressor(),\n",
    "    \"CatBoosting Regressor\": CatBoostRegressor(verbose=False),\n",
    "    \"AdaBoost\tRegressor\": AdaBoostRegressor(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Evaulation Metrics and Evaluate Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "\n",
    "    # Initialize an empty list to store results\n",
    "    results = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate Train, Validation, and Test dataset\n",
    "        model_train_mae, model_train_rmse, model_train_r2 = evaluate_model(\n",
    "            y_train, y_train_pred\n",
    "        )\n",
    "        model_val_mae, model_val_rmse, model_val_r2 = evaluate_model(y_val, y_val_pred)\n",
    "        model_test_mae, model_test_rmse, model_test_r2 = evaluate_model(\n",
    "            y_test, y_test_pred\n",
    "        )\n",
    "\n",
    "        # Append results to list\n",
    "        results.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"Train RMSE\": model_train_rmse,\n",
    "                \"Train MAE\": model_train_mae,\n",
    "                \"Train R2\": model_train_r2,\n",
    "                \"Validation RMSE\": model_val_rmse,\n",
    "                \"Validation MAE\": model_val_mae,\n",
    "                \"Validation R2\": model_val_r2,\n",
    "                \"Test RMSE\": model_test_rmse,\n",
    "                \"Test MAE\": model_test_mae,\n",
    "                \"Test R2\": model_test_r2,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Create DataFrame from results list\n",
    "    results_df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\n",
    "            \"Model\",\n",
    "            \"Train RMSE\",\n",
    "            \"Train MAE\",\n",
    "            \"Train R2\",\n",
    "            \"Validation RMSE\",\n",
    "            \"Validation MAE\",\n",
    "            \"Validation R2\",\n",
    "            \"Test RMSE\",\n",
    "            \"Test MAE\",\n",
    "            \"Test R2\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Sort results by Test RMSE\n",
    "    results_df = results_df.sort_values(\"Test RMSE\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: how to make test forecast per week, so 26 iternations of weeks 1 for one?\n",
    "\n",
    "we now have a good function. however the test (26 weeks) and validation (~26 weeks) shoukd not be done all at once for all the weks. But iterative week for week, as it is for forecasting store sales, and the sales of next week are in reality not known in advance. So we think we want it to loop the test and val set week for week. But dont want do add the val or test set to the training data, to prevent data drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def evaluate_models(models, X_train, y_train, X_val, y_val, X_test, y_test, n_weeks):\n",
    "    # Initialize an empty list to store results\n",
    "    results = []\n",
    "\n",
    "    # Calculate the number of unique weeks for week_column in val and test sets\n",
    "    n_weeks_val = X_val[week_column].nunique()\n",
    "    n_weeks_test = X_test[week_column].nunique()\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Initialize the lists to store week-by-week predictions\n",
    "        val_predictions = []\n",
    "        test_predictions = []\n",
    "\n",
    "        # Iterate over each week for validation\n",
    "        for week in range(n_weeks):\n",
    "            # Make predictions for the current validation week\n",
    "            y_val_pred = model.predict(X_val.iloc[week : week + 1])\n",
    "            val_predictions.append(y_val_pred)\n",
    "\n",
    "            # Update the model with all previous training data plus current validation data\n",
    "            model.fit(\n",
    "                X_train.append(X_val.iloc[: week + 1]),\n",
    "                y_train.append(y_val.iloc[: week + 1]),\n",
    "            )\n",
    "\n",
    "        y_val_pred = pd.Series(val_predictions).flatten()\n",
    "        model_val_mae, model_val_rmse, model_val_r2 = evaluate_model(y_val, y_val_pred)\n",
    "\n",
    "        # Iterate over each week for testing\n",
    "        for week in range(n_weeks):\n",
    "            # Make predictions for the current test week\n",
    "            y_test_pred = model.predict(X_test.iloc[week : week + 1])\n",
    "            test_predictions.append(y_test_pred)\n",
    "\n",
    "            # Only for future predictions, refit with all previous data\n",
    "            model.fit(\n",
    "                X_train.append(X_val),\n",
    "                y_train.append(y_val).append(y_test.iloc[: week + 1]),\n",
    "            )\n",
    "\n",
    "        y_test_pred = pd.Series(test_predictions).flatten()\n",
    "        model_test_mae, model_test_rmse, model_test_r2 = evaluate_model(\n",
    "            y_test, y_test_pred\n",
    "        )\n",
    "\n",
    "        # Evaluate Train dataset\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        model_train_mae, model_train_rmse, model_train_r2 = evaluate_model(\n",
    "            y_train, y_train_pred\n",
    "        )\n",
    "\n",
    "        # Append results to list\n",
    "        results.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"Train RMSE\": model_train_rmse,\n",
    "                \"Train MAE\": model_train_mae,\n",
    "                \"Train R2\": model_train_r2,\n",
    "                \"Validation RMSE\": model_val_rmse,\n",
    "                \"Validation MAE\": model_val_mae,\n",
    "                \"Validation R2\": model_val_r2,\n",
    "                \"Test RMSE\": model_test_rmse,\n",
    "                \"Test MAE\": model_test_mae,\n",
    "                \"Test R2\": model_test_r2,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Create DataFrame from results list\n",
    "    results_df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\n",
    "            \"Model\",\n",
    "            \"Train RMSE\",\n",
    "            \"Train MAE\",\n",
    "            \"Train R2\",\n",
    "            \"Validation RMSE\",\n",
    "            \"Validation MAE\",\n",
    "            \"Validation R2\",\n",
    "            \"Test RMSE\",\n",
    "            \"Test MAE\",\n",
    "            \"Test R2\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Sort results by Test RMSE\n",
    "    results_df = results_df.sort_values(\"Test RMSE\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2\n",
    "\n",
    "\n",
    "def evaluate_train(model, X_train, y_train):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    mae, rmse, r2 = evaluate_metrics(y_train, y_train_pred)\n",
    "    return {\"Train MAE\": mae, \"Train RMSE\": rmse, \"Train R2\": r2}\n",
    "\n",
    "\n",
    "def evaluate_validation(model, X_val, y_val, week_column):\n",
    "    n_weeks_val = X_val[week_column].nunique()\n",
    "    val_predictions = []\n",
    "    unique_weeks = sorted(X_val[week_column].unique())\n",
    "\n",
    "    for week in range(n_weeks_val):\n",
    "        current_week = unique_weeks[week]\n",
    "        current_data = X_val[X_val[week_column] <= current_week]\n",
    "        y_val_pred = model.predict(current_data)\n",
    "        val_predictions.extend(\n",
    "            y_val_pred[-len(X_val[X_val[week_column] == current_week]) :]\n",
    "        )\n",
    "\n",
    "    y_val_pred = pd.Series(val_predictions)\n",
    "    mae, rmse, r2 = evaluate_metrics(y_val, y_val_pred)\n",
    "    return {\"Validation MAE\": mae, \"Validation RMSE\": rmse, \"Validation R2\": r2}\n",
    "\n",
    "\n",
    "def evaluate_test(model, X_test, y_test, week_column):\n",
    "    n_weeks_test = X_test[week_column].nunique()\n",
    "    test_predictions = []\n",
    "    unique_weeks = sorted(X_test[week_column].unique())\n",
    "\n",
    "    for week in range(n_weeks_test):\n",
    "        current_week = unique_weeks[week]\n",
    "        current_data = X_test[X_test[week_column] <= current_week]\n",
    "        y_test_pred = model.predict(current_data)\n",
    "        test_predictions.extend(\n",
    "            y_test_pred[-len(X_test[X_test[week_column] == current_week]) :]\n",
    "        )\n",
    "\n",
    "    y_test_pred = pd.Series(test_predictions)\n",
    "    mae, rmse, r2 = evaluate_metrics(y_test, y_test_pred)\n",
    "    return {\"Test MAE\": mae, \"Test RMSE\": rmse, \"Test R2\": r2}\n",
    "\n",
    "\n",
    "def evaluate_models(\n",
    "    models, X_train, y_train, X_val, y_val, X_test, y_test, week_column\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Fit the model on training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate on train, validation, and test sets\n",
    "        train_results = evaluate_train(model, X_train, y_train)\n",
    "        val_results = evaluate_validation(model, X_val, y_val, week_column)\n",
    "        test_results = evaluate_test(model, X_test, y_test, week_column)\n",
    "\n",
    "        # Combine results\n",
    "        model_results = {\n",
    "            \"Model\": model_name,\n",
    "            **train_results,\n",
    "            **val_results,\n",
    "            **test_results,\n",
    "        }\n",
    "        results.append(model_results)\n",
    "\n",
    "    # Create DataFrame from results list\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Sort results by Test RMSE\n",
    "    results_df = results_df.sort_values(\"Test RMSE\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pick best one --> Optimize with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tGet\tfeature\timportances\tfrom\tthe\tmodel\n",
    "feature_importances = best_model.get_feature_importance(prettified=False)\n",
    "\n",
    "# \tGet\tfeature\tnames\t(considering\tpotential\ttransformation)\n",
    "feature_names = preprocessor.get_feature_names_out()  # \tAfter\tcolumn\ttransformation\n",
    "\n",
    "# \tSort\tfeature\timportances\tand\tnames\ttogether\tby\timportance\t(descending)\n",
    "sorted_idx = np.argsort(feature_importances)\n",
    "feature_importances = feature_importances[sorted_idx]\n",
    "feature_names = feature_names[sorted_idx]\n",
    "\n",
    "# \tDefine\tplot\tsize\tand\tcreate\ta\tbar\tchart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(feature_names)), feature_importances, align=\"center\")\n",
    "plt.yticks(range(len(feature_names)), feature_names)\n",
    "plt.xlabel(\"Feature\tImportance\")\n",
    "plt.ylabel(\"Feature\tNames\")\n",
    "plt.title(\"Feature\tImportance\tfor\tElectricity\tDemand-Supply\tPrediction\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"depth\": [4, 6, 8],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"iterations\": [50, 100, 200],\n",
    "}\n",
    "\n",
    "best_model = CatBoostRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=best_model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"svm__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    \"svm__gamma\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "pipe = pipeline.Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross-validation\n",
    "https://ml-course.github.io/master/notebooks/Tutorial%203%20-%20Machine%20Learning%20in%20Python.html#evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    GridSearchCV(SVC(), param_grid, cv=5), iris.data, iris.target, cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    GridSearchCV(SVC(), param_grid, cv=5), iris.data, iris.target, cv=5\n",
    ")\n",
    "print(\"Cross-validation scores: \", scores)\n",
    "print(\"Mean cross-validation score: \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda.to_csv(\"final_model.csv\", index=False)\n",
    "# \tSave\tthe\ttrained\tmodel\n",
    "lr_model.save_model(\"catboost_model.cbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: Residual analysis?\n",
    "--> Check if errors are randomly distributed in pointcloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_case_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
