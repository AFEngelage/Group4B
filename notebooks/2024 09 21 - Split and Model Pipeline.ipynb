{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporacion Favorita - New Superb Forecasting Model - \n",
    "\n",
    "## Split and Model Pipeline\n",
    "\n",
    "#codi\n",
    "\n",
    "Made by 4B Consultancy (Janne Heuvelmans, Georgi Duev, Alexander Engelage, Sebastiaan de Bruin) - 2024\n",
    "\n",
    "In this data pipeline, \n",
    "\n",
    "The following steps are made within this notebook:  \n",
    "\n",
    ">-0. Import Packages \n",
    "\n",
    ">-1. Load final dataset and aggregate dataset to weekly level\n",
    "    -1.1 Load final dataset made in Data Preperation Pipeline Notebook\n",
    "    -1.2 Aggregate dataset to weekly level\n",
    "\n",
    ">-2. Column transformers and Train, Test, Validation Split\n",
    "\n",
    ">-3. Models\n",
    "\n",
    ">-4. Pick best model one and optimize with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import os\n",
    "import sys\n",
    "import altair as alt\n",
    "import vegafusion as vf\n",
    "import sklearn\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load final dataset, Inpute Stockouts and Aggregate dataset to weekly level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Functions - Import raw data from local PATH\n",
    "Create import data function and give basic information function within the importing function.\n",
    "\n",
    "Return basic information on each dataframe:  \n",
    "- a) Information on the number of observation and features.  \n",
    "- b) Information on the size of the dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO: Import via polars, and use polars dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_data_and_info(import_path, file_name=\"df_final\"):\n",
    "\n",
    "    print(f\"\\nReading file {file_name}\\n\")\n",
    "\n",
    "    # Load data.\n",
    "    df = pd.read_parquet(import_path + file_name + \".parquet\")\n",
    "\n",
    "    # Getting the basic information of the dataframe (number of observations and features, and size)\n",
    "    print(\n",
    "        f\"The '{file_name}' dataframe contains: {df.shape[0]:,}\".replace(\",\", \".\")\n",
    "        + f\" observations and {df.shape[1]} features.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Prepared and transformed dataframe has optimized size of {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB.\"\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Importing raw data\n",
    "Importing parquet files with importing function (giving basic information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading file Prepped_data_20241004\n",
      "\n",
      "The 'Prepped_data_20241004' dataframe contains: 67.834.270 observations and 19 features.\n",
      "Prepared and transformed dataframe has optimized size of 2.72 GB.\n"
     ]
    }
   ],
   "source": [
    "import_path = \"C:/Users/alexander/Documents/0. Data Science and AI for Experts/EAISI_4B_Supermarket/data/processed/\"\n",
    "\n",
    "\n",
    "# import_path = \"C:/Users/sebas/OneDrive/Documenten/GitHub/Supermarketcasegroupproject/Group4B/data/raw/\n",
    "\n",
    "\n",
    "# Importing final df\n",
    "\n",
    "\n",
    "df_final = f_get_data_and_info(import_path, file_name=\"Prepped_data_20241004\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: include null_count into importing OR make basic descrption function with features, size, null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 67834270 entries, 0 to 67834269\n",
      "Data columns (total 19 columns):\n",
      " #   Column                  Dtype         \n",
      "---  ------                  -----         \n",
      " 0   store_nbr               uint8         \n",
      " 1   item_nbr                int32         \n",
      " 2   date                    datetime64[ns]\n",
      " 3   unit_sales              float32       \n",
      " 4   onpromotion             bool          \n",
      " 5   holiday_local_count     int8          \n",
      " 6   holiday_national_count  int8          \n",
      " 7   holiday_regional_count  int8          \n",
      " 8   store_type              category      \n",
      " 9   store_cluster           uint8         \n",
      " 10  item_family             category      \n",
      " 11  item_class              uint16        \n",
      " 12  perishable              uint8         \n",
      " 13  store_status            int8          \n",
      " 14  item_status             int8          \n",
      " 15  year                    int16         \n",
      " 16  weekday                 int8          \n",
      " 17  week_nbr                int8          \n",
      " 18  week_number_cum         int16         \n",
      "dtypes: bool(1), category(2), datetime64[ns](1), float32(1), int16(2), int32(1), int8(7), uint16(1), uint8(3)\n",
      "memory usage: 2.7 GB\n",
      "Column 'store_nbr' has 0 null values.\n",
      "Column 'item_nbr' has 0 null values.\n",
      "Column 'date' has 0 null values.\n",
      "Column 'unit_sales' has 39275809 null values.\n",
      "Column 'onpromotion' has 0 null values.\n",
      "Column 'holiday_local_count' has 0 null values.\n",
      "Column 'holiday_national_count' has 0 null values.\n",
      "Column 'holiday_regional_count' has 0 null values.\n",
      "Column 'store_type' has 0 null values.\n",
      "Column 'store_cluster' has 0 null values.\n",
      "Column 'item_family' has 0 null values.\n",
      "Column 'item_class' has 0 null values.\n",
      "Column 'perishable' has 0 null values.\n",
      "Column 'store_status' has 0 null values.\n",
      "Column 'item_status' has 0 null values.\n",
      "Column 'year' has 0 null values.\n",
      "Column 'weekday' has 0 null values.\n",
      "Column 'week_nbr' has 0 null values.\n",
      "Column 'week_number_cum' has 0 null values.\n"
     ]
    }
   ],
   "source": [
    "df_final.info()\n",
    "# Count nulls per column\n",
    "null_counts = df_final.isnull().sum()\n",
    "\n",
    "# Print results\n",
    "for column, count in null_counts.items():\n",
    "    print(f\"Column '{column}' has {count} null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 67834270 entries, 0 to 67834269\n",
      "Data columns (total 19 columns):\n",
      " #   Column                  Dtype         \n",
      "---  ------                  -----         \n",
      " 0   store_nbr               uint8         \n",
      " 1   item_nbr                int32         \n",
      " 2   date                    datetime64[ns]\n",
      " 3   unit_sales              float32       \n",
      " 4   onpromotion             bool          \n",
      " 5   holiday_local_count     int8          \n",
      " 6   holiday_national_count  int8          \n",
      " 7   holiday_regional_count  int8          \n",
      " 8   store_type              category      \n",
      " 9   store_cluster           uint8         \n",
      " 10  item_family             category      \n",
      " 11  item_class              uint16        \n",
      " 12  perishable              uint8         \n",
      " 13  store_status            int8          \n",
      " 14  item_status             int8          \n",
      " 15  year                    int16         \n",
      " 16  weekday                 int8          \n",
      " 17  week_nbr                int8          \n",
      " 18  week_number_cum         int16         \n",
      "dtypes: bool(1), category(2), datetime64[ns](1), float32(1), int16(2), int32(1), int8(7), uint16(1), uint8(3)\n",
      "memory usage: 2.7 GB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Train test val split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKtime\n",
    "\n",
    "ExpandingWindowSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TO-DO: Selecting on weeks or via data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"store_nbr\",\n",
    "    \"item_nbr\",\n",
    "    \"onpromotion\",\n",
    "    \"holiday_local_count\",\n",
    "    \"holiday_national_count\",\n",
    "    \"holiday_regional_count\",\n",
    "    \"store_type\",\n",
    "    \"store_cluster\",\n",
    "    \"item_family\",\n",
    "    \"item_class\",\n",
    "    \"perishable\",\n",
    "    \"store_status\",\n",
    "    \"item_status\",\n",
    "    \"year\",\n",
    "    # \"week_nbr\",\n",
    "    \"week_number_cum\",\n",
    "]\n",
    "\n",
    "target_variable = [\"unit_sales\"]\n",
    "\n",
    "# X = df[features]\n",
    "# y = df[target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SingleWindowSplitter' from 'sktime.forecasting.model_selection' (c:\\Users\\alexander\\Documents\\0. Data Science and AI for Experts\\EAISI_4B_Supermarket\\venv_case_project\\Lib\\site-packages\\sktime\\forecasting\\model_selection\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msktime\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msktime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mforecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SingleWindowSplitter\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_test_val_split\u001b[39m(\n\u001b[0;32m      7\u001b[0m     df, features, target_variable, train_week_end, test_week_end, window_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m52\u001b[39m\n\u001b[0;32m      8\u001b[0m ):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Ensure the df is sorted by store_nbr, item_nbr, and week_number_cum\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SingleWindowSplitter' from 'sktime.forecasting.model_selection' (c:\\Users\\alexander\\Documents\\0. Data Science and AI for Experts\\EAISI_4B_Supermarket\\venv_case_project\\Lib\\site-packages\\sktime\\forecasting\\model_selection\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import sktime\n",
    "from sktime.forecasting.model_selection import SingleWindowSplitter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def train_test_val_split(\n",
    "    df, features, target_variable, train_week_end, test_week_end, window_length=52\n",
    "):\n",
    "\n",
    "    # Ensure the df is sorted by store_nbr, item_nbr, and week_number_cum\n",
    "\n",
    "    df = df.sort_values([\"store_nbr\", \"item_nbr\", \"week_number_cum\"])\n",
    "\n",
    "    # Create X and y\n",
    "    X = df[features]\n",
    "    y = df[target_variable]\n",
    "\n",
    "    # Training data: From the first week to the `train_week_end`\n",
    "    X_train = X[X[\"week_number_cum\"] <= train_week_end]\n",
    "    y_train = y[X[\"week_number_cum\"] <= train_week_end]\n",
    "\n",
    "    # Testing data: Sliding window of 12 months (52 weeks) from `train_week_end` to `test_week_end`\n",
    "    test_fh = df[\n",
    "        (df[\"week_number_cum\"] > train_week_end)\n",
    "        & (df[\"week_number_cum\"] <= test_week_end)\n",
    "    ][\"week_number_cum\"].nunique()\n",
    "\n",
    "    test_splitter = SingleWindowSplitter(fh=test_fh, window_length=window_length)\n",
    "\n",
    "    test_train_idx, test_test_idx = next(\n",
    "        test_splitter.split(\n",
    "            X[\n",
    "                (X[\"week_number_cum\"] > train_week_end)\n",
    "                & (X[\"week_number_cum\"] <= test_week_end)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Get the test set indices\n",
    "    X_test = X.iloc[test_test_idx]\n",
    "    y_test = y.iloc[test_test_idx]\n",
    "\n",
    "    # Validation data: Sliding window of 12 months (52 weeks) after `test_week_end`\n",
    "    val_fh = df[df[\"week_number_cum\"] > test_week_end][\"week_number_cum\"].nunique()\n",
    "    val_splitter = SingleWindowSplitter(fh=val_fh, window_length=window_length)\n",
    "\n",
    "    val_train_idx, val_test_idx = next(\n",
    "        val_splitter.split(X[X[\"week_number_cum\"] > test_week_end])\n",
    "    )\n",
    "\n",
    "    # Get the validation set indices\n",
    "    X_val = X.iloc[val_test_idx]\n",
    "    y_val = y.iloc[val_test_idx]\n",
    "\n",
    "    # Function to print split information\n",
    "    def print_split_info(split_name, X_split, y_split):\n",
    "\n",
    "        print(f\"\\n{split_name} set:\")\n",
    "        print(f\"X_{split_name.lower()} shape: {X_split.shape}\")\n",
    "        print(f\"y_{split_name.lower()} shape: {y_split.shape}\")\n",
    "        print(f\"{split_name} Min Week: {X_split['week_number_cum'].min()}\")\n",
    "        print(f\"{split_name} Max Week: {X_split['week_number_cum'].max()}\")\n",
    "        print(f\"{split_name} number of weeks: {X_split['week_number_cum'].nunique()}\")\n",
    "        print(f\"Number of stores: {X_split['store_nbr'].nunique()}\")\n",
    "        print(f\"Number of items: {X_split['item_nbr'].nunique()}\")\n",
    "\n",
    "    # Print information about the splits\n",
    "    print_split_info(\"Train\", X_train, y_train)\n",
    "    print_split_info(\"Test\", X_test, y_test)\n",
    "    print_split_info(\"Validation\", X_val, y_val)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_test_val_split() got an unexpected keyword argument 'train_week_end'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, y_train, X_test, y_test, X_val, y_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_val_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_variable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_week_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m156\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_week_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m208\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: train_test_val_split() got an unexpected keyword argument 'train_week_end'"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val = train_test_val_split(\n",
    "    df_final, features, target_variable, train_week_end=156, test_week_end=208\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i am still not content with the train,test,val timeseries splitter using sktime. I want a function where i can select train_week_end=156, test_week_end=208 within the function. The train df should run from first week to train_week_end. The test dataframe should be a sliding window of 12 months, to test only the performance of train_week_end week 156> until week test_week_end=208. The  valdataframe should be a sliding window of 12 months, to test only the performance of week 208 until end dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_split_info(split_name, X_split, y_split):\n",
    "    print(f\"\\n{split_name} set:\")\n",
    "    print(f\"X_{split_name.lower()} shape: {X_split.shape}\")\n",
    "    print(f\"y_{split_name.lower()} shape: {y_split.shape}\")\n",
    "    print(f\"{split_name} Min Week: {X_split['week_number_cum'].min()}\")\n",
    "    print(f\"{split_name} Max Week: {X_split['week_number_cum'].max()}\")\n",
    "    print(f\"{split_name} number of weeks: {X_split['week_number_cum'].nunique()}\")\n",
    "    print(f\"Number of stores: {X_split['store_nbr'].nunique()}\")\n",
    "    print(f\"Number of items: {X_split['item_nbr'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(df, train_end, test_end):\n",
    "\n",
    "    # Ensure the df is sorted by date\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Split the df in train, test en validation subsets\n",
    "    train = df[:train_end]\n",
    "    test = df[train_end:test_end]\n",
    "    val = df[test_end:]\n",
    "\n",
    "    # Create X and y for each subset\n",
    "    X_train, y_train = train[features], train[target_variable]\n",
    "    X_test, y_test = test[features], test[target_variable]\n",
    "    X_val, y_val = val[features], val[target_variable]\n",
    "\n",
    "    # Print information about the splits\n",
    "    print(\"Train set:\")\n",
    "    print(\"X_train shape: {}\".format(X_train.shape))\n",
    "    print(\"y_train shape: {}\".format(y_train.shape))\n",
    "    print(\"Training Min Date: {}\".format(X_train.index.min()))\n",
    "    print(\"Training Max Date: {}\".format(X_train.index.max()))\n",
    "\n",
    "    print(\"\\nTest set:\")\n",
    "    print(\"X_test shape: {}\".format(X_test.shape))\n",
    "    print(\"y_test shape: {}\".format(y_test.shape))\n",
    "    print(\"Test Min Date: {}\".format(X_test.index.min()))\n",
    "    print(\"Test Max Date: {}\".format(X_test.index.max()))\n",
    "\n",
    "    print(\"\\nValidation set:\")\n",
    "    print(\"X_val shape: {}\".format(X_val.shape))\n",
    "    print(\"y_val shape: {}\".format(y_val.shape))\n",
    "    print(\"Validation Min Date: {}\".format(X_val.index.min()))\n",
    "    print(\"Validation Max Date: {}\".format(X_val.index.max()))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: do we split based on dates or based on weeks since start?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_val_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, y_train, X_test, y_test, X_val, y_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_val_split\u001b[49m(\n\u001b[0;32m      2\u001b[0m     df_final, train_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2016-06-01\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2017-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_val_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val = train_test_val_split(\n",
    "    df_final, train_end=\"2016-06-01\", test_end=\"2017-01-01\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Functions - Impute stockouts and Aggregate dataset to weekly level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Impute stockouts\n",
    "\n",
    "Stockout on store level\n",
    "\n",
    "•      Perishable good: when there are missing values for two consecutive days for a given item per individual store \n",
    "\n",
    "•      Nonperishable goods: when there are missing values for 7 consecutive days for a given item and per individual store\n",
    "\n",
    "•      Action: Impute with Rolling Mean with defeault window of 7 days \n",
    "\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_stockouts_polars(df_pandas, window_size=7):\n",
    "\n",
    "    # Convert the input Pandas DataFrame to a Polars DataFrame for efficient processing\n",
    "\n",
    "    df = pl.from_pandas(df_pandas)\n",
    "\n",
    "    # Sort the DataFrame by store number, item number, and date for consistent ordering\n",
    "\n",
    "    df = df.sort([\"store_nbr\", \"item_nbr\", \"date\"])\n",
    "\n",
    "    # Nested function calc_missing_count to calculate the count of consecutive missing values in unit_sales\n",
    "\n",
    "    def calc_missing_count(unit_sales):\n",
    "\n",
    "        return (\n",
    "            unit_sales.is_null()  # Check for null values\n",
    "            .cast(pl.Int32)  # Cast to integer (1 for null, 0 for not null)\n",
    "            .cum_sum()  # Cumulative sum to count sequential nulls\n",
    "            .over([\"store_nbr\", \"item_nbr\"])  # Group by store_nbr and item_nbr\n",
    "        )\n",
    "\n",
    "    # Nested function to Inpute with rolling mean for missing values\n",
    "    def rolling_mean_imputation(unit_sales, window_size):\n",
    "\n",
    "        return (\n",
    "            unit_sales.rolling_mean(\n",
    "                window_size=window_size, min_periods=1\n",
    "            )  # Impute strategy based on rolling mean\n",
    "            .shift(\n",
    "                1\n",
    "            )  # Shift window by one day, to prevent taking the same day into account\n",
    "            .over([\"store_nbr\", \"item_nbr\"])  # Group by store_nbr and item_nbr\n",
    "        )\n",
    "\n",
    "    # Apply the imputation logic based on the perishable status of the items\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.when(pl.col(\"perishable\") == 1)  # Check if the item is perishable = 1\n",
    "            .then(\n",
    "                pl.when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) == 1\n",
    "                )  # 1 missing value\n",
    "                .then(0)  # --> Impute with 0\n",
    "                .when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) > 2\n",
    "                )  # More than 2 missing values\n",
    "                .then(0)  # --> Impute with 0\n",
    "                .when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) == 2\n",
    "                )  # = 2 missing values\n",
    "                .then(\n",
    "                    rolling_mean_imputation(pl.col(\"unit_sales\"), window_size)\n",
    "                )  # --> Inpute with rolling mean for 2 missing days\n",
    "                .otherwise(pl.col(\"unit_sales\"))  # Otherwise keep original value\n",
    "            )\n",
    "            .when(pl.col(\"perishable\") == 0)  # If the item is not perishable = 0\n",
    "            .then(\n",
    "                pl.when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) > 7\n",
    "                )  # More than 7 missing values\n",
    "                .then(0)  # --> Impute with 0\n",
    "                .when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) <= 7\n",
    "                )  # if less 7 missing values\n",
    "                .then(\n",
    "                    rolling_mean_imputation(pl.col(\"unit_sales\"), window_size)\n",
    "                )  # --> Inpute with rolling mean for missing 7 or less days\n",
    "                .otherwise(pl.col(\"unit_sales\"))  # Otherwise keep original value\n",
    "            )\n",
    "            .otherwise(pl.col(\"unit_sales\"))  # For any other case not covered\n",
    "            .alias(\"unit_sales\")  # Alias the new column as 'unit_sales'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Convert Polars df back to Pandas df\n",
    "    df = df.to_pandas()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def impute_stockouts_(df):\n",
    "\n",
    "#     df = impute_stockouts_polars(df, window_size=7)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Aggregate dataset to weekly level\n",
    "\n",
    "- Group the DataFrame by store number, item number, year, and week_cum_number, then aggregate the columns\n",
    "--> \"unit_sales\",\"onpromotion\", \"holiday_local_count\",\"holiday_regional_count\",\"holiday_national_count\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_week(df):\n",
    "\n",
    "    # Pre-sort the DataFrame\n",
    "    df = df.sort_values([\"store_nbr\", \"item_nbr\", \"year\", \"week_nbr\"])\n",
    "\n",
    "    # Group by the specified columns and aggregate\n",
    "    df = (\n",
    "        df.groupby(\n",
    "            [\n",
    "                \"store_nbr\",\n",
    "                \"item_nbr\",\n",
    "                \"year\",\n",
    "                \"week_number_cum\",  # Aggregating by week_number_cum\n",
    "            ]\n",
    "        )\n",
    "        .agg(\n",
    "            {\n",
    "                \"unit_sales\": \"sum\",\n",
    "                \"onpromotion\": \"sum\",\n",
    "                \"holiday_local_count\": \"sum\",\n",
    "                \"holiday_regional_count\": \"sum\",\n",
    "                \"holiday_national_count\": \"sum\",\n",
    "                \"date\": \"first\",  # Keep the first day of week, needed to run Timeseries models from SKtime\n",
    "                \"store_type\": \"first\",  # Keep the first occurrence of store_type\n",
    "                \"store_cluster\": \"first\",  # Keep the first occurrence of store_cluster\n",
    "                \"item_family\": \"first\",  # Keep the first occurrence of item_family\n",
    "                \"item_class\": \"first\",  # Keep the first occurrence of item_class\n",
    "                \"perishable\": \"first\",  # Keep the first occurrence of perishable\n",
    "                \"store_status\": \"last\",  # Keep the last occurrence of store_status\n",
    "                \"item_status\": \"last\",  # Keep the last occurrence of item_status\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Define the columns weekday' to drop\n",
    "    # columns_to_drop = [\"weekday\"]\n",
    "    # df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9730820 entries, 0 to 9730819\n",
      "Data columns (total 17 columns):\n",
      " #   Column                  Dtype         \n",
      "---  ------                  -----         \n",
      " 0   store_nbr               uint8         \n",
      " 1   item_nbr                int32         \n",
      " 2   year                    int16         \n",
      " 3   week_number_cum         int16         \n",
      " 4   unit_sales              float32       \n",
      " 5   onpromotion             int64         \n",
      " 6   holiday_local_count     int8          \n",
      " 7   holiday_regional_count  int8          \n",
      " 8   holiday_national_count  int8          \n",
      " 9   date                    datetime64[ns]\n",
      " 10  store_type              category      \n",
      " 11  store_cluster           uint8         \n",
      " 12  item_family             category      \n",
      " 13  item_class              uint16        \n",
      " 14  perishable              uint8         \n",
      " 15  store_status            int8          \n",
      " 16  item_status             int8          \n",
      "dtypes: category(2), datetime64[ns](1), float32(1), int16(2), int32(1), int64(1), int8(5), uint16(1), uint8(3)\n",
      "memory usage: 371.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_agg = aggregate_week(df_final)\n",
    "\n",
    "df_agg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>year</th>\n",
       "      <th>week_number_cum</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>holiday_local_count</th>\n",
       "      <th>holiday_regional_count</th>\n",
       "      <th>holiday_national_count</th>\n",
       "      <th>date</th>\n",
       "      <th>store_type</th>\n",
       "      <th>store_cluster</th>\n",
       "      <th>item_family</th>\n",
       "      <th>item_class</th>\n",
       "      <th>perishable</th>\n",
       "      <th>store_status</th>\n",
       "      <th>item_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>96995</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>96995</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>96995</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-14</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>96995</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-21</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>96995</td>\n",
       "      <td>2013</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-28</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>96995</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-04</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>96995</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-02-11</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>96995</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-18</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>96995</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-25</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>96995</td>\n",
       "      <td>2013</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-03-04</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_nbr  item_nbr  year  week_number_cum  unit_sales  onpromotion  \\\n",
       "0          1     96995  2013                1         0.0            0   \n",
       "1          1     96995  2013                2         2.0            0   \n",
       "2          1     96995  2013                3         3.0            0   \n",
       "3          1     96995  2013                4         2.0            0   \n",
       "4          1     96995  2013                5         2.0            0   \n",
       "5          1     96995  2013                6         1.0            0   \n",
       "6          1     96995  2013                7         4.0            0   \n",
       "7          1     96995  2013                8         2.0            0   \n",
       "8          1     96995  2013                9         2.0            0   \n",
       "9          1     96995  2013               10         5.0            0   \n",
       "\n",
       "   holiday_local_count  holiday_regional_count  holiday_national_count  \\\n",
       "0                    0                       0                       1   \n",
       "1                    0                       0                       1   \n",
       "2                    0                       0                       0   \n",
       "3                    0                       0                       0   \n",
       "4                    0                       0                       0   \n",
       "5                    0                       0                       0   \n",
       "6                    0                       0                       2   \n",
       "7                    0                       0                       0   \n",
       "8                    0                       0                       0   \n",
       "9                    0                       0                       0   \n",
       "\n",
       "        date store_type  store_cluster item_family  item_class  perishable  \\\n",
       "0 2013-01-02          D             13   GROCERY I        1093           0   \n",
       "1 2013-01-07          D             13   GROCERY I        1093           0   \n",
       "2 2013-01-14          D             13   GROCERY I        1093           0   \n",
       "3 2013-01-21          D             13   GROCERY I        1093           0   \n",
       "4 2013-01-28          D             13   GROCERY I        1093           0   \n",
       "5 2013-02-04          D             13   GROCERY I        1093           0   \n",
       "6 2013-02-11          D             13   GROCERY I        1093           0   \n",
       "7 2013-02-18          D             13   GROCERY I        1093           0   \n",
       "8 2013-02-25          D             13   GROCERY I        1093           0   \n",
       "9 2013-03-04          D             13   GROCERY I        1093           0   \n",
       "\n",
       "   store_status  item_status  \n",
       "0             0            3  \n",
       "1             0            1  \n",
       "2             0            1  \n",
       "3             0            1  \n",
       "4             0            1  \n",
       "5             0            1  \n",
       "6             0            1  \n",
       "7             0            1  \n",
       "8             0            1  \n",
       "9             0            1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_week_polars(df):\n",
    "\n",
    "    # Convert to a Polars DataFrame\n",
    "    df = pl.DataFrame(df)\n",
    "\n",
    "    # Group by and aggregate, use .alias ['column'] to specify column-name\n",
    "    df = df.groupby([\"store_nbr\", \"item_nbr\", \"year\", \"week_number_cum\"]).agg(\n",
    "        [\n",
    "            pl.col(\"unit_sales\").sum().alias(\"unit_sales\"),\n",
    "            pl.col(\"onpromotion\").sum().alias(\"onpromotion\"),\n",
    "            pl.col(\"holiday_local_count\").sum().alias(\"holiday_local_count\"),\n",
    "            pl.col(\"holiday_regional_count\").sum().alias(\"holiday_regional_count\"),\n",
    "            pl.col(\"holiday_national_count\").sum().alias(\"holiday_national_count\"),\n",
    "            pl.col(\"date\")\n",
    "            .first()  # Keep the first day of week, needed to run Timeseries models from SKtime\n",
    "            .alias(\"date\"),\n",
    "            pl.col(\"store_type\")\n",
    "            .first()  # Keep the first occurrence of store_type\n",
    "            .alias(\"store_type\"),\n",
    "            pl.col(\"store_cluster\")\n",
    "            .first()  # Keep the first occurrence of store_cluster\n",
    "            .alias(\"store_cluster\"),\n",
    "            pl.col(\"item_family\")\n",
    "            .first()  # Keep the first occurrence of item_family\n",
    "            .alias(\"item_family\"),\n",
    "            pl.col(\"item_class\")\n",
    "            .first()  # Keep the first occurrence of item_class\n",
    "            .alias(\"item_class\"),\n",
    "            pl.col(\"perishable\")\n",
    "            .first()  # Keep the first occurrence of perishable\n",
    "            .alias(\"perishable\"),\n",
    "            pl.col(\"store_status\")\n",
    "            .last()  # Keep the last occurrence of store_status\n",
    "            .alias(\"store_status\"),\n",
    "            pl.col(\"item_status\")\n",
    "            .last()  # Keep the last occurrence of item_status\n",
    "            .alias(\"item_status\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Convert Polars df back to Pandas df\n",
    "    df = df.to_pandas\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Column transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Column transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"store_nbr\",\n",
    "    \"item_nbr\",\n",
    "    \"onpromotion\",\n",
    "    \"holiday_local_count\",\n",
    "    \"holiday_national_count\",\n",
    "    \"holiday_regional_count\",\n",
    "    \"store_type\",\n",
    "    \"store_cluster\",\n",
    "    \"item_family\",\n",
    "    \"item_class\",\n",
    "    \"perishable\",\n",
    "    \"store_status\",\n",
    "    \"item_status\",\n",
    "    \"year\",\n",
    "    \"week_nbr\",\n",
    "    \"week_number_cum\",\n",
    "]\n",
    "\n",
    "target_variable = [\"unit_sales\"]\n",
    "\n",
    "X = df[features]\n",
    "y = df[target_variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: Do we need onehotencoder? --> then needed to seperate between timeseries en ML models\n",
    "\n",
    "To-do: Change catagory dtypes from store_type and item_family just to numbers in prep pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Create your transformers\n",
    "num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "cat_features = X.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# Create a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"ImputeStockouts\",  # Impute stockouts\n",
    "            FunctionTransformer(impute_stockouts_polars),\n",
    "            num_features.tolist() + cat_features.tolist(),\n",
    "        ),\n",
    "        (\n",
    "            \"AggregateWeek\",  # Aggregate dataset to weekly level\n",
    "            FunctionTransformer(aggregate_week_polars),\n",
    "            num_features.tolist() + cat_features.tolist(),\n",
    "        ),\n",
    "        (\"StandardScaler\", StandardScaler(), num_features),  # To-do: --> needed?????\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1.1. ForecastingGridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Models list to compare in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <PATH>.\\venv_case_project\\Scripts\\activate\n",
    "\n",
    "# source venv_macbook/bin/activate #for Georgi ;)\n",
    "\n",
    "# pip install sktime\n",
    "# pip install statsmodels\n",
    "# pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "models = {\n",
    "    \"Naive\": NaiveForecaster(strategy=\"last\"),\n",
    "    \"Simple Moving Average\": NaiveForecaster(strategy=\"mean\", window_length=6),\n",
    "    \"Holt-Winters\": ExponentialSmoothing(trend=\"add\", seasonal=\"add\", sp=52),\n",
    "    \"Random Forest Regressor\": make_reduction(\n",
    "        RandomForestRegressor(), window_length=13, strategy=\"recursive\"\n",
    "    ),\n",
    "    \"XGBoost\": make_reduction(XGBRegressor(), window_length=13, strategy=\"recursive\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Evaulation Metrics and Evaluate Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "\n",
    "def forecast_accuracy(y_true, y_pred):\n",
    "\n",
    "    # Calculate the absolute differences between true and predicted values\n",
    "    absolute_errors = np.abs(y_true - y_pred)\n",
    "\n",
    "    # Calculate 10% of the absolute true values\n",
    "    tolerance = 0.1 * np.abs(y_true)\n",
    "\n",
    "    # Check if the absolute errors are within 10% of the true values\n",
    "    within_tolerance = absolute_errors <= tolerance\n",
    "\n",
    "    # Calculate and return the mean of the boolean array\n",
    "    # (True is treated as 1 and False as 0), which gives the proportion of accurate forecasts\n",
    "    return np.mean(within_tolerance)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    accuracy = forecast_accuracy(y_true, y_pred)\n",
    "    bias = np.mean(y_true - y_pred)\n",
    "\n",
    "    return mape, accuracy, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def evaluate_forecast(\n",
    "    y_true, y_pred, scaler=None, tolerance=0.1\n",
    "):  # scaler=StandardScaler()\n",
    "\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    # # If scaler is provided --> Inverse transform the data, because of the StandardScaler()\n",
    "    if scaler:\n",
    "        y_true = scaler.inverse_transform(y_true.reshape(-1, 1)).flatten()\n",
    "        y_pred = scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Calculate absolute errors and tolerance\n",
    "    absolute_errors = np.abs(y_true - y_pred)\n",
    "    tolerance_values = tolerance * np.abs(y_true)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    bias = np.mean(y_true - y_pred)\n",
    "\n",
    "    # Calculate accuracy (within tolerance)\n",
    "    within_tolerance = absolute_errors <= tolerance_values\n",
    "    accuracy = np.mean(within_tolerance)\n",
    "\n",
    "    # Calculate direction accuracy\n",
    "    direction_correct = np.sign(y_true[1:] - y_true[:-1]) == np.sign(\n",
    "        y_pred[1:] - y_pred[:-1]\n",
    "    )\n",
    "    direction_accuracy = np.mean(direction_correct)\n",
    "\n",
    "    return {\n",
    "        \"MAPE\": mape,\n",
    "        \"Bias\": bias,\n",
    "        \"Accuracy (within {}% tolerance)\".format(tolerance * 100): accuracy,\n",
    "        \"Direction Accuracy\": direction_accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run / Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "\n",
    "    # Initialize an empty list to store results\n",
    "    results = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate Train, Validation, and Test dataset\n",
    "        model_train_mae, model_train_rmse, model_train_r2 = evaluate_model(\n",
    "            y_train, y_train_pred\n",
    "        )\n",
    "        model_val_mae, model_val_rmse, model_val_r2 = evaluate_model(y_val, y_val_pred)\n",
    "        model_test_mae, model_test_rmse, model_test_r2 = evaluate_model(\n",
    "            y_test, y_test_pred\n",
    "        )\n",
    "\n",
    "        # Append results to list\n",
    "        results.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"Train RMSE\": model_train_rmse,\n",
    "                \"Train MAE\": model_train_mae,\n",
    "                \"Train R2\": model_train_r2,\n",
    "                \"Validation RMSE\": model_val_rmse,\n",
    "                \"Validation MAE\": model_val_mae,\n",
    "                \"Validation R2\": model_val_r2,\n",
    "                \"Test RMSE\": model_test_rmse,\n",
    "                \"Test MAE\": model_test_mae,\n",
    "                \"Test R2\": model_test_r2,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Create DataFrame from results list\n",
    "    results_df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\n",
    "            \"Model\",\n",
    "            \"Train RMSE\",\n",
    "            \"Train MAE\",\n",
    "            \"Train R2\",\n",
    "            \"Validation RMSE\",\n",
    "            \"Validation MAE\",\n",
    "            \"Validation R2\",\n",
    "            \"Test RMSE\",\n",
    "            \"Test MAE\",\n",
    "            \"Test R2\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Sort results by Test RMSE\n",
    "    results_df = results_df.sort_values(\"Test RMSE\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: how to make test forecast per week, so 26 iternations of weeks 1 for one?\n",
    "\n",
    "we now have a good function. however the test (26 weeks) and validation (~26 weeks) shoukd not be done all at once for all the weks. But iterative week for week, as it is for forecasting store sales, and the sales of next week are in reality not known in advance. So we think we want it to loop the test and val set week for week. But dont want do add the val or test set to the training data, to prevent data drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting import (\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_scaled_error,\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_models(models, y_train, y_val, y_test, fh):\n",
    "    results = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(y_train)\n",
    "\n",
    "        y_val_pred = model.predict(fh[: len(y_val)])\n",
    "        y_test_pred = model.predict(fh)\n",
    "\n",
    "        val_mape, val_accuracy, val_bias = evaluate_model(y_val, y_val_pred)\n",
    "        test_mape, test_accuracy, test_bias = evaluate_model(y_test, y_test_pred)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"Validation MAPE\": val_mape,\n",
    "                \"Validation Accuracy\": val_accuracy,\n",
    "                \"Validation Bias\": val_bias,\n",
    "                \"Test MAPE\": test_mape,\n",
    "                \"Test Accuracy\": test_accuracy,\n",
    "                \"Test Bias\": test_bias,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(\"Test MAPE\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "# Assuming you have your data split into y_train, y_val, y_test\n",
    "# and a forecast horizon fh defined\n",
    "\n",
    "# models = create_models()\n",
    "# results = evaluate_models(models, y_train, y_val, y_test, fh)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Pick best one --> Optimize with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tGet\tfeature\timportances\tfrom\tthe\tmodel\n",
    "feature_importances = best_model.get_feature_importance(prettified=False)\n",
    "\n",
    "# \tGet\tfeature\tnames\t(considering\tpotential\ttransformation)\n",
    "feature_names = preprocessor.get_feature_names_out()  # \tAfter\tcolumn\ttransformation\n",
    "\n",
    "# \tSort\tfeature\timportances\tand\tnames\ttogether\tby\timportance\t(descending)\n",
    "sorted_idx = np.argsort(feature_importances)\n",
    "feature_importances = feature_importances[sorted_idx]\n",
    "feature_names = feature_names[sorted_idx]\n",
    "\n",
    "# \tDefine\tplot\tsize\tand\tcreate\ta\tbar\tchart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(feature_names)), feature_importances, align=\"center\")\n",
    "plt.yticks(range(len(feature_names)), feature_names)\n",
    "plt.xlabel(\"Feature\tImportance\")\n",
    "plt.ylabel(\"Feature\tNames\")\n",
    "plt.title(\"Feature\tImportance\tfor\tElectricity\tDemand-Supply\tPrediction\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"depth\": [4, 6, 8],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"iterations\": [50, 100, 200],\n",
    "}\n",
    "\n",
    "best_model = CatBoostRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=best_model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"svm__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    \"svm__gamma\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "pipe = pipeline.Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross-validation\n",
    "https://ml-course.github.io/master/notebooks/Tutorial%203%20-%20Machine%20Learning%20in%20Python.html#evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    GridSearchCV(SVC(), param_grid, cv=5), iris.data, iris.target, cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    GridSearchCV(SVC(), param_grid, cv=5), iris.data, iris.target, cv=5\n",
    ")\n",
    "print(\"Cross-validation scores: \", scores)\n",
    "print(\"Mean cross-validation score: \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda.to_csv(\"final_model.csv\", index=False)\n",
    "# \tSave\tthe\ttrained\tmodel\n",
    "lr_model.save_model(\"catboost_model.cbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: Residual analysis?\n",
    "--> Check if errors are randomly distributed in pointcloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_case_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
