{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpLWCBFNuons"
   },
   "source": [
    "### Supermarket data science case study - Exploring first data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPQo8za16cQT"
   },
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1608147217504,
     "user": {
      "displayName": "pi1234",
      "photoUrl": "",
      "userId": "00485952581093036663"
     },
     "user_tz": -60
    },
    "id": "OlwScieYRTmf",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "import vegafusion as vf\n",
    "import sklearn\n",
    "import vega_datasets\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm7aDtim6guo"
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1608147217505,
     "user": {
      "displayName": "pi1234",
      "photoUrl": "",
      "userId": "00485952581093036663"
     },
     "user_tz": -60
    },
    "id": "f1PDhgg8ZBbf",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def f_concat(l_input):\n",
    "\n",
    "    # Initialize.\n",
    "    dummy = \"\"\n",
    "    n_len = len(l_input)\n",
    "\n",
    "    if n_len == 1:\n",
    "        return l_input[0]\n",
    "\n",
    "    # Loop through text elements.\n",
    "    for i in range(n_len - 1):\n",
    "        dummy = dummy + l_input[i] + \", \"\n",
    "\n",
    "    # Append last element.\n",
    "    dummy = dummy + \"and \" + l_input[n_len - 1]\n",
    "\n",
    "    # Return result.\n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def f_describe(df_input, n_top=10):\n",
    "\n",
    "    print(\"First \" + str(n_top) + \" rows in de data:\")\n",
    "    display(df_input.head(n_top))\n",
    "\n",
    "    df_numeric = df_input.select_dtypes(\n",
    "        include=[\n",
    "            \"uint8\",\n",
    "            \"uint16\",\n",
    "            \"uint32\",\n",
    "            \"uint64\",\n",
    "            \"int8\",\n",
    "            \"int16\",\n",
    "            \"int32\",\n",
    "            \"int64\",\n",
    "            \"float16\",\n",
    "            \"float32\",\n",
    "            \"float64\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if len(df_numeric.columns):\n",
    "        print(\"Numerical data:\")\n",
    "        display(df_numeric.describe())\n",
    "\n",
    "    df_textual = df_input.select_dtypes(include=[\"category\", \"object\", \"bool\"])\n",
    "\n",
    "    if len(df_textual.columns):\n",
    "        print(\"Textual data:\")\n",
    "        display(df_textual.describe())\n",
    "\n",
    "    v_na = [\n",
    "        col\n",
    "        + \" (\"\n",
    "        + str(df[col].isna().sum())\n",
    "        + \", \"\n",
    "        + str(round(100 * df[col].isna().sum() / df.shape[0], 1))\n",
    "        + \"%)\"\n",
    "        for col in df.columns\n",
    "        if df[col].isna().sum() > 0\n",
    "    ]\n",
    "\n",
    "    if len(v_na) > 0:\n",
    "        print(\"Features and their number of missing values:\")\n",
    "        display(f_concat(v_na))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downcast and transform data\n",
    "Update formatting of features to optimize memory and standardize column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def standardize_column_names(s):\n",
    "    return s.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "def optimize_memory(df):\n",
    "    # Change: Objects to Categorical.                                               #WHEN needed to transform Objects to Categorical?\n",
    "    # object_cols = df.select_dtypes(include=\"object\").columns\n",
    "    # if not object_cols.empty:\n",
    "    #     print(\"Change: Objects to Categorical\")\n",
    "    #     df[object_cols] = df[object_cols].astype(\"category\")\n",
    "\n",
    "    # Change: Convert integers to smallest unsigned integer and floats to smallest.\n",
    "    for old, new in [(\"integer\", \"unsigned\"), (\"float\", \"float\")]:\n",
    "        print(\"Change: \" + old + \" --> \" + new)\n",
    "        for col in df.select_dtypes(include=old).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=new)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def month_year_to_int(df, i):\n",
    "    # Change: Month and Year to integer\n",
    "    if i == 0:\n",
    "        print(\"Change: Month and Year to integer\")\n",
    "        df = df.astype({\"month\": int, \"year\": int})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform date-related columns to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Convert datasets to time series\n",
    "def transform_date_to_datetime(df, i):\n",
    "    if i == 0:\n",
    "        print(\"Change: Transformed 'year', 'month', 'day' columns to Datetime feature\")\n",
    "        df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]], unit=\"us\")\n",
    "\n",
    "       # print(\n",
    "       #     \"Change: Dropped 'year', 'month', 'day' columns and transformed to Datetime64[us] feature\"\n",
    "       # )\n",
    "        #df.drop(columns=[\"day\", \"month\", \"year\"], inplace=True)\n",
    "\n",
    "    else:\n",
    "        if \"date\" in df.columns:\n",
    "            print(\"Change: Transformed 'date' column to Datetime Dtype\")\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data from local PATH\n",
    "Import data trough pipeline to downcast the data and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XaDlRA_wRK0U",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def f_get_data(i=0):\n",
    "\n",
    "    # Define path.\n",
    "    c_path = \"/Users/Georgi/Documents/EAISI/EASI_4B_Supermarket/Group4B/data/raw/\"\n",
    "    # Identify file.\n",
    "    v_file = (\n",
    "        \"history-per-year\",  # 0\n",
    "        \"history_aggregated\",  # 1\n",
    "        \"holidays_events\",  # 2\n",
    "        \"items\",  # 3\n",
    "        \"oil\",  # 4\n",
    "        \"stores\",  # 5\n",
    "        \"transactions\",\n",
    "    )  # 6\n",
    "\n",
    "    # Load data.\n",
    "    df = (\n",
    "        pd.read_parquet(c_path + v_file[i] + \".parquet\")\n",
    "        .rename(columns=standardize_column_names)\n",
    "        .pipe(optimize_memory)\n",
    "        .pipe(month_year_to_int, i)\n",
    "        .pipe(transform_date_to_datetime, i)\n",
    "    )\n",
    "\n",
    "    # Return data.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zXWuFInRee5"
   },
   "source": [
    "### Importing data: Here I import the daily data. The data contains daily sales data excluding holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_salesdata\u001b[38;5;241m=\u001b[39m \u001b[43mf_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_items \u001b[38;5;241m=\u001b[39mf_get_data(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_stores \u001b[38;5;241m=\u001b[39mf_get_data(\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m, in \u001b[0;36mf_get_data\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m      6\u001b[0m v_file \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory-per-year\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 0\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory_aggregated\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 1\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransactions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m )  \u001b[38;5;66;03m# 6\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load data.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m df \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_parquet(c_path \u001b[38;5;241m+\u001b[39m v_file[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39mstandardize_column_names)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m.\u001b[39mpipe(optimize_memory)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;241m.\u001b[39mpipe(month_year_to_int, i)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39mpipe(transform_date_to_datetime, i)\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Return data.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_salesdata= f_get_data(0)\n",
    "df_items =f_get_data(3)\n",
    "df_stores =f_get_data(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_stores.info())\n",
    "#print(df_salesdata.info())\n",
    "print(df_items.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust data types and drop columns we don't need\n",
    "df_salesdata['store_nbr'] = df_salesdata['store_nbr'].astype(str)\n",
    "df_salesdata = df_salesdata.drop(columns=['year', 'day','onpromotion','month'])\n",
    "df_stores['store_nbr'] = df_stores['store_nbr'].astype(str)\n",
    "df_stores['cluster'] = df_stores['cluster'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_stores.info())\n",
    "print(df_salesdata.info())\n",
    "print(df_items.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the sales date by store and item\n",
    "df_salesdatagrouped = df_salesdata.groupby(['store_nbr','date']).agg({'unit_sales':'sum'}).reset_index()\n",
    "\n",
    "print(f' In df_salesdatagrouped zitten nu {df_salesdatagrouped.shape[0]} rijen en {df_salesdatagrouped.shape[1]} kolommen')\n",
    "print(df_salesdatagrouped.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salesandstoresdata = df_salesdatagrouped.merge(df_stores, left_on='store_nbr', right_on='store_nbr', how='inner')\n",
    "\n",
    "print(f' In df_salesandstoredata zitten nu {df_salesandstoresdata.shape[0]} rijen en {df_salesandstoresdata.shape[1]} kolommen')\n",
    "print(df_salesandstoresdata.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.0- Filter out all stores that don't have all the datapoint or atleast mark them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count amount of values per store\n",
    "se_storedatecount = df_salesandstoresdata['store_nbr'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date range from the start date to the end date of the sales data\n",
    "start_date = pd.to_datetime('2013-01-02')\n",
    "end_date = pd.to_datetime('2017-08-15')\n",
    "\n",
    "# Create a date range variable from the start date to the end date of the sales data\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Create a dataframe from the date range\n",
    "date_range = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "print(f'The date_range dataframe starts at {date_range[\"date\"].min()} and ends at {date_range[\"date\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salesandstoresdata34 = df_salesandstoresdata[df_salesandstoresdata['store_nbr'] == '34']\n",
    "\n",
    "df_salesandstoresdata34missingdates = df_salesandstoresdata34.merge(date_range, left_on='date', right_on='date', how='outer')\n",
    "\n",
    "empty_unit_sales = df_salesandstoresdata34missingdates[df_salesandstoresdata34missingdates['unit_sales'].isnull()]\n",
    "print(empty_unit_sales)\n",
    "print('As we can see, stores that have all data seem to be closed on christmas day and on new years day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to make a dataframe that consists of all stores that are missing data for a certain date\n",
    "# Step 1 - Crossjoin stores with the daterange\n",
    "df_storesreduced = df_stores.drop(columns=['city', 'state', 'type', 'cluster'])\n",
    "df_storesanddates = df_storesreduced.merge(date_range, how='cross')\n",
    "\n",
    "print(f' Now we onstructed a dataframe with all stores and all dates, it contains {df_storesanddates.shape[0]} rows')\n",
    "print(df_storesanddates.head(5))\n",
    "\n",
    "# Step 2 - Merge the salesdata with the storesanddates dataframe to have a dataframe consisting of all stores and all dates with unit_sales\n",
    "\n",
    "df_salesandstoresdata_alldates = df_salesandstoresdata.merge(df_storesanddates, on = ['store_nbr','date'], how='outer')\n",
    "\n",
    "print(f' Now we onstructed a dataframe with all stores and all dates, it contains {df_salesandstoresdata_alldates.shape[0]} rows')\n",
    "print(df_salesandstoresdata_alldates.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes and keep only the records that are in the first dataframe but not in the second dataframe\n",
    "Difference_df_salesandstoresdata_alldates_df_storesanddates = df_salesandstoresdata_alldates.merge(df_storesanddates, on = ['store_nbr','date'], how='outer', indicator=True).loc[lambda x : x['_merge']=='left_only']\n",
    "Difference_df_salesandstoresdata_alldates_df_storesanddates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's see how this works out for store number 30 (just a random one that is missing some dates according to our earlier analysis)\n",
    "df_salesandstoresdata_alldates30 = df_salesandstoresdata_alldates[df_salesandstoresdata_alldates['store_nbr']=='30']\n",
    "df_salesandstoresdata_alldates30 = df_salesandstoresdata_alldates30[df_salesandstoresdata_alldates30['unit_sales'].isnull()]\n",
    "df_salesandstoresdata_alldates30.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the stores that have are value count less than 1679 in se_storedatecount\n",
    "se_storedatecountmissing = se_storedatecount[se_storedatecount < 1679]\n",
    "\n",
    "# Now, let's have df_salesandstoresdata_alldates but only for the stores where we are missing some of the data (well, atleast we miss sales on those date, we don't have them in the original data)\n",
    "df_salesandstoresdata_alldatesnull = df_salesandstoresdata_alldates[df_salesandstoresdata_alldates['store_nbr'].isin(se_storedatecountmissing.index)]\n",
    "\n",
    "# From the stores with missing data, we only want the records where the unit_sales is missing\n",
    "df_salesandstoresdata_alldatesnull = df_salesandstoresdata_alldatesnull[df_salesandstoresdata_alldatesnull['unit_sales'].isnull()]\n",
    "\n",
    "df_salesandstoresdata_alldatesnull = df_salesandstoresdata_alldatesnull[['date', 'store_nbr','unit_sales']]\n",
    "\n",
    "# Add a unit_sales of 1 to the dataframe to make it easier to plot, it's just a dummy value\n",
    "df_salesandstoresdata_alldatesnull['unit_sales'] = 1\n",
    "\n",
    "# Merge the dataframe with the date_range dataframe to have all dates in the dataframe\n",
    "df_salesandstoresdata_alldatesnull = df_storesanddates.merge(df_salesandstoresdata_alldatesnull, on=['store_nbr','date'] ,how='left')\n",
    "\n",
    "# Now we have a dataframe with all stores and all dates, but only for the stores that are missing some data\n",
    "df_salesandstoresdata_alldatesnull = df_salesandstoresdata_alldatesnull[df_salesandstoresdata_alldatesnull['store_nbr'].isin(se_storedatecountmissing.index)]\n",
    "\n",
    "print(f\"Stores {df_salesandstoresdata_alldatesnull['store_nbr'].unique()} are in the dataset with stores with <1679 datapoints and all dates, having imputed a value of 1 for all dates missing in the range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salesandstoresdata_alldatesnull36 = df_salesandstoresdata_alldatesnull[df_salesandstoresdata_alldatesnull['store_nbr'] == '36']\n",
    "df_salesandstoresdata_alldatesnull36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores that are new we mark as 1, stores that are old we mark as 0\n",
    "se_storedatecountmissingsome = se_storedatecount[se_storedatecount < 1670]\n",
    "\n",
    "df_salesandstoresdata_alldatesnull1 = df_salesandstoresdata_alldatesnull.copy()\n",
    "\n",
    "# Identify stores that are new based on having a dummy value on 2013-01-02\n",
    "new_store_nbrs = df_salesandstoresdata_alldatesnull[\n",
    "    (df_salesandstoresdata_alldatesnull['date'] == '2013-01-02') & \n",
    "    (df_salesandstoresdata_alldatesnull['unit_sales'] == 1)\n",
    "]['store_nbr'].unique()\n",
    "\n",
    "# Make a new column missingdatacategory where stores that have a dummy unit for 2013-01-02 are marked as a new store, the rest for now is seen as an old store. This is still the whole dataset \n",
    "# We get the right storenumbers based on the isin part of the expression\n",
    "df_salesandstoresdata_alldatesnull1['missingdatacategory'] = np.where(df_salesandstoresdata_alldatesnull1['store_nbr'].isin(new_store_nbrs),\n",
    "                                                                     'new_store', \n",
    "                                                                     'old_store'\n",
    "                                                                     )\n",
    "\n",
    "# Step 2 - For all stores that have < 1670 days of data, name the stores that are not new and old store missing > days of data\n",
    "df_salesandstoresdata_alldatesnull2 = df_salesandstoresdata_alldatesnull1[df_salesandstoresdata_alldatesnull1['store_nbr'].isin(se_storedatecountmissingsome.index)]\n",
    "\n",
    "df_salesandstoresdata_alldatesnull2['missingdatacategory'] = np.where((df_salesandstoresdata_alldatesnull2['missingdatacategory'] == 'new_store'),\n",
    "                                                                    'new_store',\n",
    "                                                                    'old_store missing >9 days'\n",
    "                                                                    )\n",
    "\n",
    "# Step 3 - For all stores that are missing <9 days of data we just label them \"missing < 9 days\"\n",
    "df_salesandstoresdata_alldatesnull3 = df_salesandstoresdata_alldatesnull[~df_salesandstoresdata_alldatesnull1['store_nbr'].isin(se_storedatecountmissingsome.index)]\n",
    "df_salesandstoresdata_alldatesnull3['missingdatacategory'] =        'missing <9 days'\n",
    "\n",
    "# Put the dataframes of step 2 and 3 together to get all rows back together as in the original dataframes\n",
    "df_salesandstoresdata_alldatesnullfinal = pd.concat([df_salesandstoresdata_alldatesnull2, df_salesandstoresdata_alldatesnull3])\n",
    "\n",
    "print(df_salesandstoresdata_alldatesnull.shape)\n",
    "print(df_salesandstoresdata_alldatesnull1.shape)\n",
    "print(df_salesandstoresdata_alldatesnull2.shape)\n",
    "print(df_salesandstoresdata_alldatesnull3.shape)\n",
    "\n",
    "# Make a dataframe that groups the data by store and missingdatacategory\n",
    "df_salesandstoresdata_alldatesnullfinal = df_salesandstoresdata_alldatesnullfinal.groupby(['store_nbr','missingdatacategory']).agg({'unit_sales':'count'}).reset_index()\n",
    "\n",
    "# Add a dummy value to the dataframe to make it easier to plot or to join with other dataframes\n",
    "df_salesandstoresdata_alldatesnullfinal['missingdata'] = '1'\n",
    "df_salesandstoresdata_alldatesnullfinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4.0 Determine the imapct of stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total unit sales per store from the salesandstoresdata dataframe (this original dataframe grouped the data by store and date with the original dates)\n",
    "df_salesandstoresdatatotal = df_salesandstoresdata.groupby(['store_nbr']).agg({'unit_sales':'sum'}).reset_index()\n",
    "\n",
    "# Take the df_salesandstoresdata_alldatesnullfinal dataframe and merge it with the df_salesandstoresdatatotal dataframe to get the total unit sales per store and the marking if dates are missing per store including the categories why something is missing.\n",
    "df_salesandstoresdatatotal = df_salesandstoresdatatotal.merge(df_salesandstoresdata_alldatesnullfinal, on='store_nbr', how='left')\n",
    "\n",
    "# Drop the unit_sales_y column and rename the unit_sales_x column to unit_sales (just cleaning things from the last merge)\n",
    "df_salesandstoresdatatotal = df_salesandstoresdatatotal.drop(columns=['unit_sales_y'])\n",
    "df_salesandstoresdatatotal = df_salesandstoresdatatotal.rename(columns={'unit_sales_x':'unit_sales'})\n",
    "\n",
    "# If a store isn't missing data, give the missingdata column a value of 0, do the same for the missingdatacategory column\n",
    "df_salesandstoresdatatotal['missingdata'] = df_salesandstoresdatatotal['missingdata'].fillna('0')\n",
    "df_salesandstoresdatatotal['missingdatacategory'] = df_salesandstoresdatatotal['missingdatacategory'].fillna('0')\n",
    "\n",
    "# Sort the dataframe by unit_sales (we want to have the highest sales first)\n",
    "df_salesandstoresdatatotal = df_salesandstoresdatatotal.sort_values(by='unit_sales', ascending=False)\n",
    "\n",
    "df_salesandstoresdatatotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by missingdata and missingdatacategory and calculate the percentage of the total unit sales per store\n",
    "df_salesandstoresdatatotalgroupedby = df_salesandstoresdatatotal.groupby(['missingdata','missingdatacategory']).agg({'unit_sales':'sum'}).reset_index()\n",
    "df_salesandstoresdatatotalgroupedby['Percentage'] = df_salesandstoresdatatotalgroupedby['unit_sales']/df_salesandstoresdatatotalgroupedby['unit_sales'].sum()*100\n",
    "\n",
    "df_salesandstoresdatatotalgroupedby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the df_salesandstoresdata dataframe again and filter it for the year 2017 and the month july\n",
    "\n",
    "# Filter rows for July 2017\n",
    "df_salesandstoresdata_july_2017 = df_salesandstoresdata[(df_salesandstoresdata['date'].dt.year == 2017) & (df_salesandstoresdata['date'].dt.month == 7)]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "print(df_salesandstoresdata_july_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by store and calculate the total unit sales per store (we repeat the same steps as we did for the whole time period)\n",
    "df_salesandstoresdata_july_2017_total = df_salesandstoresdata_july_2017.groupby(['store_nbr']).agg({'unit_sales':'sum'}).reset_index()\n",
    "\n",
    "# Take the df_salesandstoresdata_alldatesnullfinal dataframe and merge it with the df_salesandstoresdatatotal dataframe to get the total unit sales per store and the marking if dates are missing per store including the categories why something is missing.\n",
    "df_salesandstoresdata_july_2017_total = df_salesandstoresdata_july_2017_total.merge(df_salesandstoresdata_alldatesnullfinal, on='store_nbr', how='left')\n",
    "\n",
    "# Drop the unit_sales_y column and rename the unit_sales_x column to unit_sales (just cleaning things from the last merge)\n",
    "df_salesandstoresdata_july_2017_total = df_salesandstoresdata_july_2017_total.drop(columns=['unit_sales_y'])\n",
    "df_salesandstoresdata_july_2017_total = df_salesandstoresdata_july_2017_total.rename(columns={'unit_sales_x':'unit_sales'})\n",
    "\n",
    "# If a store isn't missing data, give the missingdata column a value of 0, do the same for the missingdatacategory column\n",
    "df_salesandstoresdata_july_2017_total['missingdata'] = df_salesandstoresdata_july_2017_total['missingdata'].fillna('0')\n",
    "df_salesandstoresdata_july_2017_total['missingdatacategory'] = df_salesandstoresdata_july_2017_total['missingdatacategory'].fillna('0')\n",
    "\n",
    "# Sort the dataframe by unit_sales (we want to have the highest sales first)\n",
    "df_salesandstoresdata_july_2017_total = df_salesandstoresdata_july_2017_total.sort_values(by='unit_sales', ascending=False)\n",
    "\n",
    "df_salesandstoresdata_july_2017_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by missingdata and missingdatacategory and calculate the percentage of the total unit sales per store\n",
    "df_salesandstoresdata_july_2017_totalgroupedby = df_salesandstoresdata_july_2017_total.groupby(['missingdata','missingdatacategory']).agg({'unit_sales':'sum'}).reset_index()\n",
    "df_salesandstoresdata_july_2017_totalgroupedby['Percentage'] = df_salesandstoresdata_july_2017_totalgroupedby['unit_sales']/df_salesandstoresdata_july_2017_totalgroupedby['unit_sales'].sum()*100\n",
    "\n",
    "df_salesandstoresdata_july_2017_totalgroupedby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salesandstoresdata_july_2017_totalcopy = df_salesandstoresdata_july_2017_total.copy()\n",
    "\n",
    "# Let's investigate the share per store type for july 2017\n",
    "df_salesandstoresdata_july_2017_total_type = df_salesandstoresdata_july_2017_totalcopy.merge(df_stores, on='store_nbr', how='left')\n",
    "df_salesandstoresdata_july_2017_total_type = df_salesandstoresdata_july_2017_total_type.groupby(['type']).agg({'unit_sales':'sum'}).reset_index()\n",
    "df_salesandstoresdata_july_2017_total_type = df_salesandstoresdata_july_2017_total_type.sort_values(by='unit_sales', ascending=False)\n",
    "df_salesandstoresdata_july_2017_total_type['Percentage'] = df_salesandstoresdata_july_2017_total_type['unit_sales']/df_salesandstoresdata_july_2017_total_type['unit_sales'].sum()*100\n",
    "df_salesandstoresdata_july_2017_total_type['CumulativePercentage'] = df_salesandstoresdata_july_2017_total_type['Percentage'].cumsum()\n",
    "\n",
    "df_salesandstoresdata_july_2017_total_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add store information (like type, cluster, state and city) to the dataframe with the total unit sales per store for July 2017\n",
    "df_salesandstoresdata_july_2017_total2 = df_salesandstoresdata_july_2017_totalcopy.merge(df_stores, on='store_nbr', how='left')\n",
    "\n",
    "# Add the total unit sales for all stores to each row of the dataframe (this makes it easier to calculate the percentage of the total unit sales per store)\n",
    "df_salesandstoresdata_july_2017_total2['Total unit sales'] = df_salesandstoresdata_july_2017_total2['unit_sales'].sum()\n",
    "\n",
    "# Calculate the percentage of the total unit sales per store\n",
    "df_salesandstoresdata_july_2017_total2['Percentage'] = df_salesandstoresdata_july_2017_total2['unit_sales']/df_salesandstoresdata_july_2017_total2['Total unit sales']*100\n",
    "\n",
    "df_salesandstoresdata_july_2017_total2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salesandstoresdata_july_2017_total3 = df_salesandstoresdata_july_2017_total2.copy()\n",
    "\n",
    "# STEP 1 - df_salesandstoresdata_july_2017_total2 only with missingdatacategory 0 or missing <9 days\n",
    "df_salesandstoresdata_july_2017_total4 = df_salesandstoresdata_july_2017_total3[(df_salesandstoresdata_july_2017_total3['missingdatacategory'] == '0') | (df_salesandstoresdata_july_2017_total3['missingdatacategory'] == 'missing <9 days')]\n",
    "\n",
    "# STEP 2 - Drop the rows where the cluster is 10 (this is a missing value)\n",
    "df_salesandstoresdata_july_2017_total4 = df_salesandstoresdata_july_2017_total4[df_salesandstoresdata_july_2017_total4['cluster'] != '10']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_salesandstoresdata_july_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores_f = df_salesandstoresdata_july_2017_total4[['store_nbr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_salesandstoresdata_july_2017[df_salesandstoresdata_july_2017['store_nbr'].isin(df_stores_f['store_nbr'])]\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salesdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the df_salesdata DataFrame for rows where 'store_nbr' is in df_stores_f\n",
    "df_filtered_salesdata = df_salesdata[df_salesdata['store_nbr'].isin(df_stores_f['store_nbr'])]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(df_filtered_salesdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the df_salesdata DataFrame for rows where 'store_nbr' is in df_stores_f and date is in July 2017\n",
    "filtered_df_salesdata_july2017 = df_filtered_salesdata[\n",
    "   \n",
    "    (df_filtered_salesdata['date'] >= '2017-07-01') & \n",
    "    (df_filtered_salesdata['date'] <= '2017-07-31')\n",
    "]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_df_salesdata_july2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the filtered sales data with the items data\n",
    "merged_df = pd.merge(filtered_df_salesdata_july2017, df_items, on=\"item_nbr\", how=\"inner\")\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total sales for each family\n",
    "family_sales = merged_df.groupby('family')['unit_sales'].sum()\n",
    "\n",
    "# Calculate total sales across all families\n",
    "total_sales = family_sales.sum()\n",
    "\n",
    "# Calculate the percentage of total sales for each family\n",
    "family_sales_percentage = (family_sales / total_sales) * 100\n",
    "\n",
    "# Convert the result to a DataFrame for better readability\n",
    "family_sales_percentage_df = family_sales_percentage.reset_index()\n",
    "family_sales_percentage_df.columns = ['family', 'percentage_of_total_sales']\n",
    "\n",
    "# Sort the DataFrame by percentage in descending order\n",
    "family_sales_percentage_df = family_sales_percentage_df.sort_values(by='percentage_of_total_sales', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(family_sales_percentage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total sales for each family\n",
    "total_sales_per_family = merged_df.groupby('family')['unit_sales'].sum()\n",
    "\n",
    "# Calculate overall total sales\n",
    "overall_total_sales = total_sales_per_family.sum()\n",
    "\n",
    "# Calculate percentage of total sales for each family\n",
    "sales_percentage_per_family = (total_sales_per_family / overall_total_sales) * 100\n",
    "\n",
    "# Group by 'family' and count distinct 'store_nbr'\n",
    "distinct_stores_per_family = merged_df.groupby('family')['store_nbr'].nunique()\n",
    "\n",
    "# Convert results to DataFrames for merging\n",
    "distinct_stores_df = distinct_stores_per_family.reset_index()\n",
    "distinct_stores_df.columns = ['family', 'distinct_stores_count']\n",
    "\n",
    "sales_percentage_df = sales_percentage_per_family.reset_index()\n",
    "sales_percentage_df.columns = ['family', 'sales_percentage']\n",
    "\n",
    "# Merge the two DataFrames\n",
    "result_df = pd.merge(distinct_stores_df, sales_percentage_df, on='family')\n",
    "\n",
    "# Sort by the percentage of sales in descending order\n",
    "result_df = result_df.sort_values(by='sales_percentage', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So, as we can see all WE can removie baby care and books as they are not seen in all stores. We lose 0.02. Now we want to look on items level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate total sales for each item\n",
    "total_sales_per_item = merged_df.groupby('item_nbr')['unit_sales'].sum()\n",
    "\n",
    "# Calculate overall total sales\n",
    "overall_total_sales = total_sales_per_item.sum()\n",
    "\n",
    "# Calculate percentage of total sales for each item\n",
    "sales_percentage_per_item = (total_sales_per_item / overall_total_sales) * 100\n",
    "\n",
    "# Count distinct stores for each item\n",
    "distinct_stores_per_item = merged_df.groupby('item_nbr')['store_nbr'].nunique()\n",
    "\n",
    "# Convert results to DataFrames for merging\n",
    "distinct_stores_df = distinct_stores_per_item.reset_index()\n",
    "distinct_stores_df.columns = ['item_nbr', 'distinct_stores_count']\n",
    "\n",
    "sales_percentage_df = sales_percentage_per_item.reset_index()\n",
    "sales_percentage_df.columns = ['item_nbr', 'sales_percentage']\n",
    "\n",
    "# Merge the two DataFrames\n",
    "result_df = pd.merge(distinct_stores_df, sales_percentage_df, on='item_nbr')\n",
    "\n",
    "# Sort by sales percentage in descending order\n",
    "result_df = result_df.sort_values(by='sales_percentage', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(result_df)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure 'date' column is datetime type\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "\n",
    "# Define the range of dates (July 2017 in this case)\n",
    "date_range = pd.date_range(start='2017-07-01', end='2017-07-31')\n",
    "\n",
    "# Get unique store and item numbers\n",
    "stores = merged_df['store_nbr'].unique()\n",
    "items = merged_df['item_nbr'].unique()\n",
    "\n",
    "# Create a DataFrame with all combinations of store, item, and date\n",
    "all_combinations = pd.MultiIndex.from_product([stores, items, date_range], names=['store_nbr', 'item_nbr', 'date']).to_frame(index=False)\n",
    "\n",
    "# Merge with the original DataFrame\n",
    "df_full = pd.merge(all_combinations, merged_df, on=['store_nbr', 'item_nbr', 'date'], how='left')\n",
    "\n",
    "# Fill NaN values in 'unit_sales' with 0\n",
    "df_full['unit_sales'] = df_full['unit_sales'].fillna(0)\n",
    "\n",
    "# Optionally, reset the index if needed\n",
    "df_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the balanced DataFrame\n",
    "print(df_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'date' column is datetime type\n",
    "df_full['date'] = pd.to_datetime(df_full['date'])\n",
    "\n",
    "# Step 1: Filter out rows where 'unit_sales' is zero or NaN\n",
    "df_positive_sales = df_full[df_full['unit_sales'] > 0]\n",
    "\n",
    "# Step 2: Group by store and item, then count unique dates with sales > 0\n",
    "daily_sales_counts = df_positive_sales.groupby(['store_nbr', 'item_nbr'])['date'].nunique().reset_index()\n",
    "daily_sales_counts.rename(columns={'date': 'days_with_sales'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the average number of days with sales > 0 per item\n",
    "average_daily_sales = daily_sales_counts.groupby(['item_nbr'])['days_with_sales'].mean().reset_index()\n",
    "\n",
    "# Step 4: Calculate total sales for each item across all stores and dates\n",
    "total_sales_per_item = df_full.groupby('item_nbr')['unit_sales'].sum().reset_index()\n",
    "total_sales_per_item.rename(columns={'unit_sales': 'total_sales'}, inplace=True)\n",
    "\n",
    "# Step 5: Calculate total sales for all items\n",
    "grand_total_sales = total_sales_per_item['total_sales'].sum()\n",
    "\n",
    "# Step 6: Calculate the percentage of sales for each item\n",
    "total_sales_per_item['sales_percentage'] = (total_sales_per_item['total_sales'] / grand_total_sales) * 100\n",
    "\n",
    "# Step 7: Merge the percentage information with the average daily sales\n",
    "average_daily_sales = average_daily_sales.merge(total_sales_per_item[['item_nbr', 'sales_percentage']], on='item_nbr')\n",
    "\n",
    "# Step 8: Print or inspect the results\n",
    "print(average_daily_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Ensure 'date' column is datetime type\n",
    "df_full['date'] = pd.to_datetime(df_full['date'])\n",
    "\n",
    "# Filter out rows where 'unit_sales' is zero or NaN\n",
    "df_positive_sales = df_full[df_full['unit_sales'] > 0]\n",
    "\n",
    "# Group by store and item, then count unique dates with sales > 0\n",
    "daily_sales_counts = df_positive_sales.groupby(['store_nbr', 'item_nbr'])['date'].nunique().reset_index()\n",
    "daily_sales_counts.rename(columns={'date': 'days_with_sales'}, inplace=True)\n",
    "\n",
    "# Calculate the average number of days with sales > 0 per item\n",
    "average_daily_sales = daily_sales_counts.groupby(['item_nbr'])['days_with_sales'].mean().reset_index()\n",
    "\n",
    "# Calculate total sales for each item across all stores and dates\n",
    "total_sales_per_item = df_full.groupby('item_nbr')['unit_sales'].sum().reset_index()\n",
    "total_sales_per_item.rename(columns={'unit_sales': 'total_sales'}, inplace=True)\n",
    "\n",
    "# Calculate total sales for all items\n",
    "grand_total_sales = total_sales_per_item['total_sales'].sum()\n",
    "\n",
    "# Calculate the percentage of sales for each item\n",
    "total_sales_per_item['sales_percentage'] = (total_sales_per_item['total_sales'] / grand_total_sales) * 100\n",
    "\n",
    "# Merge the percentage information with the average daily sales\n",
    "average_daily_sales = average_daily_sales.merge(total_sales_per_item[['item_nbr', 'sales_percentage']], on='item_nbr')\n",
    "\n",
    "# Print basic statistics for average_daily_sales\n",
    "print(\"Basic Statistics for `average_daily_sales` DataFrame:\")\n",
    "print(average_daily_sales.describe())\n",
    "\n",
    "# View the first few rows\n",
    "print(\"\\nFirst few rows of `average_daily_sales` DataFrame:\")\n",
    "print(average_daily_sales.head())\n",
    "\n",
    "# View the last few rows\n",
    "print(\"\\nLast few rows of `average_daily_sales` DataFrame:\")\n",
    "print(average_daily_sales.tail())\n",
    "\n",
    "# DataFrame info\n",
    "print(\"\\nInfo on `average_daily_sales` DataFrame:\")\n",
    "print(average_daily_sales.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Ensure 'date' column is datetime type\n",
    "df_full['date'] = pd.to_datetime(df_full['date'])\n",
    "\n",
    "# Filter out rows where 'unit_sales' is zero or NaN\n",
    "df_positive_sales = df_full[df_full['unit_sales'] > 0]\n",
    "\n",
    "# Group by store and item, then count unique dates with sales > 0\n",
    "daily_sales_counts = df_positive_sales.groupby(['store_nbr', 'item_nbr'])['date'].nunique().reset_index()\n",
    "daily_sales_counts.rename(columns={'date': 'days_with_sales'}, inplace=True)\n",
    "\n",
    "# Calculate the average number of days with sales > 0 per item\n",
    "average_daily_sales = daily_sales_counts.groupby(['item_nbr'])['days_with_sales'].mean().reset_index()\n",
    "\n",
    "# Calculate total sales for each item across all stores and dates\n",
    "total_sales_per_item = df_full.groupby('item_nbr')['unit_sales'].sum().reset_index()\n",
    "total_sales_per_item.rename(columns={'unit_sales': 'total_sales'}, inplace=True)\n",
    "\n",
    "# Calculate total sales for all items\n",
    "grand_total_sales = total_sales_per_item['total_sales'].sum()\n",
    "\n",
    "# Calculate the percentage of sales for each item\n",
    "total_sales_per_item['sales_percentage'] = (total_sales_per_item['total_sales'] / grand_total_sales) * 100\n",
    "\n",
    "# Merge the percentage information with the average daily sales\n",
    "average_daily_sales = average_daily_sales.merge(total_sales_per_item[['item_nbr', 'sales_percentage']], on='item_nbr')\n",
    "\n",
    "# Filter items that are sold for fewer than 10 days on average\n",
    "items_sold_fewer_than_10_days = average_daily_sales[average_daily_sales['days_with_sales'] < 10]\n",
    "\n",
    "# Calculate the total number of items\n",
    "total_items = average_daily_sales.shape[0]\n",
    "\n",
    "# Calculate the number of items sold for fewer than 10 days\n",
    "items_sold_fewer_than_10_days_count = items_sold_fewer_than_10_days.shape[0]\n",
    "\n",
    "# Compute the percentage\n",
    "percentage_sold_fewer_than_10_days = (items_sold_fewer_than_10_days_count / total_items) * 100\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total percentage of items sold for fewer than 10 days on average: {percentage_sold_fewer_than_10_days:.2f}%\")\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame setup (Replace this with loading your actual DataFrame)\n",
    "# df_full = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Ensure 'date' column is datetime type\n",
    "df_full['date'] = pd.to_datetime(df_full['date'])\n",
    "\n",
    "# Filter out rows where 'unit_sales' is zero or NaN\n",
    "df_positive_sales = df_full[df_full['unit_sales'] > 0]\n",
    "\n",
    "# Group by store and item, then count unique dates with sales > 0\n",
    "daily_sales_counts = df_positive_sales.groupby(['store_nbr', 'item_nbr'])['date'].nunique().reset_index()\n",
    "daily_sales_counts.rename(columns={'date': 'days_with_sales'}, inplace=True)\n",
    "\n",
    "# Calculate the average number of days with sales > 0 per item\n",
    "average_daily_sales = daily_sales_counts.groupby(['item_nbr'])['days_with_sales'].mean().reset_index()\n",
    "\n",
    "# Calculate total sales for each item across all stores and dates\n",
    "total_sales_per_item = df_full.groupby('item_nbr')['unit_sales'].sum().reset_index()\n",
    "total_sales_per_item.rename(columns={'unit_sales': 'total_sales'}, inplace=True)\n",
    "\n",
    "# Calculate total sales for all items\n",
    "grand_total_sales = total_sales_per_item['total_sales'].sum()\n",
    "\n",
    "# Calculate the percentage of sales for each item\n",
    "total_sales_per_item['sales_percentage'] = (total_sales_per_item['total_sales'] / grand_total_sales) * 100\n",
    "\n",
    "# Merge the percentage information with the average daily sales\n",
    "average_daily_sales = average_daily_sales.merge(total_sales_per_item[['item_nbr', 'sales_percentage']], on='item_nbr')\n",
    "\n",
    "# Filter items that are sold for fewer than 10 days on average\n",
    "items_sold_fewer_than_10_days = average_daily_sales[average_daily_sales['days_with_sales'] < 10]\n",
    "\n",
    "# Calculate the total number of items\n",
    "total_items = average_daily_sales.shape[0]\n",
    "\n",
    "# Calculate the number of items sold for fewer than 10 days\n",
    "items_sold_fewer_than_10_days_count = items_sold_fewer_than_10_days.shape[0]\n",
    "\n",
    "# Compute the percentage\n",
    "percentage_sold_fewer_than_10_days = (items_sold_fewer_than_10_days_count / total_items) * 100\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total percentage of items sold for fewer than 10 days on average: {percentage_sold_fewer_than_10_days:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame setup (Replace this with loading your actual DataFrame)\n",
    "# df_full = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Ensure 'date' column is datetime type\n",
    "df_full['date'] = pd.to_datetime(df_full['date'])\n",
    "\n",
    "# Filter out rows where 'unit_sales' is zero or NaN\n",
    "df_positive_sales = df_full[df_full['unit_sales'] > 0]\n",
    "\n",
    "# Group by store and item, then count unique dates with sales > 0\n",
    "daily_sales_counts = df_positive_sales.groupby(['store_nbr', 'item_nbr'])['date'].nunique().reset_index()\n",
    "daily_sales_counts.rename(columns={'date': 'days_with_sales'}, inplace=True)\n",
    "\n",
    "# Calculate the average number of days with sales > 0 per item\n",
    "average_daily_sales = daily_sales_counts.groupby(['item_nbr'])['days_with_sales'].mean().reset_index()\n",
    "\n",
    "# Calculate total sales for each item across all stores and dates\n",
    "total_sales_per_item = df_full.groupby('item_nbr')['unit_sales'].sum().reset_index()\n",
    "total_sales_per_item.rename(columns={'unit_sales': 'total_sales'}, inplace=True)\n",
    "\n",
    "# Calculate total sales for all items\n",
    "grand_total_sales = total_sales_per_item['total_sales'].sum()\n",
    "\n",
    "# Calculate the percentage of sales for each item\n",
    "total_sales_per_item['sales_percentage'] = (total_sales_per_item['total_sales'] / grand_total_sales) * 100\n",
    "\n",
    "# Merge the percentage information with the average daily sales\n",
    "average_daily_sales = average_daily_sales.merge(total_sales_per_item[['item_nbr', 'sales_percentage']], on='item_nbr')\n",
    "\n",
    "# Calculate the total number of days in the period\n",
    "total_days = (df_full['date'].max() - df_full['date'].min()).days + 1\n",
    "\n",
    "# Calculate the number of weeks in the period\n",
    "total_weeks = total_days / 7\n",
    "\n",
    "# Determine the number of days with sales data for each item\n",
    "item_sales_per_week = daily_sales_counts.groupby('item_nbr')['days_with_sales'].sum() / total_weeks\n",
    "\n",
    "# Determine if the item is sold at least once a week on average\n",
    "items_sold_at_least_once_a_week = item_sales_per_week[item_sales_per_week >= 1].index\n",
    "\n",
    "# Calculate the total number of items\n",
    "total_items = average_daily_sales.shape[0]\n",
    "\n",
    "# Calculate the number of items sold at least once a week\n",
    "items_sold_at_least_once_a_week_count = len(items_sold_at_least_once_a_week)\n",
    "\n",
    "# Compute the percentage\n",
    "percentage_sold_at_least_once_a_week = (items_sold_at_least_once_a_week_count / total_items) * 100\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total percentage of items sold at least once a week on average: {percentage_sold_at_least_once_a_week:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP1XEh6OeJMgPK4tWBSiOU8",
   "collapsed_sections": [],
   "mount_file_id": "1tas4gpn15avV6RH91pDuPfl4mfnjqven",
   "name": "Copy of 2020 12 13 - EyeOn Supermarket - v1.ipynb",
   "provenance": [
    {
     "file_id": "1tas4gpn15avV6RH91pDuPfl4mfnjqven",
     "timestamp": 1608148229653
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
