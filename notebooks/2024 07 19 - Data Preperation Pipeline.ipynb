{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporacion Favorita - New Superb Forecasting Model - \n",
    "\n",
    "## Data Preperation Pipeline\n",
    "\n",
    "Made by 4B Consultancy (Janne Heuvelmans, Georgi Duev, Alexander Engelage, Sebastiaan de Bruin) - 2024\n",
    "\n",
    "In this data pipeline, the data used for forecasting item unit_sales will be processed and finalized before being imported in the machine learning model.   \n",
    "The following steps are made within this notebook:  \n",
    "\n",
    ">-0. Import Packages \n",
    "     \n",
    ">-1. Load and optimize raw data  \n",
    "    -1.1. Functions - Creation of downcast and normalize functions for initial data load  \n",
    "    -1.2. Functions - Import raw data from local path  \n",
    "    -1.3. Importing raw data  \n",
    "       \n",
    ">-2. Cleaning data (functions)  \n",
    "    -2.1. Return list containing stores with less then 1670 operational days with sales  \n",
    "    -2.2. Return list containing stores with cluster=10 in stores df  \n",
    "    -2.3. Function to exclude stores with less then 1670 sales days and related to cluster 10 \n",
    " \n",
    ">-3. Excluding data based on exploratory data analyses (functions)  \n",
    "    -3.1. Function (partly optional) - Excluding stores based on sales units and on cluster type 10    \n",
    "    -3.2. Function - Exclude holiday event related to the \"Terromoto\" volcano event  \n",
    "\n",
    ">-4. Enriching datasets for further analysis (functions)  \n",
    "    -4.1. Function - Determining holidays per store     \n",
    "    -4.2. Function - Determining a count per type of holiday per store  \n",
    "    -4.3. Function - Constructing a cartesian sales dataset for each store based on the maximum sales daterange  \n",
    "\n",
    ">-5. Constructing final dataset\n",
    "\n",
    "The structure of this notebook was inspired by:\n",
    "https://hamilton.dagworks.io/en/latest/how-tos/use-in-jupyter-notebook/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import altair as alt\n",
    "import vegafusion as vf\n",
    "import sklearn\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and optimize raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Functions - Creation of downcast and normalize functions for initial data load\n",
    "Update formatting of features to optimize memory and standardize column names.  \n",
    "Furthermore, get basic information on loaded data and print back to user.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.1. Optimize memory by:  \n",
    "- a) Remove spaces from column names.    \n",
    "- b) Downcasting objects, integers and floats.  \n",
    "- c) Standardize date columns to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data memory optimization function 1 - Removing spaces from the column names\n",
    "def standardize_column_names(s):\n",
    "    \"\"\"Removes spaces from the column names.\"\"\"\n",
    "\n",
    "    return s.replace(\" \", \"\")\n",
    "\n",
    "# Data memory optimization function 2 - Changing datatypes to smaller ones (downcasting)\n",
    "def optimize_memory(df):\n",
    "    \"\"\"Optimize memory usage of a DataFrame by converting object columns to categorical\n",
    "    and downcasting numeric columns to smaller types.\"\"\"\n",
    "\n",
    "    # Change: Objects to Categorical.\n",
    "    object_cols = df.select_dtypes(include=\"object\").columns\n",
    "    if not object_cols.empty:\n",
    "        print(\"Change: Objects to Categorical\")\n",
    "        df[object_cols] = df[object_cols].astype(\"category\")\n",
    "\n",
    "    # Change: Convert integers to smallest signed or unsigned integer and floats to smallest.\n",
    "    for col in df.select_dtypes(include=[\"int\"]).columns:\n",
    "        if (df[col] >= 0).all():  # Check if all values are non-negative\n",
    "            df[col] = pd.to_numeric(\n",
    "                df[col], downcast=\"unsigned\"\n",
    "            )  # Downcast to unsigned\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"integer\")  # Downcast to signed\n",
    "\n",
    "    # Downcast float columns\n",
    "    for col in df.select_dtypes(include=[\"float\"]).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Data memory optimization function 4 - Transform date-related columns to datetime format.\n",
    "def transform_date_to_datetime(df, i):\n",
    "    \"\"\"Transform date-related columns to datetime format.\"\"\"\n",
    "    if i != 0:\n",
    "        if \"date\" in df.columns:\n",
    "            print(\"Change: Transformed 'date' column to Datetime Dtype\")\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None).dt.floor('D')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2. Return basic information on each dataframe:  \n",
    "- a) Information on the number of observation and features.  \n",
    "- b) Information on the optimized size of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the basic information of the dataframe (number of observations and features, optimized size)\n",
    "def df_basic_info(df, dataframe_name):\n",
    "    print(\n",
    "        f\"The '{dataframe_name}' dataframe contains: {df.shape[0]:,}\".replace(\",\", \".\") + f\" observations and {df.shape[1]} features.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"After optimizing by downcasting and normalizing it has optimized size of    {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Functions - Import raw data from local PATH\n",
    "Create import data function and apply downcast, normalize functions and give basic information function within the importing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_data(i=0):\n",
    "\n",
    "    # Define path.\n",
    "    c_path = \"C:/Users/sebas/OneDrive/Documenten/GitHub/Supermarketcasegroupproject/Group4B/data/raw/\"\n",
    "\n",
    "    # Identify file.\n",
    "    v_file = (\n",
    "        \"history-per-year\",  # 0\n",
    "        \"holidays_events\",  # 1\n",
    "        \"items\",  # 2\n",
    "        \"stores\",  # 3\n",
    "    )\n",
    "\n",
    "    print(f\"\\nReading file {i}\\n\")\n",
    "\n",
    "    # Load data.\n",
    "    df = (\n",
    "        pd.read_parquet(c_path + v_file[i] + \".parquet\")\n",
    "        .rename(columns=standardize_column_names)\n",
    "        .pipe(optimize_memory)\n",
    "        .pipe(transform_date_to_datetime, i)\n",
    "    )\n",
    "\n",
    "    # Return data.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Importing raw data\n",
    "Importing parquet files with importing function (downcasting, normalizing and giving basic information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading file 0\n",
      "\n",
      "\n",
      "Reading file 1\n",
      "\n",
      "Change: Objects to Categorical\n",
      "Change: Transformed 'date' column to Datetime Dtype\n",
      "\n",
      "Reading file 2\n",
      "\n",
      "Change: Objects to Categorical\n",
      "\n",
      "Reading file 3\n",
      "\n",
      "Change: Objects to Categorical\n"
     ]
    }
   ],
   "source": [
    "# Sales History per year\n",
    "df_sales = f_get_data(0)\n",
    "\n",
    "# Holidays\n",
    "df_holidays = f_get_data(1)\n",
    "\n",
    "# Items\n",
    "df_items = f_get_data(2)\n",
    "\n",
    "# Stores\n",
    "df_stores = f_get_data(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning data (functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Prepare and clean df_sales\n",
    "Drop of columns \"id\", \"year\", \"month\", \"day\" and create a date column based on the columns \"year\" , \"month\" and \"day\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_sales by cleaning up df for merging with holidays by dropping unneeded columns\n",
    "def sales_cleaned(df_sales):\n",
    "    df_sales[\"date\"] = pd.to_datetime(df_sales[[\"year\", \"month\", \"day\"]])\n",
    "    df_sales = df_sales.drop(columns=[\"id\", \"year\", \"month\", \"day\"])\n",
    "\n",
    "    return df_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Prepare, clean and rename df_items\n",
    "Renaming columns: \"family\" to \"item_family\" and  \"class\" to \"item_class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_items by cleaning up df by renaming columns for clearity in final df\n",
    "def items_cleaned_renamed(df_items):\n",
    "\n",
    "    df_items = df_items.rename(columns={\"family\": \"item_family\", \"class\": \"item_class\"})\n",
    "\n",
    "    return df_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Prepare, clean and rename df_stores\n",
    "Drop of columns \"state\"  \n",
    "Rename of columns \"city\" to \"store_city\", \"cluster\" to \"store_cluster\" and \"type\" to \"store_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_stores by cleaning up df by dropping unneeded columns and rename columns for clearity in final df\n",
    "def stores_cleaned_renamed(df_stores):\n",
    "\n",
    "    df_stores = df_stores.drop(columns=[\"state\"])\n",
    "\n",
    "    df_stores = df_stores.rename(\n",
    "        columns={\"city\": \"store_city\", \"cluster\": \"store_cluster\", \"type\": \"store_type\"}\n",
    "    )\n",
    "\n",
    "    return df_stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Excluding data based on exploratory data analyses (functions)\n",
    "Excluding sales data based on store sales availability  \n",
    "Excluding holiday events related to the \"Terromoto\" volcano event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Function (partly optional) - Excluding stores based on sales units and on cluster type 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.1. Function (optional) - Return list containing stores with less then 1670 operational days with sales  \n",
    "default parameter: store_exclusion_cutoff_number = 1670 days. Based on Exploratory data analysis, 17 stores do not have 1670 days of date present in the sales dataset and either are new stores are were closed for a significant number of days during the timeframe within the sales dataset. It might be functional to make the model only for stores that had sales for all dates (and not new) as that might influence model behavior. This function gives the flexibility as so the user can choose him/herself the cutoff point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stores_exclude_sales_days(df_sales, store_exclusion_cutoff_number=1670):\n",
    "\n",
    "    # Group the sales data by store and date\n",
    "    df_sales_grouped = (\n",
    "        df_sales.groupby([\"store_nbr\", \"date\"]).agg({\"unit_sales\": \"sum\"}).reset_index()\n",
    "    )\n",
    "\n",
    "    # Count the number of daily sale records per store\n",
    "    store_count = df_sales_grouped[\"store_nbr\"].value_counts()\n",
    "\n",
    "    # Get stores with counts less than the exclusion cutoff\n",
    "    store_count_exclusion = store_count[store_count < store_exclusion_cutoff_number]\n",
    "\n",
    "    # Get the list of store numbers to be excluded\n",
    "    list_excluded_stores_sales_days = store_count_exclusion.index.tolist()\n",
    "    \n",
    "    return list_excluded_stores_sales_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.2. Function - Return list containing stores with cluster=10 in stores df  \n",
    "From our exploratory data analysis we found that cluster 10 had data issues as it was the only cluster that could was assigned to multiple storetypes. Therefore and because these stores are not part of the top 10 in terms of unit sales, we excluded all stores assigned to cluster 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stores_exclude_cluster(df_stores, cluster_number=10):\n",
    "\n",
    "    # Get the list of store numbers that belong to cluster 10\n",
    "\n",
    "    list_stores_cluster_10 = df_stores[df_stores[\"cluster\"] == cluster_number][\n",
    "        \"store_nbr\"\n",
    "    ].tolist()\n",
    "\n",
    "    return list_stores_cluster_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.3. Function - Exclude stores with less then X sales days and stores related to cluster 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sales_cleaned_stores(df_sales, df_stores, store_exclusion_cutoff_number=1670):\n",
    "\n",
    "    # Excluded less then 1670 salesdays\n",
    "    list_excluded_stores_sales_days = stores_exclude_sales_days(\n",
    "        df_sales, store_exclusion_cutoff_number\n",
    "    )\n",
    "\n",
    "    df_sales = df_sales.drop(\n",
    "        df_sales[df_sales[\"store_nbr\"].isin(list_excluded_stores_sales_days)].index\n",
    "    )\n",
    "\n",
    "    # Cluster 10\n",
    "    list_stores_cluster_10 = stores_exclude_cluster(df_stores, cluster_number=10)\n",
    "\n",
    "    df_sales = df_sales.drop(\n",
    "        df_sales[df_sales[\"store_nbr\"].isin(list_stores_cluster_10)].index\n",
    "    )\n",
    "\n",
    "    return df_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Function - Exclude holiday event related to the \"Terromoto\" volcano event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.1. Function - Create dataframe based on df_holidays with only events containing \"Terremoto Manabi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holiday_filter_vulcano_event(df_holidays, event_substring=\"Terremoto Manabi\"):\n",
    "\n",
    "    # Filter the DataFrame where 'description' contains the event_substring\n",
    "    df_vulcano_event_filtered = df_holidays[\n",
    "        df_holidays[\"description\"].str.contains(event_substring)\n",
    "    ]\n",
    "\n",
    "    return df_vulcano_event_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2. Function - Exclude the \"Terremoto Manabi\" from the df_holidays dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_holidays_cleaned(df_holidays):\n",
    "\n",
    "    # Exclude holiday_filter_vulcano_event function to return filtered df\n",
    "    df_vulcano_event_filtered = holiday_filter_vulcano_event(df_holidays)\n",
    "\n",
    "    # Filter the specific holiday events from the holiday DataFrame\n",
    "    df_holidays = df_holidays.loc[\n",
    "        ~df_holidays.index.isin(df_vulcano_event_filtered.index)\n",
    "    ]\n",
    "\n",
    "    return df_holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enriching datasets for further analysis (functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Function - Determining holidays per store\n",
    "The holidays dataset contains information on local, regional and national holidays. For each of these types, there is a different key/identifier that corresponds with the stores data found in df_stores (the raw data). To overcome this issue, three separate dataframes are made for each type of holiday where the data is merged (joined) with the stores dataframe. Thereafter, these dataframes are combined as to construct one big dataframe containing all the holidays per store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.1. Function - Make cleaned versions of the holidays and stores dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_holiday and df_stores by cleaning up df for merging with holidays by dropping unneeded columns\n",
    "def clean_holidays_stores_prep(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned = df_holidays.drop(\n",
    "        columns=[\n",
    "            \"description\",\n",
    "            \"transferred\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_stores_cleaned = df_stores.drop(columns=[\"cluster\", \"type\"])\n",
    "\n",
    "    return df_holidays_cleaned, df_stores_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.2. Function - Create a dataframe with all the local holidays per store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_local(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # select locale 'Local' from holiday df and merge with city stores df\n",
    "    df_holidays_local = df_holidays_cleaned[df_holidays_cleaned[\"locale\"] == \"Local\"]\n",
    "\n",
    "    df_holidays_prep_local = df_holidays_local.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"city\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.3. Function - Create a dataframe with all the regional holidays per store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_regional(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # select locale 'Regional' from holiday df and merge with state stores df\n",
    "    df_holidays_regional = df_holidays_cleaned[\n",
    "        df_holidays_cleaned[\"locale\"] == \"Regional\"\n",
    "    ]\n",
    "\n",
    "    df_holidays_prep_regional = df_holidays_regional.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"state\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_regional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.4. Function - Create a dataframe with all the national holidays per store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_national(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # Select locale 'Regional' from holiday df and merge with national stores df\n",
    "    df_holidays_national = df_holidays_cleaned[\n",
    "        df_holidays_cleaned[\"locale\"] == \"National\"\n",
    "    ]\n",
    "\n",
    "    # Create extra column for merge on \"Ecuador\"\n",
    "    df_stores_cleaned[\"national_merge\"] = \"Ecuador\"\n",
    "\n",
    "    df_holidays_prep_national = df_holidays_national.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"national_merge\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Drop newly created column national_merge, not needed further\n",
    "    df_holidays_prep_national = df_holidays_prep_national.drop(\n",
    "        columns=[\"national_merge\"]\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_national"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.5. Function - Create a dataframe that merges all the separate dataframe for each type of holiday and store combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_merged(df_holidays, df_stores):\n",
    "\n",
    "    # Load prep functions from local, Regional and National df's\n",
    "    df_holidays_prep_local = holidays_prep_local(df_holidays, df_stores)\n",
    "\n",
    "    df_holidays_prep_regional = holidays_prep_regional(df_holidays, df_stores)\n",
    "\n",
    "    df_holidays_prep_national = holidays_prep_national(df_holidays, df_stores)\n",
    "\n",
    "    # Combine local, regional and national dataframes into 1 merged dataframe\n",
    "    df_holidays_merged = pd.concat(\n",
    "        [df_holidays_prep_local, df_holidays_prep_regional, df_holidays_prep_national]\n",
    "    )\n",
    "\n",
    "    # Clean df_holidays_merged by dropping locale_name\", \"city\", \"state\"\n",
    "    df_holidays_merged = df_holidays_merged.drop(\n",
    "        columns=[\"locale_name\", \"city\", \"state\"]\n",
    "    )\n",
    "\n",
    "    # Rename 'type' of holiday to 'holiday_type'\n",
    "    df_holidays_merged = df_holidays_merged.rename(\n",
    "        columns={\"type\": \"holiday_type\", \"locale\": \"holiday_locale\"}\n",
    "    )\n",
    "\n",
    "    return df_holidays_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Function - Determining a count per type of holiday per store\n",
    "The dataframe resulting from the function described in 4.1. gives duplicate values because there sometimes are multiple holidays on one date. Duplicate values per date would result in multiple sales rows for each date, making it not workable. Therfore, we transform the holiday and stores combination to contain 3 columns (for each type of holiday, namely, local, regional and national) that count the amount of holidays found for a specific date. Thereby we create a unique list of date and store combinations for all the holidays within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1. Function - Creating unique combination of store and date with three count columns for each type of holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_merged_grouped(df_holidays, df_stores):\n",
    "\n",
    "    # Merge the holiday dataframes and clean the merged dataframe\n",
    "    df_holidays_merged = holidays_prep_merged(df_holidays, df_stores)\n",
    "\n",
    "    # Group by date and store_nbr and count the number of holidays per date per store\n",
    "    df_holidays_merged_grouped = df_holidays_merged.pivot_table(\n",
    "        index=['date', 'store_nbr'],\n",
    "        columns='holiday_locale',\n",
    "        values='holiday_type', \n",
    "        aggfunc='count'\n",
    "    ).reset_index()\n",
    "\n",
    "    # The nature of the pivot function causes it to append date and store_nbrs for all possible combinations and thus not only the date and store combinations that we originally had in our data, we will conduct an inner join with the original data to get the original date and store_nbr combinations back\n",
    "\n",
    "    # Remove the name of the columns\n",
    "    df_holidays_merged_grouped.columns.name = None\n",
    "\n",
    "    # Rename the columns to countoflocalholidays, countofregionalholidays, countofnationalholidays\n",
    "    df_holidays_merged_grouped = df_holidays_merged_grouped.rename(columns={\n",
    "        'Local': 'countoflocalholidays', \n",
    "        'Regional': 'countofregionalholidays', \n",
    "        'National': 'countofnationalholidays'\n",
    "    })\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    df_holidays_merged_grouped = df_holidays_merged_grouped.fillna(0)\n",
    "\n",
    "    # Let's do an inner join with the original data to get the original date and store_nbr combinations back. Therefore we need to make another dataframe.\n",
    "    df_holidays_merged_grouped_inner = holidays_prep_merged(df_holidays, df_stores)\n",
    "    df_holidays_merged_grouped_inner = df_holidays_merged_grouped_inner.groupby(['date', 'store_nbr']).size().reset_index().drop(columns=0)\n",
    "\n",
    "    df_holidays_merged_grouped = df_holidays_merged_grouped.merge(df_holidays_merged_grouped_inner, on=['date', 'store_nbr'], how='inner')\n",
    "\n",
    "    # Convert the count columns to integer\n",
    "    df_holidays_merged_grouped = df_holidays_merged_grouped.astype({'countoflocalholidays': int, 'countofregionalholidays': int, 'countofnationalholidays': int})\n",
    "\n",
    "    print(f'In the orignal unioned holiday dataframe, df_holidays_merged we found (including duplicates) {df_holidays_merged.shape[0]} rows')\n",
    "    print(f'In our new adjusted dataframe we have {df_holidays_merged_grouped.shape[0]} rows')\n",
    "    print(f'Thus, we have removed {df_holidays_merged.shape[0] - df_holidays_merged_grouped.shape[0]} rows')\n",
    "\n",
    "    # Might want to filter out the holiday dates that will never be in de salesdate range. However, they will be left out anyway when joining with the sales data.\n",
    "    return df_holidays_merged_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the orignal unioned holiday dataframe, df_holidays_merged we found (including duplicates) 9950 rows\n",
      "In our new adjusted dataframe we have 9601 rows\n",
      "Thus, we have removed 349 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_7372\\3396661543.py:7: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  df_holidays_merged_grouped = df_holidays_merged.pivot_table(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(174, 5)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixdf = holidays_prep_merged_grouped(df_holidays, df_stores)\n",
    "fixdf = fixdf[fixdf['store_nbr'] == 52]\n",
    "\n",
    "fixdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_merged_grouped1(df_holidays, df_stores):\n",
    "\n",
    "    # Merge the holiday dataframes and clean the merged dataframe\n",
    "    df_holidays_merged = holidays_prep_merged(df_holidays, df_stores)\n",
    "\n",
    "    # Group by date and store_nbr and count the number of holidays per date per store\n",
    "    df_holidays_merged_grouped = df_holidays_merged.pivot_table(\n",
    "        index=['date', 'store_nbr'],\n",
    "        columns='holiday_locale',\n",
    "        values='holiday_type', \n",
    "        aggfunc='count',\n",
    "        observed=True\n",
    "    ).reset_index()\n",
    "\n",
    "    # The nature of the pivot function causes it to append date and store_nbrs for all possible combinations and thus not only the date and store combinations that we originally had in our data, we will conduct an inner join with the original data to get the original date and store_nbr combinations back\n",
    "\n",
    "    # Remove the name of the columns\n",
    "    df_holidays_merged_grouped.columns.name = None\n",
    "\n",
    "    # Rename the columns to countoflocalholidays, countofregionalholidays, countofnationalholidays\n",
    "    df_holidays_merged_grouped = df_holidays_merged_grouped.rename(columns={\n",
    "        'Local': 'countoflocalholidays', \n",
    "        'Regional': 'countofregionalholidays', \n",
    "        'National': 'countofnationalholidays'\n",
    "    })\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    df_holidays_merged_grouped = df_holidays_merged_grouped.fillna(0)\n",
    "\n",
    "    # Let's do an inner join with the original data to get the original date and store_nbr combinations back. Therefore we need to make another dataframe.\n",
    "    # df_holidays_merged_grouped_inner = holidays_prep_merged(df_holidays, df_stores)\n",
    "    # df_holidays_merged_grouped_inner = df_holidays_merged_grouped_inner.groupby(['date', 'store_nbr']).size().reset_index().drop(columns=0)\n",
    "    # df_holidays_merged_grouped = df_holidays_merged_grouped.merge(df_holidays_merged_grouped_inner, on=['date', 'store_nbr'], how='inner')\n",
    "\n",
    "    # Convert the count columns to integer\n",
    "    df_holidays_merged_grouped = df_holidays_merged_grouped.astype({'countoflocalholidays': int, 'countofregionalholidays': int, 'countofnationalholidays': int})\n",
    "\n",
    "    print(f'In the orignal unioned holiday dataframe, df_holidays_merged we found (including duplicates) {df_holidays_merged.shape[0]} rows')\n",
    "    print(f'In our new adjusted dataframe we have {df_holidays_merged_grouped.shape[0]} rows')\n",
    "    print(f'Thus, we have removed {df_holidays_merged.shape[0] - df_holidays_merged_grouped.shape[0]} rows')\n",
    "\n",
    "    # Might want to filter out the holiday dates that will never be in de salesdate range. However, they will be left out anyway when joining with the sales data.\n",
    "    return df_holidays_merged_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the orignal unioned holiday dataframe, df_holidays_merged we found (including duplicates) 9950 rows\n",
      "In our new adjusted dataframe we have 9601 rows\n",
      "Thus, we have removed 349 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(174, 5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixdf1 = holidays_prep_merged_grouped1(df_holidays, df_stores)\n",
    "\n",
    "fixdf1 = fixdf1[fixdf1['store_nbr'] == 52]\n",
    "\n",
    "fixdf1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2. Function - Filling in NA values for each count column whenever no holiday could be found for a specific holiday date and store combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fill newly created NaN columns, due to holiday join, with 'no' on thates where there are now holidays\n",
    "def holidays_fill_zero_normal(df,df_holidays_merged_grouped):\n",
    "    \"\"\"\n",
    "    Fills the NaN values with 0 for all columns with type \"int32\" in the combined dataframe.\n",
    "    It will only fill the columns that are in the original dataframe and not in the holiday dataframe.\n",
    "    \"\"\" \n",
    "    columns_to_fill = df_holidays_merged_grouped.columns.intersection(df.columns)\n",
    "    int32_columns_to_fill = [col for col in columns_to_fill if df_holidays_merged_grouped[col].dtype == 'int32']\n",
    "    df[int32_columns_to_fill] = df[int32_columns_to_fill].fillna(0).astype('int32')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Function - Constructing a cartesian sales dataset for each store based on the maximum sales daterange\n",
    "The df_sales dataset contains unit sales data for each store but not all stores have data for each date. To overcome this and make sure each date is present for each store we construct a new dataframe based on the minimum- and maximum date found within the sales dataframe. The result is thus a sales dataframe with each date, store and item combination for the whole timerange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filling_dates_cartesian(df_sales):\n",
    "\n",
    "    # Create new df to include all daily dates in the range, filling missing dates with NaNs\n",
    "    df = df_sales.copy()\n",
    "\n",
    "    # Print first and last date of df\n",
    "    print(f'First date in df: {df[\"date\"].min()}')\n",
    "    print(f'Last date in df:  {df[\"date\"].max()}')\n",
    "\n",
    "    # Calculate memory size and shape size of start df\n",
    "    df_mem_start = sys.getsizeof(df)\n",
    "    df_shape_start = df.shape[0] / 1e6\n",
    "    print(\n",
    "        f\"Start size of df_sales:     {round(df_mem_start/1024/1024/1024, 2)} GB and start observations:     {round(df_shape_start, 1)} million.\"\n",
    "    )\n",
    "\n",
    "    # Create a complete date range for the entire dataset, it's a datetimeindex object \n",
    "    all_dates = pd.date_range(start=df[\"date\"].min(), end=df[\"date\"].max(), freq=\"D\")\n",
    "\n",
    "    # Create a multi-index from all possible combinations of 'item_nbr' and 'date'\n",
    "    all_combinations = pd.MultiIndex.from_product(\n",
    "        [df[\"store_nbr\"].unique(), df[\"item_nbr\"].unique(), all_dates],\n",
    "        names=[\"store_nbr\", \"item_nbr\", \"date\"],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f'The multi-index (all_combinations of store, date and item for the minimum and maximum dates found result in {round(all_combinations.shape[0]/1e6,1)} million rows, this is the amount of rows we expect in the final dataframe.'\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "    # Check for duplicates in the combination of 'store_nbr', 'item_nbr', and 'date'\n",
    "    # This method is based on boolean indexing, when there's a true value for the duplicated method, it will return those rows to the duplicate_rows variable\n",
    "    duplicate_rows = df[\n",
    "        df.duplicated(subset=[\"store_nbr\", \"item_nbr\", \"date\"], keep=False)\n",
    "    ]\n",
    "    if not duplicate_rows.empty:\n",
    "        print(\n",
    "            \"Warning: Duplicate entries found in the combination of 'store_nbr', 'item_nbr', and 'date'.\"\n",
    "        )\n",
    "        print(f\"Total dublicate rows {duplicate_rows.shape[0]}\")\n",
    "        print(\"-\" * 71)\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Reindex the original DataFrame to include all combinations of 'store_nbr', 'item_nbr', and 'date'\n",
    "    df_reindexed = df.set_index([\"store_nbr\", \"item_nbr\", \"date\"]).reindex(\n",
    "        all_combinations\n",
    "    )\n",
    "\n",
    "    # Reset the index to turn the multi-index back into regular columns\n",
    "    df_sales_cartesian = df_reindexed.reset_index()\n",
    "\n",
    "    # Calculate memory size and shape size of final end df\n",
    "    df_mem_end = sys.getsizeof(df_sales_cartesian)\n",
    "    df_mem_change_perc = ((df_mem_end - df_mem_start) / df_mem_start) * 100\n",
    "    df_mem_change = df_mem_end - df_mem_start\n",
    "\n",
    "    df_shape_end = df_sales_cartesian.shape[0] / 1e6\n",
    "    df_shape_change_perc = ((df_shape_end - df_shape_start) / df_shape_start) * 100\n",
    "    df_shape_change = df_shape_end - df_shape_start\n",
    "\n",
    "    print(\n",
    "        f\"Final size of the dataframe is:     {round(df_mem_end/1024/1024/1024, 2)} GB and end observations:       {round(df_shape_end, 1)} million.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Change in size of the dataframe is: {round(df_mem_change_perc, 2)} % and observations:           {round(df_shape_change_perc, 2)}     %.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Increased size of the dataframe is: {round(df_mem_change/1024/1024/1024, 2)} GB and increased observations: {round(df_shape_change, 1)} million.\"\n",
    "    )\n",
    "\n",
    "    return df_sales_cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date in df: 2013-01-01 00:00:00\n",
      "Last date in df:  2017-08-15 00:00:00\n",
      "Start size of df_sales:     3.04 GB and start observations:     125.5 million.\n",
      "The multi-index (all_combinations of store, date and item for the minimum and maximum dates found result in 367.9 million rows, this is the amount of rows we expect in the final dataframe.\n",
      "Final size of the dataframe is:     12.68 GB and end observations:       367.9 million.\n",
      "Change in size of the dataframe is: 317.17 % and observations:           193.15     %.\n",
      "Increased size of the dataframe is: 9.64 GB and increased observations: 242.4 million.\n"
     ]
    }
   ],
   "source": [
    "df_cartesiantest = filling_dates_cartesian(df_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125497040 entries, 0 to 125497039\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Dtype         \n",
      "---  ------       -----         \n",
      " 0   id           uint32        \n",
      " 1   store_nbr    uint8         \n",
      " 2   item_nbr     uint32        \n",
      " 3   unit_sales   float32       \n",
      " 4   onpromotion  boolean       \n",
      " 5   day          uint8         \n",
      " 6   year         category      \n",
      " 7   month        category      \n",
      " 8   date         datetime64[ns]\n",
      "dtypes: boolean(1), category(2), datetime64[ns](1), float32(1), uint32(2), uint8(2)\n",
      "memory usage: 3.0 GB\n"
     ]
    }
   ],
   "source": [
    "df_sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Constructing final dataset\n",
    "In this step all the datasets will be merged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "def merge_datasets(df_sales, df_items, df_stores, df_holidays):\n",
    "    \n",
    "    # Basic information of loaded data\n",
    "    print(\"Step 1 - Importing, downcasting and normalizing data and optimizing memory, the following data has been imported.\")\n",
    "    df_basic_info(df_sales, \"df_sales\")\n",
    "    print(\"\"\"\"\"\")\n",
    "    df_basic_info(df_items, \"df_items\")\n",
    "    print(\"\"\"\"\"\")\n",
    "    df_basic_info(df_stores, \"df_stores\")\n",
    "    print(\"\"\"\"\"\")\n",
    "    df_basic_info(df_holidays, \"df_holidays\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Sales prep   \n",
    "    print(\"Step 2 - Cleaning sales data and making a cartesian product of the sales data and the minimum and maximum dates found in the data.\")\n",
    "    df_sales = sales_cleaned(df_sales)\n",
    "    df_sales = df_sales_cleaned_stores(df_sales, df_stores)\n",
    "    df_sales_cartesian = filling_dates_cartesian(df_sales)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Holidays prep\n",
    "    print(\"Step 3 - Cleaning holiday data and counting the number of holidays per date per store for each type of holiday (national, regional, local).\")\n",
    "    df_holidays = df_holidays_cleaned(df_holidays)\n",
    "    df_holidays_merged_grouped = holidays_prep_merged_grouped(df_holidays, df_stores)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Stores prep\n",
    "    print(\"Step 4 - Cleaning stores data (read: dropping unnecessary columns and renaming columns for clarity).\")    \n",
    "    df_stores = stores_cleaned_renamed(df_stores)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Items prep\n",
    "    print(\"Step 5 - Cleaning items data (read: dropping unnecessary columns and renaming columns for clarity).\")  \n",
    "    df_items = items_cleaned_renamed(df_items)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Holidays merge on sales\n",
    "    print(\"Step 6 - Adding holiday data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of holiday columns.\")  \n",
    "    df_merged = df_sales_cartesian.merge(df_holidays_merged_grouped, on=[\"date\", \"store_nbr\"], how=\"left\")\n",
    "    df_merged = holidays_fill_zero_normal(df_merged,df_holidays_merged_grouped)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Stores merged with sales+holidays\n",
    "    print(\"Step 7 - Adding holiday data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of holiday columns.\")      \n",
    "    df_merged = df_merged.merge(df_stores, on=\"store_nbr\", how=\"left\")\n",
    "    print(\"-\" * 100)\n",
    "  \n",
    "    # Change the dtype for item_nbr from uint32 to int32, during testing we found that the merge was not working properly with uint32\n",
    "    df_merged[\"item_nbr\"] = df_merged[\"item_nbr\"].astype(int)\n",
    "    df_items[\"item_nbr\"] = df_items[\"item_nbr\"].astype(int)\n",
    " \n",
    "    # Items merged with sales+holidays+stores\n",
    "    print(\"Step 8 - Adding items data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of holiday columns. Remember, in our last step we added a lot of store information as well\")   \n",
    "    df_final = df_merged.merge(df_items, on=\"item_nbr\", how=\"left\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Print some referential integrity checks to make sure we have the same amount of rows\n",
    "    print(f'The amount of rows in the sales dataframe was {df_sales.shape[0] /1_000_000:.2f} million.')\n",
    "    print(f'After making a cartesian product with date, store and item we had a total of {df_sales_cartesian.shape[0]/1_000_000:.2f} million rows.')\n",
    "    print(f'After mergin with the holidays, stores, and items we have {df_final.shape[0]/1_000_000:.2f} million rows')\n",
    "    print(f'The difference between the incoming and outgoing data from this function is {(df_final.shape[0] - df_sales.shape[0])/1_000_000:.2f} rows')\n",
    "    print(f'If we compare the outgoing dataframe called \"df_final\" with the cartesian product of sales data and dates we see that the difference is {df_sales_cartesian.shape[0] - df_final.shape[0]} rows')\n",
    "    print(f'If the difference is 0, we have a perfect match and we can continue with the next steps.')\n",
    "\n",
    "# f\"Final size of the dataframe is:     {round(df_mem_end/1024/1024/1024, 2)} GB and end observations:       {round(df_shape_end, 1)} million.\"\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Importing, downcasting and normalizing data and optimizing memory, the following data has been imported.\n",
      "The 'df_sales' dataframe contains: 125.497.040 observations and 8 features.\n",
      "After optimizing by downcasting and normalizing it has optimized size of    2.1 GB.\n",
      "\n",
      "The 'df_items' dataframe contains: 4.100 observations and 4 features.\n",
      "After optimizing by downcasting and normalizing it has optimized size of    0.0 GB.\n",
      "\n",
      "The 'df_stores' dataframe contains: 54 observations and 5 features.\n",
      "After optimizing by downcasting and normalizing it has optimized size of    0.0 GB.\n",
      "\n",
      "The 'df_holidays' dataframe contains: 350 observations and 6 features.\n",
      "After optimizing by downcasting and normalizing it has optimized size of    0.0 GB.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 2 - Cleaning sales data and making a cartesian product of the sales data and the minimum and maximum dates found in the data.\n",
      "First date in df: 2013-01-02 00:00:00\n",
      "Last date in df:  2017-08-15 00:00:00\n",
      "Start size of df_sales:     2.35 GB and start observations:     93.3 million.\n",
      "The multi-index (all_combinations of store, date and item for the minimum and maximum dates found result in 245.1 million rows, this is the amount of rows we expect in the final dataframe.\n",
      "Final size of the dataframe is:     4.34 GB and end observations:       245.1 million.\n",
      "Change in size of the dataframe is: 84.74 % and observations:           162.52     %.\n",
      "Increased size of the dataframe is: 1.99 GB and increased observations: 151.7 million.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 3 - Cleaning holiday data and counting the number of holidays per date per store for each type of holiday (national, regional, local).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_7372\\3396661543.py:7: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  df_holidays_merged_grouped = df_holidays_merged.pivot_table(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the orignal unioned holiday dataframe, df_holidays_merged we found (including duplicates) 8276 rows\n",
      "In our new adjusted dataframe we have 8091 rows\n",
      "Thus, we have removed 185 rows\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 4 - Cleaning stores data (read: dropping unnecessary columns and renaming columns for clarity).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 5 - Cleaning items data (read: dropping unnecessary columns and renaming columns for clarity).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 6 - Adding holiday data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of holiday columns.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 7 - Adding holiday data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of holiday columns.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step 8 - Adding items data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of holiday columns. Remember, in our last step we added a lot of store information as well\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The amount of rows in the sales dataframe was 93.35 million.\n",
      "After making a cartesian product with date, store and item we had a total of 245.05 million rows.\n",
      "After mergin with the holidays, stores, and items we have 245.05 million rows\n",
      "The difference between the incoming and outgoing data from this function is -151707400 rows\n",
      "If we compare the outgoing dataframe called \"df_final\" with the cartesian product of sales data and dates we see that the difference is 0 rows\n",
      "If the difference is 0, we have a perfect match and we can continue with the next steps.\n"
     ]
    }
   ],
   "source": [
    "df_final = merge_datasets(df_sales, df_items, df_stores, df_holidays)  # --> 2.44 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245053620"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.shape[0]\n",
    "df_final.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan code -\n",
    "# What about the \"onpromotion\" column, seems that it has a lot of NaN values. Are these quality issues or is just that there's no promotion. \n",
    "# This issue didn't arrive after merging, it was there from the beginning (in the df_sales dataframe).\n",
    "# You would expect that if there's no promotion going on the value to be \"False\"\n",
    "df_sales1 = sales_cleaned(df_sales)\n",
    "\n",
    "df_sales1_unique = df_sales1['onpromotion'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.X Brainstorm ideas for imputing missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.1 Create a date for all missing values/dates and keep the value of sales as NA\n",
    "\n",
    "Action: Create all daily dates in the date range. Date range starts from first available date in df to last available date in df. Then filling missing dates with NaNs for per unique item per unique store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshooting of filling_dates_NaN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 245053620 entries, 0 to 245053619\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Dtype         \n",
      "---  ------                   -----         \n",
      " 0   store_nbr                uint8         \n",
      " 1   item_nbr                 int32         \n",
      " 2   date                     datetime64[ns]\n",
      " 3   unit_sales               float32       \n",
      " 4   onpromotion              boolean       \n",
      " 5   countoflocalholidays     int32         \n",
      " 6   countofnationalholidays  int32         \n",
      " 7   countofregionalholidays  int32         \n",
      " 8   store_city               category      \n",
      " 9   store_type               category      \n",
      " 10  store_cluster            uint8         \n",
      " 11  item_family              category      \n",
      " 12  item_class               uint16        \n",
      " 13  perishable               uint8         \n",
      "dtypes: boolean(1), category(3), datetime64[ns](1), float32(1), int32(4), uint16(1), uint8(3)\n",
      "memory usage: 8.7 GB\n",
      "Column 'store_nbr' has 0 null values.\n",
      "Column 'item_nbr' has 0 null values.\n",
      "Column 'date' has 0 null values.\n",
      "Column 'unit_sales' has 151707400 null values.\n",
      "Column 'onpromotion' has 169079771 null values.\n",
      "Column 'countoflocalholidays' has 0 null values.\n",
      "Column 'countofnationalholidays' has 0 null values.\n",
      "Column 'countofregionalholidays' has 0 null values.\n",
      "Column 'store_city' has 0 null values.\n",
      "Column 'store_type' has 0 null values.\n",
      "Column 'store_cluster' has 0 null values.\n",
      "Column 'item_family' has 0 null values.\n",
      "Column 'item_class' has 0 null values.\n",
      "Column 'perishable' has 0 null values.\n"
     ]
    }
   ],
   "source": [
    "df_final.info()\n",
    "# Count nulls per column\n",
    "null_counts = df_final.isnull().sum()\n",
    "\n",
    "# Print results\n",
    "for column, count in null_counts.items():\n",
    "    print(f\"Column '{column}' has {count} null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>date</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>countoflocalholidays</th>\n",
       "      <th>countofnationalholidays</th>\n",
       "      <th>countofregionalholidays</th>\n",
       "      <th>store_city</th>\n",
       "      <th>store_type</th>\n",
       "      <th>store_cluster</th>\n",
       "      <th>item_family</th>\n",
       "      <th>item_class</th>\n",
       "      <th>perishable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238329188</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-06-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329189</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-06-28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329190</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329191</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329192</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329193</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-02</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329194</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329195</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-04</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329196</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329197</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329198</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-07</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329199</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-08</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329200</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-09</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329201</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329202</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329203</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329204</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329205</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329206</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329207</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-16</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329208</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329209</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329210</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329211</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329212</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-21</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329213</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329214</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329215</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329216</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329217</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329218</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329219</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329220</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329221</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-30</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329222</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-07-31</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329223</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329224</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329225</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329226</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329227</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329228</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329229</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329230</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329231</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329232</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329233</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329234</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329235</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329236</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238329237</th>\n",
       "      <td>54</td>\n",
       "      <td>129296</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>El Carmen</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           store_nbr  item_nbr       date  unit_sales  onpromotion  \\\n",
       "238329188         54    129296 2017-06-27         NaN         <NA>   \n",
       "238329189         54    129296 2017-06-28         1.0         True   \n",
       "238329190         54    129296 2017-06-29         3.0         True   \n",
       "238329191         54    129296 2017-06-30         1.0         True   \n",
       "238329192         54    129296 2017-07-01         3.0         True   \n",
       "238329193         54    129296 2017-07-02         3.0         True   \n",
       "238329194         54    129296 2017-07-03         2.0         True   \n",
       "238329195         54    129296 2017-07-04         4.0         True   \n",
       "238329196         54    129296 2017-07-05         1.0         True   \n",
       "238329197         54    129296 2017-07-06         1.0        False   \n",
       "238329198         54    129296 2017-07-07         5.0        False   \n",
       "238329199         54    129296 2017-07-08         4.0        False   \n",
       "238329200         54    129296 2017-07-09         3.0        False   \n",
       "238329201         54    129296 2017-07-10         1.0        False   \n",
       "238329202         54    129296 2017-07-11         1.0        False   \n",
       "238329203         54    129296 2017-07-12         1.0        False   \n",
       "238329204         54    129296 2017-07-13         NaN         <NA>   \n",
       "238329205         54    129296 2017-07-14         2.0        False   \n",
       "238329206         54    129296 2017-07-15         3.0        False   \n",
       "238329207         54    129296 2017-07-16         3.0        False   \n",
       "238329208         54    129296 2017-07-17         1.0        False   \n",
       "238329209         54    129296 2017-07-18         NaN         <NA>   \n",
       "238329210         54    129296 2017-07-19         1.0        False   \n",
       "238329211         54    129296 2017-07-20         4.0        False   \n",
       "238329212         54    129296 2017-07-21         3.0        False   \n",
       "238329213         54    129296 2017-07-22         1.0        False   \n",
       "238329214         54    129296 2017-07-23         3.0        False   \n",
       "238329215         54    129296 2017-07-24         2.0        False   \n",
       "238329216         54    129296 2017-07-25         1.0        False   \n",
       "238329217         54    129296 2017-07-26         3.0        False   \n",
       "238329218         54    129296 2017-07-27         NaN         <NA>   \n",
       "238329219         54    129296 2017-07-28         3.0        False   \n",
       "238329220         54    129296 2017-07-29         1.0        False   \n",
       "238329221         54    129296 2017-07-30         9.0        False   \n",
       "238329222         54    129296 2017-07-31         2.0        False   \n",
       "238329223         54    129296 2017-08-01         4.0        False   \n",
       "238329224         54    129296 2017-08-02         NaN         <NA>   \n",
       "238329225         54    129296 2017-08-03         NaN         <NA>   \n",
       "238329226         54    129296 2017-08-04         NaN         <NA>   \n",
       "238329227         54    129296 2017-08-05         NaN         <NA>   \n",
       "238329228         54    129296 2017-08-06         NaN         <NA>   \n",
       "238329229         54    129296 2017-08-07         NaN         <NA>   \n",
       "238329230         54    129296 2017-08-08         NaN         <NA>   \n",
       "238329231         54    129296 2017-08-09         NaN         <NA>   \n",
       "238329232         54    129296 2017-08-10         NaN         <NA>   \n",
       "238329233         54    129296 2017-08-11         NaN         <NA>   \n",
       "238329234         54    129296 2017-08-12         NaN         <NA>   \n",
       "238329235         54    129296 2017-08-13         NaN         <NA>   \n",
       "238329236         54    129296 2017-08-14         NaN         <NA>   \n",
       "238329237         54    129296 2017-08-15         NaN         <NA>   \n",
       "\n",
       "           countoflocalholidays  countofnationalholidays  \\\n",
       "238329188                     0                        0   \n",
       "238329189                     0                        0   \n",
       "238329190                     0                        0   \n",
       "238329191                     0                        0   \n",
       "238329192                     0                        0   \n",
       "238329193                     0                        0   \n",
       "238329194                     1                        0   \n",
       "238329195                     0                        0   \n",
       "238329196                     0                        0   \n",
       "238329197                     0                        0   \n",
       "238329198                     0                        0   \n",
       "238329199                     0                        0   \n",
       "238329200                     0                        0   \n",
       "238329201                     0                        0   \n",
       "238329202                     0                        0   \n",
       "238329203                     0                        0   \n",
       "238329204                     0                        0   \n",
       "238329205                     0                        0   \n",
       "238329206                     0                        0   \n",
       "238329207                     0                        0   \n",
       "238329208                     0                        0   \n",
       "238329209                     0                        0   \n",
       "238329210                     0                        0   \n",
       "238329211                     0                        0   \n",
       "238329212                     0                        0   \n",
       "238329213                     0                        0   \n",
       "238329214                     0                        0   \n",
       "238329215                     0                        0   \n",
       "238329216                     0                        0   \n",
       "238329217                     0                        0   \n",
       "238329218                     0                        0   \n",
       "238329219                     0                        0   \n",
       "238329220                     0                        0   \n",
       "238329221                     0                        0   \n",
       "238329222                     0                        0   \n",
       "238329223                     0                        0   \n",
       "238329224                     0                        0   \n",
       "238329225                     0                        0   \n",
       "238329226                     0                        0   \n",
       "238329227                     0                        0   \n",
       "238329228                     0                        0   \n",
       "238329229                     0                        0   \n",
       "238329230                     0                        0   \n",
       "238329231                     0                        0   \n",
       "238329232                     0                        1   \n",
       "238329233                     0                        1   \n",
       "238329234                     0                        0   \n",
       "238329235                     0                        0   \n",
       "238329236                     0                        0   \n",
       "238329237                     0                        0   \n",
       "\n",
       "           countofregionalholidays store_city store_type  store_cluster  \\\n",
       "238329188                        0  El Carmen          C              3   \n",
       "238329189                        0  El Carmen          C              3   \n",
       "238329190                        0  El Carmen          C              3   \n",
       "238329191                        0  El Carmen          C              3   \n",
       "238329192                        0  El Carmen          C              3   \n",
       "238329193                        0  El Carmen          C              3   \n",
       "238329194                        0  El Carmen          C              3   \n",
       "238329195                        0  El Carmen          C              3   \n",
       "238329196                        0  El Carmen          C              3   \n",
       "238329197                        0  El Carmen          C              3   \n",
       "238329198                        0  El Carmen          C              3   \n",
       "238329199                        0  El Carmen          C              3   \n",
       "238329200                        0  El Carmen          C              3   \n",
       "238329201                        0  El Carmen          C              3   \n",
       "238329202                        0  El Carmen          C              3   \n",
       "238329203                        0  El Carmen          C              3   \n",
       "238329204                        0  El Carmen          C              3   \n",
       "238329205                        0  El Carmen          C              3   \n",
       "238329206                        0  El Carmen          C              3   \n",
       "238329207                        0  El Carmen          C              3   \n",
       "238329208                        0  El Carmen          C              3   \n",
       "238329209                        0  El Carmen          C              3   \n",
       "238329210                        0  El Carmen          C              3   \n",
       "238329211                        0  El Carmen          C              3   \n",
       "238329212                        0  El Carmen          C              3   \n",
       "238329213                        0  El Carmen          C              3   \n",
       "238329214                        0  El Carmen          C              3   \n",
       "238329215                        0  El Carmen          C              3   \n",
       "238329216                        0  El Carmen          C              3   \n",
       "238329217                        0  El Carmen          C              3   \n",
       "238329218                        0  El Carmen          C              3   \n",
       "238329219                        0  El Carmen          C              3   \n",
       "238329220                        0  El Carmen          C              3   \n",
       "238329221                        0  El Carmen          C              3   \n",
       "238329222                        0  El Carmen          C              3   \n",
       "238329223                        0  El Carmen          C              3   \n",
       "238329224                        0  El Carmen          C              3   \n",
       "238329225                        0  El Carmen          C              3   \n",
       "238329226                        0  El Carmen          C              3   \n",
       "238329227                        0  El Carmen          C              3   \n",
       "238329228                        0  El Carmen          C              3   \n",
       "238329229                        0  El Carmen          C              3   \n",
       "238329230                        0  El Carmen          C              3   \n",
       "238329231                        0  El Carmen          C              3   \n",
       "238329232                        0  El Carmen          C              3   \n",
       "238329233                        0  El Carmen          C              3   \n",
       "238329234                        0  El Carmen          C              3   \n",
       "238329235                        0  El Carmen          C              3   \n",
       "238329236                        0  El Carmen          C              3   \n",
       "238329237                        0  El Carmen          C              3   \n",
       "\n",
       "          item_family  item_class  perishable  \n",
       "238329188   GROCERY I        1032           0  \n",
       "238329189   GROCERY I        1032           0  \n",
       "238329190   GROCERY I        1032           0  \n",
       "238329191   GROCERY I        1032           0  \n",
       "238329192   GROCERY I        1032           0  \n",
       "238329193   GROCERY I        1032           0  \n",
       "238329194   GROCERY I        1032           0  \n",
       "238329195   GROCERY I        1032           0  \n",
       "238329196   GROCERY I        1032           0  \n",
       "238329197   GROCERY I        1032           0  \n",
       "238329198   GROCERY I        1032           0  \n",
       "238329199   GROCERY I        1032           0  \n",
       "238329200   GROCERY I        1032           0  \n",
       "238329201   GROCERY I        1032           0  \n",
       "238329202   GROCERY I        1032           0  \n",
       "238329203   GROCERY I        1032           0  \n",
       "238329204   GROCERY I        1032           0  \n",
       "238329205   GROCERY I        1032           0  \n",
       "238329206   GROCERY I        1032           0  \n",
       "238329207   GROCERY I        1032           0  \n",
       "238329208   GROCERY I        1032           0  \n",
       "238329209   GROCERY I        1032           0  \n",
       "238329210   GROCERY I        1032           0  \n",
       "238329211   GROCERY I        1032           0  \n",
       "238329212   GROCERY I        1032           0  \n",
       "238329213   GROCERY I        1032           0  \n",
       "238329214   GROCERY I        1032           0  \n",
       "238329215   GROCERY I        1032           0  \n",
       "238329216   GROCERY I        1032           0  \n",
       "238329217   GROCERY I        1032           0  \n",
       "238329218   GROCERY I        1032           0  \n",
       "238329219   GROCERY I        1032           0  \n",
       "238329220   GROCERY I        1032           0  \n",
       "238329221   GROCERY I        1032           0  \n",
       "238329222   GROCERY I        1032           0  \n",
       "238329223   GROCERY I        1032           0  \n",
       "238329224   GROCERY I        1032           0  \n",
       "238329225   GROCERY I        1032           0  \n",
       "238329226   GROCERY I        1032           0  \n",
       "238329227   GROCERY I        1032           0  \n",
       "238329228   GROCERY I        1032           0  \n",
       "238329229   GROCERY I        1032           0  \n",
       "238329230   GROCERY I        1032           0  \n",
       "238329231   GROCERY I        1032           0  \n",
       "238329232   GROCERY I        1032           0  \n",
       "238329233   GROCERY I        1032           0  \n",
       "238329234   GROCERY I        1032           0  \n",
       "238329235   GROCERY I        1032           0  \n",
       "238329236   GROCERY I        1032           0  \n",
       "238329237   GROCERY I        1032           0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_store_54_item_129296 = df_final[(df_final[\"store_nbr\"] == 54) & (df_final[\"item_nbr\"] == 129296)]\n",
    "\n",
    "df_final_store_54_item_129296.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is very memory costly!!! --> will result in huge df\n",
    "df_merged_full_nan = filling_dates_NaN(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.2 Fill newly created dates for non-sales columns using forward fill and backward fill --> items, stores, holidays columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for non-sales columns using forward fill and backward fill --> items, stores, holidays\n",
    "non_sales_columns = [\n",
    "    \"store_city\",\n",
    "    \"store_type\",\n",
    "    \"store_cluster\",\n",
    "    \"item_family\",\n",
    "    \"item_class\",\n",
    "    \"perishable\",\n",
    "]\n",
    "\n",
    "# To-do: test more in individual item level how this works\n",
    "df_merged_full_nan[non_sales_columns] = df_merged_full_nan.groupby(\n",
    "    [\"item_nbr\", \"store_nbr\"]\n",
    ")[non_sales_columns].transform(lambda group: group.ffill().bfill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_store_54_item_129296 = df_merged_full_nan[(df_merged_full_nan[\"store_nbr\"] == 54) & (df_merged_full_nan[\"item_nbr\"] == 129296)]\n",
    "\n",
    "df_final_store_54_item_129296.tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2: Detect negative values\n",
    "\n",
    "\tAction: Delete unit_sales if values are lower than zero --> N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sales_cleaned(df):\n",
    "\n",
    "    # Check the number of negative values before replacement\n",
    "    before_replacement = (df[\"unit_sales\"] < 0).sum()\n",
    "    print(f\"Number of negative values before replacement: {before_replacement}\")\n",
    "\n",
    "    # Create a boolean mask for the negative sales rows to create a 'boolean flag-list' containing all negative rows, used to filter full df_sales df\n",
    "    negative_sales_mask = df[\"unit_sales\"] < 0\n",
    "\n",
    "    # Use the mask to update the flagged 'unit_sales' column in the original DataFrame\n",
    "    df.loc[negative_sales_mask, \"unit_sales\"] = df.loc[\n",
    "        negative_sales_mask, \"unit_sales\"\n",
    "    ].where(df.loc[negative_sales_mask, \"unit_sales\"] >= 0, np.nan)\n",
    "\n",
    "    # Check the number of negative values after replacement\n",
    "    after_replacement = (df[\"unit_sales\"] < 0).sum()\n",
    "    print(f\"Number of negative values after replacement: {after_replacement}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full merged df_merged_full\n",
    "\n",
    "# df_sales_nan = negative_sales_cleaned(df_merged_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check function with df_sales\n",
    "df_sales_nan = negative_sales_cleaned(df_sales)\n",
    "\n",
    "df_sales_nan_check = df_sales_nan[df_sales_nan[\"unit_sales\"].isna()].sort_values(\n",
    "    by=[\"date\", \"store_nbr\", \"item_nbr\"]\n",
    ")\n",
    "\n",
    "df_sales_nan_check.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Define new, old and closed stores\n",
    "\n",
    "\tCondition: sales for all items a given store and date are NA\n",
    "\n",
    "\tAction: Impute with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum/Agg all sales group  by store, date\n",
    "# --> Sum_sales > 0 then store_opened\n",
    "# else --> closed --> inputed with 0\n",
    "\n",
    "\n",
    "# TO-do: discuss about closed_store --> inpute with 0 or N/A?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 New product\n",
    "\n",
    "\tBefore the very first sale of an item, all observations are kept as NA\n",
    "\n",
    "\tAfter the very first sale of an item, we go to step 3:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum/Agg all sales group  by item, date\n",
    "# --> Sum_sales > 0 then first_sales_day of product\n",
    "# else <first_sales_day of product --> delete unit_sales --> N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8  Stockout on store level\n",
    "\n",
    "      Perishable good: when there are missing values for two consecutive days for a given item per individual store \n",
    "\n",
    "      Nonperishable goods: when there are missing values for 7 consecutive days for a given item and per individual store\n",
    "\n",
    "      Action: Impute with algorithm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perishable good\n",
    "if ['perishable'] == 1 and item_missing_count > 2 #-->  inpute with 0?\n",
    "\n",
    "if ['perishable'] == 1 and item_missing_count <= 2 #-->  inpute with mean? or intrepolate?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#non-perishable good  \n",
    "if ['perishable'] == 0 and and item_missing_count > 7 #-->  inpute with 0?\n",
    "\n",
    "if ['perishable'] == 1 and item_missing_count <= 7 #-->  inpute with mean? or intrepolate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate between missing datapoints --> sales\n",
    "\n",
    "fillna(method=\"mean\")\n",
    "\n",
    "df[\"column_name\"].interpolate(method=\"linear\", inplace=True)\n",
    "\n",
    "df[\"column_name\"].interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "df[\"column_name\"].interpolate(method=\"polynomial\", order=2, inplace=True)\n",
    "\n",
    "# Interpolate missing values for the 'unit_sales' column\n",
    "df[\"unit_sales\"] = df.groupby([\"store_nbr\", \"item_nbr\"])[\"unit_sales\"].apply(\n",
    "    lambda group: group.interpolate(method=\"linear\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 Missing sales data: Zero sales\n",
    "\n",
    "\tAll other cases\n",
    "\n",
    "\tAction: Impute with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.X Negative values imputing to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 Promotional Data \n",
    "\n",
    "   All missing values are interpreted a day with no promotion\n",
    "\n",
    "   Action: Inpute onpromotion N/A with False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing N/A values in boolean columns with False\n",
    "def sales_fill_onpromotion(df):\n",
    "\n",
    "    df[\"onpromotion\"] = df[\"onpromotion\"].fillna(False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# To-do: when perform this function? Before filling_dates_NaN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged_test = sales_fill_onpromotion(df_merged)\n",
    "\n",
    "# df_merged_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Feature construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.X Extracting datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime_features(df):\n",
    "    \"\"\"\n",
    "    Extracting datetime features\n",
    "    year, month, day of month, weekday (1-7), week number-year, week_year_date\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure the date column is sorted\n",
    "    df = df.sort_values(\"date\")\n",
    "\n",
    "    # df[\"year\"] = df[\"date\"].dt.year\n",
    "    # df[\"month\"] = df[\"date\"].dt.month\n",
    "    # df[\"day\"] = df[\"date\"].dt.day\n",
    "\n",
    "    # Adjusting weekday to start from 1 (Monday) to 7 (Sunday)\n",
    "    df[\"weekday\"] = df[\"date\"].dt.dayofweek + 1\n",
    "\n",
    "    # Adding week number-year feature\n",
    "    df[\"week_number\"] = df[\"date\"].dt.isocalendar().week\n",
    "    df[\"week_year\"] = df[\"week_number\"].astype(str).str.zfill(2) + df[\"year\"].astype(\n",
    "        str\n",
    "    )\n",
    "\n",
    "    # Convert week_year to datetime with monday as startdate of week\n",
    "    df[\"week_year_date\"] = pd.to_datetime(\n",
    "        df[\"year\"].astype(str) + df[\"week_number\"].astype(str).str.zfill(2) + \"1\",\n",
    "        format=\"%Y%W%w\",\n",
    "    )\n",
    "\n",
    "    # Adding trend feature: number of weeks since the start of the dataset\n",
    "    start_date = df[\"date\"].min()\n",
    "    df[\"weeks_since_start\"] = ((df[\"date\"] - start_date).dt.days / 7).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime_features(df):\n",
    "    \"\"\"\n",
    "    Extracting datetime features:\n",
    "    year, month, day of month, weekday (1-7), week number-year, and trend (weeks since start, starting at 1)\n",
    "    \"\"\"\n",
    "    # Ensure the date column is sorted\n",
    "    df = df.copy().sort_values(\"date\")\n",
    "\n",
    "    # Use isocalendar for consistent week-based calculations\n",
    "\n",
    "    iso_calendar = df[\"date\"].dt.isocalendar()\n",
    "\n",
    "    # Year, Month, Day\n",
    "    # df[\"year\"] = iso_calendar.year\n",
    "    # df[\"month\"] = df[\"date\"].dt.month\n",
    "    # df[\"day\"] = df[\"date\"].dt.day\n",
    "\n",
    "    # Weekday (1 = Monday, 7 = Sunday)\n",
    "    df[\"weekday\"] = iso_calendar.day\n",
    "\n",
    "    # Week number\n",
    "    df[\"week_number\"] = iso_calendar.week\n",
    "\n",
    "    # Week-year\n",
    "    df[\"week_year\"] = df[\"week_number\"].astype(str).str.zfill(2) + df[\"year\"].astype(\n",
    "        str\n",
    "    )\n",
    "\n",
    "    # Convert week_year to datetime with monday as startdate of week\n",
    "    df[\"week_year_date\"] = pd.to_datetime(\n",
    "        df[\"year\"].astype(str) + df[\"week_number\"].astype(str).str.zfill(2) + \"1\",\n",
    "        format=\"%Y%W%w\",\n",
    "    )\n",
    "\n",
    "    # First day of the ISO year containing the start date\n",
    "\n",
    "    start_date = df[\"date\"].min()\n",
    "    start_year_first_day = datetime(start_date.year, 1, 1)\n",
    "\n",
    "    # 'search' for first monday of year\n",
    "    while start_year_first_day.isocalendar()[1] != 1:\n",
    "\n",
    "        start_year_first_day = start_year_first_day + pd.Timedelta(days=1)\n",
    "\n",
    "    ##Itemweek number\n",
    "    # Weeks since start (aligned with ISO week numbers)\n",
    "    df[\"weeks_since_start\"] = (\n",
    "        iso_calendar.week + (iso_calendar.year - start_year_first_day.year) * 52\n",
    "    )\n",
    "\n",
    "    # Adjust weeks_since_start to start from 1\n",
    "    df[\"weeks_since_start\"] = (\n",
    "        df[\"weeks_since_start\"] - df[\"weeks_since_start\"].min() + 1\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = extract_datetime_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.X Promotion\n",
    "\n",
    "The number of days a item was on promotion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY FROM OLD NOTEBOOK\n",
    "# TO-DO 1: transform with new df names\n",
    "# TO-DO 2: total promotion days month --> week\n",
    "\n",
    "\n",
    "def onpromotion_month_count(df):\n",
    "\n",
    "    if \"onpromotion\" in df.columns:\n",
    "\n",
    "        df[\"onpromotion_month_count\"] = df.groupby(\n",
    "            [\"item_nbr\", \"store_nbr\", \"day\", \"month\", \"year\"]\n",
    "        )[\"onpromotion\"].transform(\"sum\")\n",
    "\n",
    "        print(\n",
    "            \"Change: 'onpromotion' column transformed to 'onpromotion_month_count' feature.\"\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        print(\"The DataFrame does not contain an 'onpromotion' column.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_agg = (\n",
    "    onpromotion_month_count(df_0)  # Transformation to 'onpromotion_month_count' feature\n",
    "    .drop(\n",
    "        columns=[\"id\", \"date\", \"onpromotion\"]\n",
    "    )  # Drop unnecessary columns \"id\", \"date\", \"onpromotion\"\n",
    "    .groupby([\"month\", \"year\", \"store_nbr\", \"item_nbr\"])\n",
    "    .agg({\"unit_sales\": \"sum\", \"onpromotion_month_count\": \"sum\"})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.X Store closed on 25-12 and 01-01 \n",
    "\n",
    "STore closed in between when inputed with 0\n",
    "\n",
    "--> can we also use this feature to include the excluded stores with >9 days data, due to closing or later openings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_case_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
