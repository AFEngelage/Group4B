{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporacion Favorita - New Superb Forecasting Model - \n",
    "\n",
    "## Data Preperation Pipeline\n",
    "\n",
    "Made by 4B Consultancy (Janne Heuvelmans, Georgi Duev, Alexander Engelage, Sebastiaan de Bruin) - 2024\n",
    "\n",
    "In this data pipeline, the data used for forecasting item unit_sales will be processed and finalized before being imported in the machine learning model.   \n",
    "The following steps are made within this notebook:  \n",
    "\n",
    ">-0. Import Packages \n",
    "     \n",
    ">-1. Load and optimize raw data  \n",
    "    -1.1. Functions - Creation of downcast and normalize functions for initial data load  \n",
    "    -1.2. Functions - Import raw data from local path  \n",
    "    -1.3. Importing raw data  \n",
    "       \n",
    ">-2. Cleaning data (functions)  \n",
    "    -2.1. Return list containing stores with less then 1670 operational days with sales  \n",
    "    -2.2. Return list containing stores with cluster=10 in stores df  \n",
    "    -2.3. Function to exclude stores with less then 1670 sales days and related to cluster 10 \n",
    " \n",
    ">-3. Excluding data based on exploratory data analyses (functions)  \n",
    "    -3.1. Function (partly optional) - Excluding stores based on sales units and on cluster type 10    \n",
    "    -3.2. Function - Exclude holiday event related to the \"Terromoto\" volcano event  \n",
    "\n",
    ">-4. Enriching datasets for further analysis (functions)  \n",
    "    -4.1. Function - Determining holidays per store     \n",
    "    -4.2. Function - Determining a count per type of holiday per store  \n",
    "    -4.3. Function - Constructing a cartesian sales dataset for each store based on the maximum sales daterange  \n",
    "\n",
    ">-5. Constructing final dataset\n",
    "\n",
    "The structure of this notebook was inspired by:\n",
    "https://hamilton.dagworks.io/en/latest/how-tos/use-in-jupyter-notebook/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import os\n",
    "import sys\n",
    "import altair as alt\n",
    "import vegafusion as vf\n",
    "import sklearn\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and optimize raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Functions - Creation of downcast and normalize functions for initial data load\n",
    "Update formatting of features to optimize memory and standardize column names.  \n",
    "Furthermore, get basic information on loaded data and print back to user.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.1. Optimize memory by:  \n",
    "- a) Remove spaces from column names.    \n",
    "- b) Downcasting objects, integers and floats.  \n",
    "- c) Standardize date columns to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data memory optimization function 1 - Removing spaces from the column names\n",
    "def standardize_column_names(s):\n",
    "    \"\"\"Removes spaces from the column names.\"\"\"\n",
    "\n",
    "    return s.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "# Data memory optimization function 2 - Changing datatypes to smaller ones (downcasting)\n",
    "def optimize_memory(df):\n",
    "    \"\"\"Optimize memory usage of a DataFrame by converting object columns to categorical\n",
    "    and downcasting numeric columns to smaller types.\"\"\"\n",
    "\n",
    "    # Change: Objects to Categorical.\n",
    "    object_cols = df.select_dtypes(include=\"object\").columns\n",
    "    if not object_cols.empty:\n",
    "        print(\"Change: Objects to Categorical\")\n",
    "        df[object_cols] = df[object_cols].astype(\"category\")\n",
    "\n",
    "    # Change: Convert integers to smallest signed or unsigned integer and floats to smallest.\n",
    "    for col in df.select_dtypes(include=[\"int\"]).columns:\n",
    "        if (df[col] >= 0).all():  # Check if all values are non-negative\n",
    "            df[col] = pd.to_numeric(\n",
    "                df[col], downcast=\"unsigned\"\n",
    "            )  # Downcast to unsigned\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"integer\")  # Downcast to signed\n",
    "\n",
    "    # Downcast float columns\n",
    "    for col in df.select_dtypes(include=[\"float\"]).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Data memory optimization function 4 - Transform date-related columns to datetime format.\n",
    "def transform_date_to_datetime(df, i):\n",
    "    \"\"\"Transform date-related columns to datetime format.\"\"\"\n",
    "    if i != 0:\n",
    "        if \"date\" in df.columns:\n",
    "            print(\"Change: Transformed 'date' column to Datetime Dtype\")\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None).dt.floor(\"D\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2. Return basic information on each dataframe:  \n",
    "- a) Information on the number of observation and features.  \n",
    "- b) Information on the optimized size of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the basic information of the dataframe (number of observations and features, optimized size)\n",
    "def df_basic_info(df, dataframe_name):\n",
    "    print(\n",
    "        f\"The '{dataframe_name}' dataframe contains: {df.shape[0]:,}\".replace(\",\", \".\")\n",
    "        + f\" observations and {df.shape[1]} features.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"After optimizing by downcasting and normalizing it has optimized size of    {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Functions - Import raw data from local PATH\n",
    "Create import data function and apply downcast, normalize functions and give basic information function within the importing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_data(i=0):\n",
    "\n",
    "    # Define path.\n",
    "    # c_path = \"C:/Users/sebas/OneDrive/Documenten/GitHub/Supermarketcasegroupproject/Group4B/data/raw/\"\n",
    "\n",
    "    c_path = \"C:/Users/alexander/Documents/0. Data Science and AI for Experts/EAISI_4B_Supermarket/data/raw/\"\n",
    "\n",
    "    # c_path = 'https://www.dropbox.com/scl/fo/4f5xcrzfqlyv3qjzm0kgc/AAJkdVC_Wa8NjoTBMwG4gx4?rlkey=gyi9pc4rcmghkzk2wgqyb7y4o&dl=0' Checking if possible to use c_path of dropbox\n",
    "\n",
    "    # Identify file.\n",
    "    v_file = (\n",
    "        \"history-per-year\",  # 0\n",
    "        \"holidays_events\",  # 1\n",
    "        \"items\",  # 2\n",
    "        \"stores\",  # 3\n",
    "    )\n",
    "\n",
    "    print(f\"\\nReading file {i}\\n\")\n",
    "\n",
    "    # Load data.\n",
    "    df = (\n",
    "        pd.read_parquet(c_path + v_file[i] + \".parquet\")\n",
    "        .rename(columns=standardize_column_names)\n",
    "        .pipe(optimize_memory)\n",
    "        .pipe(transform_date_to_datetime, i)\n",
    "    )\n",
    "\n",
    "    # Return data.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Importing raw data\n",
    "Importing parquet files with importing function (downcasting, normalizing and giving basic information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales History per year\n",
    "df_sales = f_get_data(0)\n",
    "\n",
    "# Holidays\n",
    "df_holidays = f_get_data(1)\n",
    "\n",
    "# Items\n",
    "df_items = f_get_data(2)\n",
    "\n",
    "# Stores\n",
    "df_stores = f_get_data(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning data (functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Prepare and clean df_sales\n",
    "Drop of columns \"id\", \"year\", \"month\", \"day\" and create a date column based on the columns \"year\" , \"month\" and \"day\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_sales by cleaning up df for merging with holidays by dropping unneeded columns\n",
    "def sales_cleaned(df_sales):\n",
    "    df_sales[\"date\"] = pd.to_datetime(df_sales[[\"year\", \"month\", \"day\"]])\n",
    "    df_sales = df_sales.drop(columns=[\"id\", \"year\", \"month\", \"day\"])\n",
    "\n",
    "    return df_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Prepare, clean and rename df_items\n",
    "Renaming columns: \"family\" to \"item_family\" and  \"class\" to \"item_class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_items by cleaning up df by renaming columns for clearity in final df\n",
    "def items_cleaned_renamed(df_items):\n",
    "\n",
    "    df_items = df_items.rename(columns={\"family\": \"item_family\", \"class\": \"item_class\"})\n",
    "\n",
    "    return df_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Prepare, clean and rename df_stores\n",
    "Drop of columns \"state\"  \n",
    "Rename of columns \"city\" to \"store_city\", \"cluster\" to \"store_cluster\" and \"type\" to \"store_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_stores by cleaning up df by dropping unneeded columns and rename columns for clearity in final df\n",
    "def stores_cleaned_renamed(df_stores):\n",
    "\n",
    "    df_stores = df_stores.drop(columns=[\"state\", \"city\"])\n",
    "\n",
    "    df_stores = df_stores.rename(\n",
    "        columns={\n",
    "            \"cluster\": \"store_cluster\",\n",
    "            \"type\": \"store_type\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Excluding data based on exploratory data analyses (functions)\n",
    "Excluding sales data based on store sales availability  \n",
    "Excluding holiday events related to the \"Terromoto\" volcano event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Function (partly optional) - Excluding stores based on sales units and on cluster type 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.1. Function (optional) - Return list containing stores with less then 1670 operational days with sales  \n",
    "default parameter: store_exclusion_cutoff_number = 1670 days. Based on Exploratory data analysis, 17 stores do not have 1670 days of date present in the sales dataset and either are new stores are were closed for a significant number of days during the timeframe within the sales dataset. It might be functional to make the model only for stores that had sales for all dates (and not new) as that might influence model behavior. This function gives the flexibility as so the user can choose him/herself the cutoff point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stores_exclude_sales_days(df_sales, store_exclusion_cutoff_number=1670):\n",
    "\n",
    "    # Group the sales data by store and date\n",
    "    df_sales_grouped = (\n",
    "        df_sales.groupby([\"store_nbr\", \"date\"]).agg({\"unit_sales\": \"sum\"}).reset_index()\n",
    "    )\n",
    "\n",
    "    # Count the number of daily sale records per store\n",
    "    store_count = df_sales_grouped[\"store_nbr\"].value_counts()\n",
    "\n",
    "    # Get stores with counts less than the exclusion cutoff\n",
    "    store_count_exclusion = store_count[store_count < store_exclusion_cutoff_number]\n",
    "\n",
    "    # Get the list of store numbers to be excluded\n",
    "    list_excluded_stores_sales_days = store_count_exclusion.index.tolist()\n",
    "\n",
    "    return list_excluded_stores_sales_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.2. Function - Return list containing stores with cluster=10 in stores df  \n",
    "From our exploratory data analysis we found that cluster 10 had data issues as it was the only cluster that could was assigned to multiple storetypes. Therefore and because these stores are not part of the top 10 in terms of unit sales, we excluded all stores assigned to cluster 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stores_exclude_cluster(df_stores, cluster_number=10):\n",
    "\n",
    "    # Get the list of store numbers that belong to cluster 10\n",
    "\n",
    "    list_stores_cluster_10 = df_stores[df_stores[\"cluster\"] == cluster_number][\n",
    "        \"store_nbr\"\n",
    "    ].tolist()\n",
    "\n",
    "    return list_stores_cluster_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.3. Function - Exclude stores with less then X sales days and stores related to cluster 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sales_cleaned_stores(df_sales, df_stores, store_exclusion_cutoff_number=500):\n",
    "\n",
    "    # Excluded less then 1670 salesdays\n",
    "    list_excluded_stores_sales_days = stores_exclude_sales_days(\n",
    "        df_sales, store_exclusion_cutoff_number\n",
    "    )\n",
    "\n",
    "    df_sales = df_sales.drop(\n",
    "        df_sales[df_sales[\"store_nbr\"].isin(list_excluded_stores_sales_days)].index\n",
    "    )\n",
    "\n",
    "    # Cluster 10\n",
    "    list_stores_cluster_10 = stores_exclude_cluster(df_stores, cluster_number=10)\n",
    "\n",
    "    df_sales = df_sales.drop(\n",
    "        df_sales[df_sales[\"store_nbr\"].isin(list_stores_cluster_10)].index\n",
    "    )\n",
    "\n",
    "    return df_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Function - Exclude holiday event related to the \"Terromoto\" volcano event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.1. Function - Create dataframe based on df_holidays with only events containing \"Terremoto Manabi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holiday_filter_vulcano_event(df_holidays, event_substring=\"Terremoto Manabi\"):\n",
    "\n",
    "    # Filter the DataFrame where 'description' contains the event_substring\n",
    "    df_vulcano_event_filtered = df_holidays[\n",
    "        df_holidays[\"description\"].str.contains(event_substring)\n",
    "    ]\n",
    "\n",
    "    return df_vulcano_event_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2. Function - Exclude the \"Terremoto Manabi\" from the df_holidays dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_holidays_cleaned(df_holidays):\n",
    "\n",
    "    # Exclude holiday_filter_vulcano_event function to return filtered df\n",
    "    df_vulcano_event_filtered = holiday_filter_vulcano_event(df_holidays)\n",
    "\n",
    "    # Filter the specific holiday events from the holiday DataFrame\n",
    "    df_holidays = df_holidays.loc[\n",
    "        ~df_holidays.index.isin(df_vulcano_event_filtered.index)\n",
    "    ]\n",
    "\n",
    "    return df_holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enriching datasets for further analysis (functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Function - Determining holidays per store\n",
    "The holidays dataset contains information on local, regional and national holidays. For each of these types, there is a different key/identifier that corresponds with the stores data found in df_stores (the raw data). To overcome this issue, three separate dataframes are made for each type of holiday where the data is merged (joined) with the stores dataframe. Thereafter, these dataframes are combined as to construct one big dataframe containing all the holidays per store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.1. Function - Make cleaned versions of the holidays and stores dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_holiday and df_stores by cleaning up df for merging with holidays by dropping unneeded columns\n",
    "def clean_holidays_stores_prep(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned = df_holidays.drop(\n",
    "        columns=[\n",
    "            \"description\",\n",
    "            \"transferred\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_stores_cleaned = df_stores.drop(columns=[\"cluster\", \"type\"])\n",
    "\n",
    "    return df_holidays_cleaned, df_stores_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.2. Function - Create a dataframe with all the local holidays per store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_local(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # select locale 'Local' from holiday df and merge with city stores df\n",
    "    df_holidays_local = df_holidays_cleaned[df_holidays_cleaned[\"locale\"] == \"Local\"]\n",
    "\n",
    "    df_holidays_prep_local = df_holidays_local.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"city\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.3. Function - Create a dataframe with all the regional holidays per store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_regional(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # select locale 'Regional' from holiday df and merge with state stores df\n",
    "    df_holidays_regional = df_holidays_cleaned[\n",
    "        df_holidays_cleaned[\"locale\"] == \"Regional\"\n",
    "    ]\n",
    "\n",
    "    df_holidays_prep_regional = df_holidays_regional.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"state\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_regional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.4. Function - Create a dataframe with all the national holidays per store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_national(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # Select locale 'Regional' from holiday df and merge with national stores df\n",
    "    df_holidays_national = df_holidays_cleaned[\n",
    "        df_holidays_cleaned[\"locale\"] == \"National\"\n",
    "    ]\n",
    "\n",
    "    # Create extra column for merge on \"Ecuador\"\n",
    "    df_stores_cleaned[\"national_merge\"] = \"Ecuador\"\n",
    "\n",
    "    df_holidays_prep_national = df_holidays_national.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"national_merge\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Drop newly created column national_merge, not needed further\n",
    "    df_holidays_prep_national = df_holidays_prep_national.drop(\n",
    "        columns=[\"national_merge\"]\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_national"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.5. Function - Create a dataframe that merges all the separate dataframe for each type of holiday and store combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_merged(df_holidays, df_stores):\n",
    "\n",
    "    # Load prep functions from local, Regional and National df's\n",
    "    df_holidays_prep_local = holidays_prep_local(df_holidays, df_stores)\n",
    "\n",
    "    df_holidays_prep_regional = holidays_prep_regional(df_holidays, df_stores)\n",
    "\n",
    "    df_holidays_prep_national = holidays_prep_national(df_holidays, df_stores)\n",
    "\n",
    "    # Combine local, regional and national dataframes into 1 merged dataframe\n",
    "    df_holidays_merged = pd.concat(\n",
    "        [df_holidays_prep_local, df_holidays_prep_regional, df_holidays_prep_national]\n",
    "    )\n",
    "\n",
    "    # Clean df_holidays_merged by dropping \"locale_name\", \"city\", \"state\"\n",
    "    df_holidays_merged = df_holidays_merged.drop(\n",
    "        columns=[\"locale_name\", \"city\", \"state\"]\n",
    "    )\n",
    "\n",
    "    # Rename 'type' of holiday to 'holiday_type'\n",
    "    df_holidays_merged = df_holidays_merged.rename(\n",
    "        columns={\"type\": \"holiday_type\", \"locale\": \"holiday_locale\"}\n",
    "    )\n",
    "\n",
    "    return df_holidays_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Function - Determining a count per type of holiday per store\n",
    "The dataframe resulting from the function described in 4.1. gives duplicate values because there sometimes are multiple holidays on one date. Duplicate values per date would result in multiple sales rows for each date, making it not workable. Therfore, we transform the holiday and stores combination to contain 3 columns (for each type of holiday, namely, local, regional and national) that count the amount of holidays found for a specific date. Thereby we create a unique list of date and store combinations for all the holidays within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1. Function - Creating unique combination of store and date with three count columns for each type of holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_merged_grouped(df_holidays, df_stores):\n",
    "\n",
    "    # Merge the holiday dataframes and clean the merged dataframe\n",
    "    df_holidays_merged = holidays_prep_merged(df_holidays, df_stores)\n",
    "\n",
    "    # Group by date and store_nbr and count the number of holidays per date per store\n",
    "    df_holidays_merged_grouped = df_holidays_merged.pivot_table(\n",
    "        index=[\"date\", \"store_nbr\"],\n",
    "        columns=\"holiday_locale\",\n",
    "        values=\"holiday_type\",\n",
    "        aggfunc=\"count\",\n",
    "        observed=True,\n",
    "    ).reset_index()\n",
    "\n",
    "    # Remove the name of the columns\n",
    "    df_holidays_merged_grouped.columns.name = None\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    df_holidays_merged_grouped = df_holidays_merged_grouped.fillna(0)\n",
    "\n",
    "    # Convert the count columns to Int8-dtype (note the capital 'I'). This dtype can handle null values, needed to prevent float64 from the merge in Step 6\n",
    "    # Rename the columns to holiday_local_count,  holiday_regional_count, holiday_national_count\n",
    "    df_holidays_merged_grouped = df_holidays_merged_grouped.astype(\n",
    "        {\"Local\": \"Int8\", \"Regional\": \"Int8\", \"National\": \"Int8\"}\n",
    "    ).rename(\n",
    "        columns={\n",
    "            \"Local\": \"holiday_local_count\",\n",
    "            \"Regional\": \"holiday_regional_count\",\n",
    "            \"National\": \"holiday_national_count\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Let's do an inner join with the original data to get the original date and store_nbr combinations back. Therefore we need to make another dataframe.\n",
    "    df_holidays_merged_grouped_inner = holidays_prep_merged(df_holidays, df_stores)\n",
    "    df_holidays_merged_grouped_inner = (\n",
    "        df_holidays_merged_grouped_inner.groupby([\"date\", \"store_nbr\"])\n",
    "        .size()\n",
    "        .reset_index()\n",
    "        .drop(columns=0)\n",
    "    )\n",
    "\n",
    "    df_holidays_merged_grouped = df_holidays_merged_grouped.merge(\n",
    "        df_holidays_merged_grouped_inner, on=[\"date\", \"store_nbr\"], how=\"inner\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"In the orignal unioned holiday dataframe, df_holidays_merged we found (including duplicates) {df_holidays_merged.shape[0]} rows\"\n",
    "    )\n",
    "    print(\n",
    "        f\"In our new adjusted dataframe we have {df_holidays_merged_grouped.shape[0]} rows\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Thus, we have removed {df_holidays_merged.shape[0] - df_holidays_merged_grouped.shape[0]} rows\"\n",
    "    )\n",
    "\n",
    "    # Might want to filter out the holiday dates that will never be in de salesdate range. However, they will be left out anyway when joining with the sales data.\n",
    "    return df_holidays_merged_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2. Function - Filling in NA values for each count column whenever no holiday could be found for a specific holiday date and store combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fill newly created NaN columns, due to holiday join, with 'no' on thates where there are now holidays\n",
    "def holidays_fill_zero_normal(df):\n",
    "    \"\"\"\n",
    "    Fills the NaN values with 0 for all columns \"holiday_local_count\", \"holiday_regional_count\", \"holiday_national_count\", in the combined dataframe.\n",
    "    It will only fill the columns that are in the original dataframe and not in the holiday dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    columns_to_fill = [\n",
    "        \"holiday_local_count\",\n",
    "        \"holiday_regional_count\",\n",
    "        \"holiday_national_count\",\n",
    "    ]\n",
    "\n",
    "    df[columns_to_fill] = df[columns_to_fill].fillna(0).astype(\"int8\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Function - Constructing a cartesian sales dataset for each store based on the maximum sales daterange\n",
    "The df_sales dataset contains unit sales data for each store but not all stores have data for each date. To overcome this and make sure each date is present for each store we construct a new dataframe based on the minimum- and maximum date found within the sales dataframe. The result is thus a sales dataframe with each date, store and item combination for the whole timerange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filling_dates_cartesian(df):\n",
    "\n",
    "    # Print first and last date of df\n",
    "    print(f'First date in df: {df[\"date\"].min()}')\n",
    "    print(f'Last date in df:  {df[\"date\"].max()}')\n",
    "\n",
    "    # Calculate memory size and shape size of start df\n",
    "    df_mem_start = sys.getsizeof(df)\n",
    "    df_shape_start = df.shape[0] / 1e6\n",
    "    print(\n",
    "        f\"Start size of df_sales:     {round(df_mem_start/1024/1024/1024, 2)} GB and start observations:     {round(df_shape_start, 1)} million.\"\n",
    "    )\n",
    "\n",
    "    # Create a complete date range for the entire dataset, it's a datetimeindex object\n",
    "    all_dates = pd.date_range(start=df[\"date\"].min(), end=df[\"date\"].max(), freq=\"D\")\n",
    "\n",
    "    # Create a multi-index from all possible combinations of 'item_nbr' and 'date'\n",
    "    all_combinations = pd.MultiIndex.from_product(\n",
    "        [df[\"store_nbr\"].unique(), df[\"item_nbr\"].unique(), all_dates],\n",
    "        names=[\"store_nbr\", \"item_nbr\", \"date\"],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"The multi-index (all_combinations of store, date and item) for the minimum and maximum dates found result in {round(all_combinations.shape[0]/1e6,1)} million rows, this is the amount of rows we expect in the final dataframe.\"\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "    # Check for duplicates in the combination of 'store_nbr', 'item_nbr', and 'date'\n",
    "    # This method is based on boolean indexing, when there's a true value for the duplicated method, it will return those rows to the duplicate_rows variable\n",
    "    duplicate_rows = df[\n",
    "        df.duplicated(subset=[\"store_nbr\", \"item_nbr\", \"date\"], keep=False)\n",
    "    ]\n",
    "    if not duplicate_rows.empty:\n",
    "        print(\n",
    "            \"Warning: Duplicate entries found in the combination of 'store_nbr', 'item_nbr', and 'date'.\"\n",
    "        )\n",
    "        print(f\"Total dublicate rows {duplicate_rows.shape[0]}\")\n",
    "        print(\"-\" * 71)\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Reindex the original DataFrame to include all combinations of 'store_nbr', 'item_nbr', and 'date'\n",
    "    df_reindexed = df.set_index([\"store_nbr\", \"item_nbr\", \"date\"]).reindex(\n",
    "        all_combinations\n",
    "    )\n",
    "\n",
    "    # Reset the index to turn the multi-index back into regular columns\n",
    "    df_sales_cartesian = df_reindexed.reset_index()\n",
    "\n",
    "    # Calculate memory size and shape size of final end df\n",
    "    df_mem_end = sys.getsizeof(df_sales_cartesian)\n",
    "    df_mem_change_perc = ((df_mem_end - df_mem_start) / df_mem_start) * 100\n",
    "    df_mem_change = df_mem_end - df_mem_start\n",
    "\n",
    "    df_shape_end = df_sales_cartesian.shape[0] / 1e6\n",
    "    df_shape_change_perc = ((df_shape_end - df_shape_start) / df_shape_start) * 100\n",
    "    df_shape_change = df_shape_end - df_shape_start\n",
    "\n",
    "    print(\n",
    "        f\"Final size of the dataframe is:     {round(df_mem_end/1024/1024/1024, 2)} GB and end observations:       {round(df_shape_end, 1)} million.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Change in size of the dataframe is: {round(df_mem_change_perc, 2)} % and observations:           {round(df_shape_change_perc, 2)}     %.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Increased size of the dataframe is: {round(df_mem_change/1024/1024/1024, 2)} GB and increased observations: {round(df_shape_change, 1)} million.\"\n",
    "    )\n",
    "\n",
    "    return df_sales_cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Constructing final dataset\n",
    "In this step all the datasets will be merged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "def merge_datasets(df_sales, df_items, df_stores, df_holidays):\n",
    "\n",
    "    # Basic information of loaded data\n",
    "    print(\n",
    "        \"Step 1 - Importing, downcasting and normalizing data and optimizing memory, the following data has been imported.\"\n",
    "    )\n",
    "    df_basic_info(df_sales, \"df_sales\")\n",
    "    print(\"\"\"\"\"\")\n",
    "    df_basic_info(df_items, \"df_items\")\n",
    "    print(\"\"\"\"\"\")\n",
    "    df_basic_info(df_stores, \"df_stores\")\n",
    "    print(\"\"\"\"\"\")\n",
    "    df_basic_info(df_holidays, \"df_holidays\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Sales prep\n",
    "    print(\n",
    "        \"Step 2 - Cleaning sales data and making a cartesian product of the sales data and the minimum and maximum dates found in the data.\"\n",
    "    )\n",
    "    df_sales = sales_cleaned(df_sales)\n",
    "    df_sales = df_sales_cleaned_stores(df_sales, df_stores)\n",
    "    df_sales_cartesian = filling_dates_cartesian(df_sales)\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Holidays prep\n",
    "    print(\n",
    "        \"Step 3 - Cleaning holiday data and counting the number of holidays per date per store for each type of holiday (national, regional, local).\"\n",
    "    )\n",
    "    df_holidays = df_holidays_cleaned(df_holidays)\n",
    "    df_holidays_merged_grouped = holidays_prep_merged_grouped(df_holidays, df_stores)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Stores prep\n",
    "    print(\n",
    "        \"Step 4 - Cleaning stores data (read: dropping unnecessary columns and renaming columns for clarity).\"\n",
    "    )\n",
    "    df_stores = stores_cleaned_renamed(df_stores)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Items prep\n",
    "    print(\n",
    "        \"Step 5 - Cleaning items data  (read: dropping unnecessary columns and renaming columns for clarity).\"\n",
    "    )\n",
    "    df_items = items_cleaned_renamed(df_items)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Holidays merge on sales\n",
    "    print(\n",
    "        \"Step 6 - Adding holiday data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of three holiday columns.\"\n",
    "    )\n",
    "\n",
    "    df_merged = df_sales_cartesian.merge(\n",
    "        df_holidays_merged_grouped, on=[\"date\", \"store_nbr\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    df_merged = holidays_fill_zero_normal(df_merged)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Stores merged with sales+holidays\n",
    "    print(\n",
    "        \"Step 7 - Adding holiday data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of holiday columns.\"\n",
    "    )\n",
    "    df_merged = df_merged.merge(df_stores, on=\"store_nbr\", how=\"left\")\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Change the dtype for item_nbr from uint32 to int32, during testing we found that the merge was not working properly with uint32\n",
    "    df_merged[\"item_nbr\"] = df_merged[\"item_nbr\"].astype(int)\n",
    "    df_items[\"item_nbr\"] = df_items[\"item_nbr\"].astype(int)\n",
    "\n",
    "    # Items merged with sales+holidays+stores\n",
    "    print(\n",
    "        \"Step 8 - Adding items data to our cartesian product of sales data (with store, item and date combinations) and cleaning up null values for count of holiday columns. Remember, in our last step we added a lot of store information as well\"\n",
    "    )\n",
    "    df_final = df_merged.merge(df_items, on=\"item_nbr\", how=\"left\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Print some referential integrity checks to make sure we have the same amount of rows\n",
    "    print(\n",
    "        f\"The amount of rows in the sales dataframe was {df_sales.shape[0] /1_000_000:.2f} million.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"After making a cartesian product with date, store and item we had a total of {df_sales_cartesian.shape[0]/1_000_000:.2f} million rows.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"After mergin with the holidays, stores, and items we have {df_final.shape[0]/1_000_000:.2f} million rows\"\n",
    "    )\n",
    "    print(\n",
    "        f\"The difference between the incoming and outgoing data from this function is {df_sales.shape[0] - df_final.shape[0]} rows\"\n",
    "    )\n",
    "    print(\n",
    "        f'If we compare the outgoing dataframe called \"df_final\" with the cartesian product of sales data and dates we see that the difference is {df_sales_cartesian.shape[0] - df_final.shape[0]} rows'\n",
    "    )\n",
    "    print(\n",
    "        f\"If the difference is 0, we have a perfect match and we can continue with the next steps.\"\n",
    "    )\n",
    "\n",
    "    # f\"Final size of the dataframe is:     {round(df_mem_end/1024/1024/1024, 2)} GB and end observations:       {round(df_shape_end, 1)} million.\"\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sales = df_sales[(df_sales[\"store_nbr\"] == 1)]\n",
    "\n",
    "# df_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = merge_datasets(df_sales, df_items, df_stores, df_holidays)  # --> 2.44 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastiaan code -\n",
    "# What about the \"onpromotion\" column, seems that it has a lot of NaN values. Are these quality issues or is just that there's no promotion.\n",
    "# This issue didn't arrive after merging, it was there from the beginning (in the df_sales dataframe).\n",
    "# You would expect that if there's no promotion going on the value to be \"False\"\n",
    "\n",
    "# df_sales1 = sales_cleaned(df_sales)\n",
    "\n",
    "# df_sales1_unique = df_sales1[\"onpromotion\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X.X. Count nulls per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()\n",
    "# Count nulls per column\n",
    "null_counts = df_final.isnull().sum()\n",
    "\n",
    "# Print results\n",
    "for column, count in null_counts.items():\n",
    "    print(f\"Column '{column}' has {count} null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2: Detect negative values\n",
    "\n",
    "•\tAction: Delete unit_sales if values are lower than zero --> N/A\n",
    "\n",
    "To-do: do we want do make negative --> 0 or delete values --> Inpute later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sales_cleaned(df):\n",
    "\n",
    "    # Check the number of negative values before replacement\n",
    "    before_replacement = (df[\"unit_sales\"] < 0).sum()\n",
    "    print(f\"Number of negative values before replacement: {before_replacement}\")\n",
    "\n",
    "    # Create a boolean mask for the negative sales rows to create a 'boolean flag-list' containing all negative rows, used to filter full df_sales df\n",
    "    negative_sales_mask = df[\"unit_sales\"] < 0\n",
    "\n",
    "    # Use the mask to update the flagged 'unit_sales' column in the original DataFrame\n",
    "    df.loc[negative_sales_mask, \"unit_sales\"] = df.loc[\n",
    "        negative_sales_mask, \"unit_sales\"\n",
    "    ].where(df.loc[negative_sales_mask, \"unit_sales\"] >= 0, np.nan)\n",
    "\n",
    "    # Check the number of negative values after replacement\n",
    "    after_replacement = (df[\"unit_sales\"] < 0).sum()\n",
    "    print(f\"Number of negative values after replacement: {after_replacement}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Define new, old and closed stores\n",
    "\n",
    "•\tCondition: sales for all items a given store and date are NA\n",
    "\n",
    "•\tAction: Impute with 0\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "Label Variable for atributing numbers to store status:\n",
    "\n",
    "-     OPEN = 0\n",
    "-     NEW = 2\n",
    "-     CLOSED = 4\n",
    "-     OLD = 6\n",
    "-     NEVER_OPENED = 8\n",
    "\n",
    "----------\n",
    "\n",
    "To-do: Write in polars??\n",
    "\n",
    "To-do: Can the ML model run with NaN values? Or need the new / old stores also need to inputed with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_store_status(df):\n",
    "\n",
    "    # Label Variable for atributing numbers to store status, to save memory in df\n",
    "    OPEN = 0\n",
    "    NEW = 2\n",
    "    CLOSED = 4\n",
    "    OLD = 6\n",
    "    NEVER_OPENED = 8\n",
    "\n",
    "    # Group by store and date, then sum sales\n",
    "    df_grouped = (\n",
    "        df.groupby([\"store_nbr\", \"date\"]).agg({\"unit_sales\": \"sum\"}).reset_index()\n",
    "    ).reset_index()\n",
    "\n",
    "    # Sort by store and date\n",
    "    df_grouped = df_grouped.sort_values([\"store_nbr\", \"date\"])\n",
    "\n",
    "    # Create a new column for store status, label al stores as 'open' by default and make dtype in8\n",
    "    df_grouped[\"store_status\"] = np.int8(OPEN)\n",
    "\n",
    "    # Find the first and last day with sales for each store\n",
    "    first_sale_date = (\n",
    "        df_grouped[df_grouped[\"unit_sales\"] > 0].groupby(\"store_nbr\")[\"date\"].min()\n",
    "    )\n",
    "\n",
    "    last_sale_date = (\n",
    "        df_grouped[df_grouped[\"unit_sales\"] > 0].groupby(\"store_nbr\")[\"date\"].max()\n",
    "    )\n",
    "\n",
    "    # Loop trhough stores by lapeling them as 'NEW', 'CLOSED', 'OLD' or 'NEVER_OPENED' based on first sale date and last sale date\n",
    "    for store in df_grouped[\"store_nbr\"].unique():\n",
    "        store_data = df_grouped[df_grouped[\"store_nbr\"] == store]\n",
    "\n",
    "        if store in first_sale_date.index:\n",
    "            first_date = first_sale_date[store]\n",
    "            last_date = last_sale_date[store]\n",
    "\n",
    "            # Mark as 'NEW' before first sale date\n",
    "            df_grouped.loc[\n",
    "                (df_grouped[\"store_nbr\"] == store) & (df_grouped[\"date\"] < first_date),\n",
    "                \"store_status\",\n",
    "            ] = NEW\n",
    "            # --> To-do: Do we call this  not opened' or a 'new store'?\n",
    "\n",
    "            # Mark as 'closed' after first sale date if sales are 0\n",
    "            df_grouped.loc[\n",
    "                (df_grouped[\"store_nbr\"] == store)\n",
    "                & (df_grouped[\"date\"] > first_date)\n",
    "                & (df_grouped[\"unit_sales\"] == 0),\n",
    "                \"store_status\",\n",
    "            ] = CLOSED\n",
    "\n",
    "            # Mark as 'OLD' after last sale date\n",
    "            df_grouped.loc[\n",
    "                (df_grouped[\"store_nbr\"] == store) & (df_grouped[\"date\"] > last_date),\n",
    "                \"store_status\",\n",
    "            ] = OLD\n",
    "\n",
    "        else:\n",
    "            # If a store never had any sales, mark all dates as 'NEVER_OPENED' --> no records?\n",
    "            df_grouped.loc[df_grouped[\"store_nbr\"] == store, \"store_status\"] = (\n",
    "                NEVER_OPENED\n",
    "            )\n",
    "\n",
    "    # Merging store_status on df_sales\n",
    "    df = df.merge(\n",
    "        df_grouped[[\"store_nbr\", \"date\", \"store_status\"]],\n",
    "        left_on=[\"store_nbr\", \"date\"],\n",
    "        right_on=[\"store_nbr\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Get list of NEW stores at 01-01-2013 and OPEN stores at 02-01-2013\n",
    "    mask_new = (df[\"store_status\"] == NEW) & (df[\"date\"] == \"2013-01-01\")\n",
    "    mask_open = (df[\"store_status\"] == OPEN) & (df[\"date\"] == \"2013-01-02\")\n",
    "\n",
    "    # Get list of thores that meet both the coditions of NEW AT 01-01-2013 and OPEN at 02-01-2013\n",
    "    stores_new = set(df[mask_new][\"store_nbr\"].unique())\n",
    "    stores_open = set(df[mask_open][\"store_nbr\"].unique())\n",
    "    stores_status_change = stores_new.intersection(stores_open)\n",
    "\n",
    "    # Change status of stores that are NEW on 01-01-2013 but OPEN on 02-01-2013 to CLOSED on 01-01-2013\n",
    "    df.loc[\n",
    "        (df[\"store_nbr\"].isin(stores_status_change)) & (df[\"date\"] == \"2013-01-01\"),\n",
    "        [\"store_status\"],\n",
    "    ] = [CLOSED]\n",
    "\n",
    "    # Using a mask to flag al 'CLOSED' or (|) 'NEW' stores and impute 'closed' and 'new' stores with 0\n",
    "    mask = (df[\"store_status\"] == CLOSED) | (df[\"store_status\"] == NEW)\n",
    "    df.loc[mask, \"unit_sales\"] = 0\n",
    "\n",
    "    print(\"-\" * 72)\n",
    "    print(\n",
    "        f\"Size of df:     {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB and end observations:       {round(df.shape[0] / 1e6, 1)} million.\"\n",
    "    )\n",
    "    print(\"- \" * 36)\n",
    "    print(\"df_grouped store_status value counts:\")\n",
    "    print(df_grouped[\"store_status\"].value_counts())\n",
    "\n",
    "    print(\"-\" * 72)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 New product --> !Polars function!\n",
    "\n",
    "•\tBefore the very first sale of an item, all observations are kept as NA\n",
    "\n",
    "•\tAfter the very first sale of an item, we go to step 3: \n",
    "\n",
    " -----------------------------------\n",
    "\n",
    "Label Variable for atributing numbers to store status, to save memory in df\n",
    "-     EXISTING = 1\n",
    "-     NEW = 3\n",
    "-     OLD = 7\n",
    "-     NEVER_SOLD = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO: Add polars to requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <PATH>.\\venv_case_project\\Scripts\\activate\n",
    "# pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl  # later to import packages step at 0\n",
    "\n",
    "\n",
    "def merge_item_status_polars(df_pandas):\n",
    "\n",
    "    # Record the start time of the function\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Label variables\n",
    "\n",
    "    EXISTING = 1\n",
    "    NEW = 3\n",
    "    OLD = 7\n",
    "    NEVER_SOLD = 9\n",
    "\n",
    "    # Convert the Pandas df to Polars df\n",
    "    df = pl.from_pandas(df_pandas)\n",
    "\n",
    "    # Sort by store, item, and date\n",
    "    df = df.sort([\"store_nbr\", \"item_nbr\", \"date\"])\n",
    "\n",
    "    print(f\"Elapsed time: {time.time() - start_time:.2f} seconds | LINE | df sorted |\")\n",
    "\n",
    "    # Create a new column for item status, initialise to EXISTING\n",
    "    df = df.with_columns(pl.lit(EXISTING).cast(pl.Int8).alias(\"item_status\"))\n",
    "\n",
    "    print(\n",
    "        f\"Elapsed time: {time.time() - start_time:.2f} seconds | LINE | item_status added |\"\n",
    "    )\n",
    "\n",
    "    # Filter for rows with unit_sales > 0 and calculate first/last sale dates\n",
    "    first_sale_date = (\n",
    "        df.filter(pl.col(\"unit_sales\") > 0)\n",
    "        .group_by([\"store_nbr\", \"item_nbr\"])\n",
    "        .agg([pl.col(\"date\").min().alias(\"first_sale_date\")])\n",
    "    )\n",
    "\n",
    "    last_sale_date = (\n",
    "        df.filter(pl.col(\"unit_sales\") > 0)\n",
    "        .group_by([\"store_nbr\", \"item_nbr\"])\n",
    "        .agg([pl.col(\"date\").max().alias(\"last_sale_date\")])\n",
    "    )\n",
    "    print(\n",
    "        f\"Elapsed time: {time.time() - start_time:.2f} seconds | LINE | first and last sale dates |\"\n",
    "    )\n",
    "\n",
    "    # Join first and last sale dates to the original dataframe\n",
    "    df = df.join(first_sale_date, on=[\"store_nbr\", \"item_nbr\"], how=\"left\")\n",
    "\n",
    "    df = df.join(last_sale_date, on=[\"store_nbr\", \"item_nbr\"], how=\"left\")\n",
    "\n",
    "    print(\n",
    "        f\"Elapsed time: {time.time() - start_time:.2f} seconds | LINE | joined sale dates |\"\n",
    "    )\n",
    "\n",
    "    # Update the item_status column based on first and last sale dates\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"date\") < pl.col(\"first_sale_date\"))\n",
    "        .then(pl.lit(NEW))\n",
    "        .when(pl.col(\"date\") > pl.col(\"last_sale_date\"))\n",
    "        .then(pl.lit(OLD))\n",
    "        .otherwise(pl.col(\"item_status\"))\n",
    "        .alias(\"item_status\")\n",
    "    )\n",
    "\n",
    "    # Handle NEVER_SOLD case where first_sale_date is null\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"first_sale_date\").is_null())\n",
    "        .then(pl.lit(NEVER_SOLD))\n",
    "        .otherwise(pl.col(\"item_status\"))\n",
    "        .alias(\"item_status\")\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Elapsed time: {time.time() - start_time:.2f} seconds | LINE | updated item status |\"\n",
    "    )\n",
    "\n",
    "    # Drop columns first_sale_date\" and \"last_sale_date\" as these are not longer needed\n",
    "    df = df.drop([\"first_sale_date\", \"last_sale_date\"])\n",
    "\n",
    "    # Convert Polars df back to Pandas df\n",
    "    df = df.to_pandas()\n",
    "\n",
    "    print(\"-\" * 72)\n",
    "    print(f\"Total execution time: {(time.time() - start_time) / 60:.2f} minutes\")\n",
    "    print(\"- \" * 36)\n",
    "    print(\"df_grouped item_status value counts:\")\n",
    "    print(df[\"item_status\"].value_counts())\n",
    "\n",
    "    print(\"-\" * 72)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8  Stockout on store level\n",
    "\n",
    "•      Perishable good: when there are missing values for two consecutive days for a given item per individual store \n",
    "\n",
    "•      Nonperishable goods: when there are missing values for 7 consecutive days for a given item and per individual store\n",
    "\n",
    "•      Action: Impute with algorithm \n",
    "\n",
    "------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do: Add print function to keep track of type of inputations\n",
    "\n",
    "To-do: .interpolate() --> ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "def impute_stockouts_polars(df_pandas, window_size=7):\n",
    "\n",
    "    # Convert the input Pandas DataFrame to a Polars DataFrame for efficient processing\n",
    "\n",
    "    df = pl.from_pandas(df_pandas)\n",
    "\n",
    "    # Sort the DataFrame by store number, item number, and date for consistent ordering\n",
    "\n",
    "    df = df.sort([\"store_nbr\", \"item_nbr\", \"date\"])\n",
    "\n",
    "    # Nested function calc_missing_count to calculate the count of consecutive missing values in unit_sales\n",
    "\n",
    "    def calc_missing_count(unit_sales):\n",
    "\n",
    "        return (\n",
    "            unit_sales.is_null()  # Check for null values\n",
    "            .cast(pl.Int32)  # Cast to integer (1 for null, 0 for not null)\n",
    "            .cum_sum()  # Cumulative sum to count sequential nulls\n",
    "            .over([\"store_nbr\", \"item_nbr\"])  # Group by store_nbr and item_nbr\n",
    "        )\n",
    "\n",
    "    # Nested function to Inpute with rolling mean for missing values\n",
    "    def rolling_mean_imputation(unit_sales, window_size):\n",
    "\n",
    "        return (\n",
    "            unit_sales.rolling_mean(\n",
    "                window_size=window_size, min_periods=1\n",
    "            )  # Impute strategy based on rolling mean\n",
    "            .shift(\n",
    "                1\n",
    "            )  # Shift window by one day, to prevent taking the same day into account\n",
    "            .over([\"store_nbr\", \"item_nbr\"])  # Group by store_nbr and item_nbr\n",
    "        )\n",
    "\n",
    "    # Apply the imputation logic based on the perishable status of the items\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.when(pl.col(\"perishable\") == 1)  # Check if the item is perishable = 1\n",
    "            .then(\n",
    "                pl.when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) == 1\n",
    "                )  # 1 missing value\n",
    "                .then(0)  # --> Impute with 0\n",
    "                .when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) > 2\n",
    "                )  # More than 2 missing values\n",
    "                .then(0)  # --> Impute with 0\n",
    "                .when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) == 2\n",
    "                )  # = 2 missing values\n",
    "                .then(\n",
    "                    rolling_mean_imputation(pl.col(\"unit_sales\"), window_size)\n",
    "                )  # --> Inpute with rolling mean for 2 missing days\n",
    "                .otherwise(pl.col(\"unit_sales\"))  # Otherwise keep original value\n",
    "            )\n",
    "            .when(pl.col(\"perishable\") == 0)  # If the item is not perishable = 0\n",
    "            .then(\n",
    "                pl.when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) > 7\n",
    "                )  # More than 7 missing values\n",
    "                .then(0)  # --> Impute with 0\n",
    "                .when(\n",
    "                    calc_missing_count(pl.col(\"unit_sales\")) <= 7\n",
    "                )  # if less 7 missing values\n",
    "                .then(\n",
    "                    rolling_mean_imputation(pl.col(\"unit_sales\"), window_size)\n",
    "                )  # --> Inpute with rolling mean for missing 7 or less days\n",
    "                .otherwise(pl.col(\"unit_sales\"))  # Otherwise keep original value\n",
    "            )\n",
    "            .otherwise(pl.col(\"unit_sales\"))  # For any other case not covered\n",
    "            .alias(\"unit_sales\")  # Alias the new column as 'unit_sales'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Convert Polars df back to Pandas df\n",
    "\n",
    "    df = df.to_pandas()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 Promotional Data \n",
    "\n",
    "•   All missing values are interpreted a day with no promotion\n",
    "\n",
    "•   Action: Inpute onpromotion N/A with False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing N/A values in onpromotion column with False\n",
    "def sales_fill_onpromotion(df):\n",
    "\n",
    "    df[\"onpromotion\"] = df[\"onpromotion\"].fillna(False).astype(bool)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Feature construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.X Extracting datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_features(df):\n",
    "    # Ensure the date column is sorted\n",
    "    df = df.sort_values(\"date\")\n",
    "\n",
    "    # Add column with ISO year\n",
    "    df[\"year\"] = df[\"date\"].dt.isocalendar().year.astype(\"int16\")\n",
    "\n",
    "    # Add column with weekday (1-7, where 1 is Monday)\n",
    "    df[\"weekday\"] = df[\"date\"].dt.dayofweek.add(1).astype(\"int8\")\n",
    "\n",
    "    # Add column with ISO week number (1-53)\n",
    "    df[\"week_nbr\"] = df[\"date\"].dt.isocalendar().week.astype(\"int8\")\n",
    "\n",
    "    # Calculate the date of the Monday of the first week\n",
    "    first_date = df[\"date\"].iloc[0]\n",
    "    days_to_last_monday = (first_date.weekday() - 0 + 7) % 7\n",
    "    monday_first_week = first_date - pd.Timedelta(days=days_to_last_monday)\n",
    "\n",
    "    # Calculate cumulative week numbers starting from the first Monday\n",
    "    df[\"week_number_cum\"] = (\n",
    "        ((df[\"date\"] - monday_first_week).dt.days // 7) + 1\n",
    "    ).astype(\"int16\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data Manipulation and Feature construction --> Final-Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate_final_dataset(df):\n",
    "\n",
    "    df = negative_sales_cleaned(df)\n",
    "\n",
    "    df = merge_store_status(df)\n",
    "\n",
    "    df = merge_item_status_polars(df)\n",
    "\n",
    "    df = impute_stockouts_polars(df, window_size=7)\n",
    "\n",
    "    df = sales_fill_onpromotion(df)\n",
    "\n",
    "    df = datetime_features(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = manipulate_final_dataset(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()\n",
    "# Count nulls per column\n",
    "null_counts = df_final.isnull().sum()\n",
    "\n",
    "# Print results\n",
    "for column, count in null_counts.items():\n",
    "    print(f\"Column '{column}' has {count} null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Parquet fil and saves it in output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_parquet(df, output_path, file_prefix=\"Prepped_data\"):\n",
    "    try:\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Generate today's date for the filename\n",
    "        today = date.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "        # Create the full filename with path\n",
    "        filename = f\"{file_prefix}_{today}.parquet\"\n",
    "        full_path = os.path.join(output_path, filename)\n",
    "\n",
    "        # Save the DataFrame to a Parquet file\n",
    "        df.to_parquet(full_path)\n",
    "\n",
    "        print(f\"DataFrame successfully saved to {full_path}\")\n",
    "\n",
    "        return full_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame to Parquet file: {e}\")\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"C:/Users/alexander/Documents/0. Data Science and AI for Experts/TEST\"\n",
    "\n",
    "saved_path = save_dataframe_to_parquet(df_final, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X # Function to print memory usage of DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print memory usage of DataFrames\n",
    "def print_memory_usage(dataframes):\n",
    "    for name, df in dataframes.items():\n",
    "        mem_usage = df.memory_usage(deep=True)\n",
    "        total_mem = mem_usage.sum()\n",
    "\n",
    "        print(f\"DataFrame: {name}\")\n",
    "        print(mem_usage)\n",
    "        print(f\"Total Memory Usage: {total_mem} bytes\\n\")\n",
    "\n",
    "\n",
    "# Check for DataFrames\n",
    "dataframes = {\n",
    "    name: obj for name, obj in globals().items() if isinstance(obj, pd.DataFrame)\n",
    "}\n",
    "print_memory_usage(dataframes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_case_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
