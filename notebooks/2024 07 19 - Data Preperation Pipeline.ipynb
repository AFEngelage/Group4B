{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation Pipeline\n",
    "\n",
    "### Hamilton Framework\n",
    "https://hamilton.dagworks.io/en/latest/how-tos/use-in-jupyter-notebook/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import altair as alt\n",
    "\n",
    "import vegafusion as vf\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Downcast and transform data\n",
    "Update formatting of features to optimize memory and standardize column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_column_names(s):\n",
    "    \"\"\"Removes spaces from the column names.\"\"\"\n",
    "    return s.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "def optimize_memory(df):\n",
    "    \"\"\"Optimize memory usage of a DataFrame by converting object columns to categorical\n",
    "    and downcasting numeric columns to smaller types.\"\"\"\n",
    "\n",
    "    # Change: Objects to Categorical.\n",
    "    object_cols = df.select_dtypes(include=\"object\").columns\n",
    "    if not object_cols.empty:\n",
    "        print(\"Change: Objects to Categorical\")\n",
    "        df[object_cols] = df[object_cols].astype(\"category\")\n",
    "\n",
    "    # Change: Convert integers to smallest signed or unsigned integer and floats to smallest.\n",
    "    for col in df.select_dtypes(include=[\"int\"]).columns:\n",
    "        if (df[col] >= 0).all():  # Check if all values are non-negative\n",
    "            df[col] = pd.to_numeric(\n",
    "                df[col], downcast=\"unsigned\"\n",
    "            )  # Downcast to unsigned\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"integer\")  # Downcast to signed\n",
    "\n",
    "    # Downcast float columns\n",
    "    for col in df.select_dtypes(include=[\"float\"]).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def month_year_to_int(df, i):\n",
    "\n",
    "    # Change: Month and Year to integer.\n",
    "\n",
    "    if i == 0:\n",
    "\n",
    "        print(\"Change: Month and Year to integer\")\n",
    "\n",
    "        df = df.astype({\"month\": int, \"year\": int})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Transform date-related columns to datetime format.\n",
    "\n",
    "\n",
    "def transform_date_to_datetime(df, i):\n",
    "\n",
    "    if i == 0:\n",
    "\n",
    "        # print(\"Change: Transformed 'year', 'month', 'day' columns to Datetime feature\")\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]], unit=\"us\")\n",
    "\n",
    "        print(\n",
    "            \"Change: Dropped 'year', 'month', 'day' columns and transformed to Datetime64[us]\"\n",
    "        )\n",
    "\n",
    "        df.drop(columns=[\"day\", \"month\", \"year\"], inplace=True)\n",
    "\n",
    "    else:\n",
    "        if \"date\" in df.columns:\n",
    "\n",
    "            print(\"Change: Transformed 'date' column to Datetime Dtype\")\n",
    "\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_basic_info_before(df):\n",
    "    print(\n",
    "        f\"-> Contains:                {df.shape[0]} observations and {df.shape[1]} features.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-> Has original size of    {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def df_basic_info_after(df):\n",
    "    print(\n",
    "        f\"-> Contains:                {df.shape[0]} observations and {df.shape[1]} features.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-> Has optimized size of    {round(sys.getsizeof(df)/1024/1024/1024, 2)} GB.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import data from local PATH\n",
    "Import data trough pipeline to downcast the data and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_data(i=0):\n",
    "\n",
    "    # Define path.\n",
    "    c_path = \"C:/Users/alexander/Documents/0. Data Science and AI for Experts/EAISI_4B_Supermarket/data/raw/\"\n",
    "\n",
    "    # Identify file.\n",
    "    v_file = (\n",
    "        \"history-per-year\",  # 0\n",
    "        \"holidays_events\",  # 1\n",
    "        \"items\",  # 2\n",
    "        \"stores\",  # 3\n",
    "    )\n",
    "\n",
    "    print(f\"\\nReading file {i}\\n\")\n",
    "\n",
    "    # Load data.\n",
    "    df = (\n",
    "        pd.read_parquet(c_path + v_file[i] + \".parquet\")\n",
    "        .rename(columns=standardize_column_names)\n",
    "        .pipe(optimize_memory)\n",
    "        .pipe(month_year_to_int, i)\n",
    "        .pipe(transform_date_to_datetime, i)\n",
    "    )\n",
    "\n",
    "    # Return data.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading file 0\n",
      "\n",
      "Change: Month and Year to integer\n",
      "Change: Dropped 'year', 'month', 'day' columns and transformed to Datetime64[us]\n",
      "-> Contains:                125497040 observations and 6 features.\n",
      "-> Has optimized size of    2.69 GB.\n",
      "\n",
      "Reading file 1\n",
      "\n",
      "Change: Objects to Categorical\n",
      "Change: Transformed 'date' column to Datetime Dtype\n",
      "-> Contains:                350 observations and 6 features.\n",
      "-> Has optimized size of    0.0 GB.\n",
      "\n",
      "Reading file 2\n",
      "\n",
      "Change: Objects to Categorical\n",
      "-> Contains:                4100 observations and 4 features.\n",
      "-> Has optimized size of    0.0 GB.\n",
      "\n",
      "Reading file 3\n",
      "\n",
      "Change: Objects to Categorical\n",
      "-> Contains:                54 observations and 5 features.\n",
      "-> Has optimized size of    0.0 GB.\n"
     ]
    }
   ],
   "source": [
    "# To-do: write this in function. But where executed?\n",
    "\n",
    "# Sales History per year\n",
    "\n",
    "df_sales = f_get_data(0)\n",
    "\n",
    "df_basic_info_after(df_sales)\n",
    "\n",
    "\n",
    "# Holidays\n",
    "\n",
    "df_holidays = f_get_data(1)\n",
    "\n",
    "df_basic_info_after(df_holidays)\n",
    "\n",
    "\n",
    "# Items\n",
    "\n",
    "df_items = f_get_data(2)\n",
    "\n",
    "df_basic_info_after(df_items)\n",
    "\n",
    "\n",
    "# Stores\n",
    "\n",
    "df_stores = f_get_data(3)\n",
    "\n",
    "df_basic_info_after(df_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Exclude Stores + Vulcano Eruption holiday + Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Return list containing stores with less then 1670 operational days with sales\n",
    "\n",
    "parameter: store_exclusion_cutoff_number = 1670 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stores_exclude_sales_days(df_sales, df_stores, store_exclusion_cutoff_number=1670):\n",
    "\n",
    "    # Group the sales date by store and item\n",
    "    df_sales_grouped = (\n",
    "        df_sales.groupby([\"store_nbr\", \"date\"]).agg({\"unit_sales\": \"sum\"}).reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge the grouped sales data with the store data\n",
    "    df_sales_stores_merged = df_sales_grouped.merge(\n",
    "        df_stores, left_on=\"store_nbr\", right_on=\"store_nbr\", how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Count the number of daily sale records per store\n",
    "    store_count = df_sales_stores_merged[\"store_nbr\"].value_counts()\n",
    "\n",
    "    # Get stores with counts less than the exclusion cutoff\n",
    "    store_count_exclusion = store_count[store_count < store_exclusion_cutoff_number]\n",
    "\n",
    "    # Get the list of store numbers to be excluded\n",
    "    list_excluded_stores_sales_days = store_count_exclusion.index.tolist()\n",
    "\n",
    "    return list_excluded_stores_sales_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 14, 12, 25, 24, 18, 36, 53, 20, 29, 21, 42, 22, 52]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores_exclude_sales_days(\n",
    "    df_sales, df_stores, store_exclusion_cutoff_number=1670\n",
    ")  # --> [30, 14, 12, 25, 24, 18, 36, 53, 20, 29, 21, 42, 22, 52]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Return list containing stores with cluster=10 in stores df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stores_exclude_cluster(df_stores, cluster_number=10):\n",
    "\n",
    "    # Get the list of store numbers that belong to cluster 10\n",
    "\n",
    "    list_stores_cluster_10 = df_stores[df_stores[\"cluster\"] == cluster_number][\n",
    "        \"store_nbr\"\n",
    "    ].tolist()\n",
    "\n",
    "    return list_stores_cluster_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26, 28, 29, 31, 36, 43]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores_exclude_cluster(df_stores, cluster_number=10)  # --> [26, 28, 29, 31, 36, 43]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 Function to exclude stores with less then 1670 sales days and related to cluster 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sales_cleaned_stores(df_sales, store_exclusion_cutoff_number=1670):\n",
    "\n",
    "    # Excluded less then 1670 salesdays\n",
    "    list_excluded_stores_sales_days = stores_exclude_sales_days(\n",
    "        df_sales, df_stores, store_exclusion_cutoff_number\n",
    "    )\n",
    "\n",
    "    df_sales = df_sales.drop(\n",
    "        df_sales[df_sales[\"store_nbr\"].isin(list_excluded_stores_sales_days)].index\n",
    "    )\n",
    "\n",
    "    # Cluster 10\n",
    "    list_stores_cluster_10 = stores_exclude_cluster(df_stores, cluster_number=10)\n",
    "\n",
    "    df_sales = df_sales.drop(\n",
    "        df_sales[df_sales[\"store_nbr\"].isin(list_stores_cluster_10)].index\n",
    "    )\n",
    "\n",
    "    return df_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution of final function --> In pipeline\n",
    "df_sales = df_sales_cleaned_stores(df_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4 Filter Vulcano Eruption from holiday df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holiday_filter_vulcano_event(df_holidays, event_substring=\"Terremoto Manabi\"):\n",
    "\n",
    "    # Filter the DataFrame where 'description' contains the event_substring\n",
    "    df_vulcano_event_filtered = df_holidays[\n",
    "        df_holidays[\"description\"].str.contains(event_substring)\n",
    "    ]\n",
    "\n",
    "    return df_vulcano_event_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_holidays_cleaned(df_holidays):\n",
    "\n",
    "    # Exclude holiday_filter_vulcano_event function to return filtered df\n",
    "    df_vulcano_event_filtered = holiday_filter_vulcano_event(df_holidays)\n",
    "\n",
    "    # Filter the specific holiday events from the holiday DataFrame\n",
    "    df_holidays = df_holidays.loc[\n",
    "        ~df_holidays.index.isin(df_vulcano_event_filtered.index)\n",
    "    ]\n",
    "\n",
    "    return df_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution of final function --> In pipeline?\n",
    "df_holidays = df_holidays_cleaned(df_holidays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Filter and exclude of Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Orginal, first try and check on item level\n",
    "\n",
    "\n",
    "def item_check(start_date, x_days):\n",
    "\n",
    "    # start_date = \"2013-02-01\"\n",
    "    # x_days = 31\n",
    "\n",
    "    # Convert start_date to datetime\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    print(start_date)\n",
    "\n",
    "    # Calculate end_date\n",
    "    end_date = start_date + timedelta(days=x_days)\n",
    "    print(x_days)\n",
    "    print(end_date)\n",
    "\n",
    "    # Filter the DataFrame based on the date range\n",
    "    df_sales_filtered = df_sales[\n",
    "        (df_sales[\"date\"] >= start_date) & (df_sales[\"date\"] <= end_date)\n",
    "    ]\n",
    "\n",
    "    # Group by item_nbr and sum unit_sales, so this will be the same criteria for all stores.\n",
    "    df_sales_item = (\n",
    "        df_sales_filtered.groupby(\"item_nbr\").agg({\"unit_sales\": \"sum\"}).reset_index()\n",
    "    )\n",
    "\n",
    "    # Get the list of store numbers to stay included, as they have sales within the first 28 days\n",
    "    list_sales_items = df_sales_item[\"item_nbr\"].tolist()\n",
    "    unique_values_count = df_sales_item[\"item_nbr\"].nunique()\n",
    "    print(unique_values_count)\n",
    "\n",
    "    # If sum_sales = 0 --> drop item\n",
    "\n",
    "    # first 4 weeks\n",
    "    # last 4 weeks\n",
    "\n",
    "    # sum_total per item for these two months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_check(\"2016-01-01\", 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date = \"2013-01-01\"\n",
    "# x_days = 28\n",
    "\n",
    "# # Convert start_date to datetime\n",
    "# start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "# print(start_date)\n",
    "\n",
    "# # Calculate end_date\n",
    "# end_date = start_date + timedelta(days=x_days)\n",
    "# print(x_days)\n",
    "# print(end_date)\n",
    "\n",
    "# # Filter the DataFrame based on the date range\n",
    "# df_sales_filtered = df_sales[\n",
    "#     (df_sales[\"date\"] >= start_date) & (df_sales[\"date\"] <= end_date)\n",
    "# ]\n",
    "\n",
    "# # Change the dtype for item_nbr from uint32 to int32\n",
    "# df_sales[\"item_nbr\"] = df_sales[\"item_nbr\"].astype(int)\n",
    "# df_items[\"item_nbr\"] = df_items[\"item_nbr\"].astype(int)\n",
    "\n",
    "# # Merge the filtered sales data with the items data\n",
    "# df_sales_items_merged = df_sales.merge(df_items, on=\"item_nbr\", how=\"left\")\n",
    "\n",
    "# # print(df_sales_items_merged.info())\n",
    "# # print(df_sales_items_merged.sample(5))\n",
    "\n",
    "# df_sales_items_merged[\"class\"] = df_sales_items_merged[\"class\"].astype(str)\n",
    "\n",
    "# # Group by item_nbr and sum unit_sales, so this will be the same criteria for all stores.\n",
    "# df_sales_item = (\n",
    "#     df_sales_filtered.groupby(\"class\").agg({\"unit_sales\": \"sum\"}).reset_index()\n",
    "# )\n",
    "\n",
    "# # Get the list of store numbers to stay included, as they have sales within the first 28 days\n",
    "# list_sales_items = df_sales_item[\"class\"].tolist()\n",
    "# unique_values_count = df_sales_item[\"class\"].nunique()\n",
    "# print(unique_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(df, column_name):\n",
    "    \"\"\"Get the all values and the count for specific column\"\"\"\n",
    "    unique_values_count = df[column_name].nunique()\n",
    "    unique_values = df[column_name].unique()\n",
    "\n",
    "    # Convert unique values to a single string to print\n",
    "    unique_values_str = \", \".join(map(str, unique_values))\n",
    "\n",
    "    print(f\"Number of unique values in {column_name}: {unique_values_count}\")\n",
    "    print(\"Unique values:\")\n",
    "    print(unique_values_str)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Prepare and Merge df_sales + df_items + df_stores + df_holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Prepare df_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df_holiday and df_stores by cleaning up df for merging with holidays by dropping unneeded columns\n",
    "def clean_holidays_stores_prep(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned = df_holidays.drop(\n",
    "        columns=[\n",
    "            \"description\",\n",
    "            \"transferred\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_stores_cleaned = df_stores.drop(columns=[\"cluster\", \"type\"])\n",
    "\n",
    "    return df_holidays_cleaned, df_stores_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_local(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # select locale 'Local' from holiday df and merge with city stores df\n",
    "    df_holidays_local = df_holidays_cleaned[df_holidays_cleaned[\"locale\"] == \"Local\"]\n",
    "\n",
    "    df_holidays_prep_local = df_holidays_local.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"city\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_regional(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # select locale 'Regional' from holiday df and merge with state stores df\n",
    "    df_holidays_regional = df_holidays_cleaned[\n",
    "        df_holidays_cleaned[\"locale\"] == \"Regional\"\n",
    "    ]\n",
    "\n",
    "    df_holidays_prep_regional = df_holidays_regional.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"state\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_regional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_national(df_holidays, df_stores):\n",
    "\n",
    "    df_holidays_cleaned, df_stores_cleaned = clean_holidays_stores_prep(\n",
    "        df_holidays, df_stores\n",
    "    )\n",
    "\n",
    "    # Select locale 'Regional' from holiday df and merge with national stores df\n",
    "    df_holidays_national = df_holidays_cleaned[\n",
    "        df_holidays_cleaned[\"locale\"] == \"National\"\n",
    "    ]\n",
    "\n",
    "    # Create extra column for merge on \"Ecuador\"\n",
    "    df_stores_cleaned[\"national_merge\"] = \"Ecuador\"\n",
    "\n",
    "    df_holidays_prep_national = df_holidays_national.merge(\n",
    "        df_stores_cleaned, left_on=\"locale_name\", right_on=\"national_merge\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Drop newly created column national_merge, not needed further\n",
    "    df_holidays_prep_national = df_holidays_prep_national.drop(\n",
    "        columns=[\"national_merge\"]\n",
    "    )\n",
    "\n",
    "    return df_holidays_prep_national"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holidays_prep_merged(df_holidays, df_stores):\n",
    "\n",
    "    # Load prep functions from local, Regional and National df's\n",
    "    df_holidays_prep_local = holidays_prep_local(df_holidays, df_stores)\n",
    "\n",
    "    df_holidays_prep_regional = holidays_prep_regional(df_holidays, df_stores)\n",
    "\n",
    "    df_holidays_prep_national = holidays_prep_national(df_holidays, df_stores)\n",
    "\n",
    "    # Combine local, regional and national dataframes into 1 merged dataframe\n",
    "    df_holidays_merged = pd.concat(\n",
    "        [df_holidays_prep_local, df_holidays_prep_regional, df_holidays_prep_national]\n",
    "    )\n",
    "\n",
    "    # Clean df_holidays_merged by dropping locale_name\", \"city\", \"state\"\n",
    "    df_holidays_merged = df_holidays_merged.drop(\n",
    "        columns=[\"locale_name\", \"city\", \"state\"]\n",
    "    )\n",
    "\n",
    "    # Rename 'type' of holiday to 'holiday_type'\n",
    "    df_holidays_merged = df_holidays_merged.rename(columns={\"type\": \"holiday_type\"})\n",
    "\n",
    "    return df_holidays_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill newly created NaN columns, due to holiday join, with 'no' on thates where there are now holidays\n",
    "def holidays_fill_no_normal(df):\n",
    "\n",
    "    cat_col = df.select_dtypes(include=[\"category\"]).columns\n",
    "\n",
    "    for col in cat_col:\n",
    "\n",
    "        if \"no\" not in df[col].cat.categories:\n",
    "\n",
    "            df[col] = df[col].cat.add_categories(\"no\")\n",
    "\n",
    "    df[cat_col] = df[cat_col].fillna(\"no\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.X Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "def merge_datasets(df_sales, df_items, df_stores, df_holidays):\n",
    "\n",
    "    # Holidays prep\n",
    "    df_holidays_merged = holidays_prep_merged(df_holidays, df_stores)\n",
    "\n",
    "    # Holidays merge on sales\n",
    "    df_merged = df_sales.merge(df_holidays_merged, on=[\"date\", \"store_nbr\"], how=\"left\")\n",
    "    df_merged = holidays_fill_no_normal(df_merged)\n",
    "\n",
    "    # Stores merged with sales+holidays\n",
    "    df_merged = df_merged.merge(df_stores, on=\"store_nbr\", how=\"left\")\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # To-do: Check if problem is in dtype of item_nbr of df_merged or df_items\n",
    "\n",
    "    print(df_merged[\"item_nbr\"].dtype)\n",
    "    print(df_items[\"item_nbr\"].dtype)\n",
    "\n",
    "    # # Change the dtype for item_nbr from uint32 to int32\n",
    "    df_merged[\"item_nbr\"] = df_merged[\"item_nbr\"].astype(int)\n",
    "    df_items[\"item_nbr\"] = df_items[\"item_nbr\"].astype(int)\n",
    "    print(\"-\" * 30)\n",
    "    print(df_merged[\"item_nbr\"].dtype)\n",
    "    print(df_items[\"item_nbr\"].dtype)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "\n",
    "    # Items merged with sales+holidays+stores\n",
    "    df_merged = df_merged.merge(df_items, on=\"item_nbr\", how=\"left\")\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = merge_datasets(df_sales, df_items, df_stores, df_holidays)  # --> 2.9 GB\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>date</th>\n",
       "      <th>holiday_type</th>\n",
       "      <th>locale</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>family</th>\n",
       "      <th>class</th>\n",
       "      <th>perishable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578</td>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>2.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>BREAD/BAKERY</td>\n",
       "      <td>2712</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>105574</td>\n",
       "      <td>8.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>580</td>\n",
       "      <td>1</td>\n",
       "      <td>105575</td>\n",
       "      <td>15.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>581</td>\n",
       "      <td>1</td>\n",
       "      <td>105577</td>\n",
       "      <td>2.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>582</td>\n",
       "      <td>1</td>\n",
       "      <td>105737</td>\n",
       "      <td>2.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>583</td>\n",
       "      <td>1</td>\n",
       "      <td>105857</td>\n",
       "      <td>12.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>584</td>\n",
       "      <td>1</td>\n",
       "      <td>106716</td>\n",
       "      <td>2.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>585</td>\n",
       "      <td>1</td>\n",
       "      <td>108696</td>\n",
       "      <td>3.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>DELI</td>\n",
       "      <td>2636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>586</td>\n",
       "      <td>1</td>\n",
       "      <td>108698</td>\n",
       "      <td>6.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>DELI</td>\n",
       "      <td>2644</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>587</td>\n",
       "      <td>1</td>\n",
       "      <td>108701</td>\n",
       "      <td>3.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>DELI</td>\n",
       "      <td>2644</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  store_nbr  item_nbr  unit_sales  onpromotion       date holiday_type  \\\n",
       "0  578          1    103665         2.0         <NA> 2013-01-02           no   \n",
       "1  579          1    105574         8.0         <NA> 2013-01-02           no   \n",
       "2  580          1    105575        15.0         <NA> 2013-01-02           no   \n",
       "3  581          1    105577         2.0         <NA> 2013-01-02           no   \n",
       "4  582          1    105737         2.0         <NA> 2013-01-02           no   \n",
       "5  583          1    105857        12.0         <NA> 2013-01-02           no   \n",
       "6  584          1    106716         2.0         <NA> 2013-01-02           no   \n",
       "7  585          1    108696         3.0         <NA> 2013-01-02           no   \n",
       "8  586          1    108698         6.0         <NA> 2013-01-02           no   \n",
       "9  587          1    108701         3.0         <NA> 2013-01-02           no   \n",
       "\n",
       "  locale   city      state type  cluster        family  class  perishable  \n",
       "0     no  Quito  Pichincha    D       13  BREAD/BAKERY   2712           1  \n",
       "1     no  Quito  Pichincha    D       13     GROCERY I   1045           0  \n",
       "2     no  Quito  Pichincha    D       13     GROCERY I   1045           0  \n",
       "3     no  Quito  Pichincha    D       13     GROCERY I   1045           0  \n",
       "4     no  Quito  Pichincha    D       13     GROCERY I   1044           0  \n",
       "5     no  Quito  Pichincha    D       13     GROCERY I   1092           0  \n",
       "6     no  Quito  Pichincha    D       13     GROCERY I   1032           0  \n",
       "7     no  Quito  Pichincha    D       13          DELI   2636           1  \n",
       "8     no  Quito  Pichincha    D       13          DELI   2644           1  \n",
       "9     no  Quito  Pichincha    D       13          DELI   2644           1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head(10)\n",
    "# to-do: rename columns to holiday_locale, store_city, store_type, store_cluster, item_family, item_class\n",
    "\n",
    "# to-do: re-order column order: date, store_nbr, item_nbr, unit_sales ,...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>date</th>\n",
       "      <th>holiday_type</th>\n",
       "      <th>locale</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>family</th>\n",
       "      <th>class</th>\n",
       "      <th>perishable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52470497</th>\n",
       "      <td>67847544</td>\n",
       "      <td>39</td>\n",
       "      <td>1113872</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Cuenca</td>\n",
       "      <td>Azuay</td>\n",
       "      <td>B</td>\n",
       "      <td>6</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43870883</th>\n",
       "      <td>47651989</td>\n",
       "      <td>40</td>\n",
       "      <td>129296</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Machala</td>\n",
       "      <td>El Oro</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54923315</th>\n",
       "      <td>94462931</td>\n",
       "      <td>47</td>\n",
       "      <td>258396</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>A</td>\n",
       "      <td>14</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8454429</th>\n",
       "      <td>6050884</td>\n",
       "      <td>23</td>\n",
       "      <td>268443</td>\n",
       "      <td>11.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Ambato</td>\n",
       "      <td>Tungurahua</td>\n",
       "      <td>D</td>\n",
       "      <td>9</td>\n",
       "      <td>CLEANING</td>\n",
       "      <td>3026</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49222232</th>\n",
       "      <td>54835059</td>\n",
       "      <td>27</td>\n",
       "      <td>559044</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-08-25</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Daule</td>\n",
       "      <td>Guayas</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>BREAD/BAKERY</td>\n",
       "      <td>2716</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36030559</th>\n",
       "      <td>63271437</td>\n",
       "      <td>38</td>\n",
       "      <td>1173210</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-11-27</td>\n",
       "      <td>Event</td>\n",
       "      <td>National</td>\n",
       "      <td>Loja</td>\n",
       "      <td>Loja</td>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>CLEANING</td>\n",
       "      <td>3040</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5238794</th>\n",
       "      <td>2079036</td>\n",
       "      <td>49</td>\n",
       "      <td>752987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2013-02-21</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>A</td>\n",
       "      <td>11</td>\n",
       "      <td>DAIRY</td>\n",
       "      <td>2116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90775044</th>\n",
       "      <td>121659336</td>\n",
       "      <td>15</td>\n",
       "      <td>1321497</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Ibarra</td>\n",
       "      <td>Imbabura</td>\n",
       "      <td>C</td>\n",
       "      <td>15</td>\n",
       "      <td>BREAD/BAKERY</td>\n",
       "      <td>2714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89161168</th>\n",
       "      <td>119319219</td>\n",
       "      <td>16</td>\n",
       "      <td>2081095</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-06-18</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Santo Domingo</td>\n",
       "      <td>Santo Domingo de los Tsachilas</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>PREPARED FOODS</td>\n",
       "      <td>2962</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47662202</th>\n",
       "      <td>52700401</td>\n",
       "      <td>49</td>\n",
       "      <td>1113847</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-07-30</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>A</td>\n",
       "      <td>11</td>\n",
       "      <td>DELI</td>\n",
       "      <td>2654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  store_nbr  item_nbr  unit_sales  onpromotion       date  \\\n",
       "52470497   67847544         39   1113872         6.0        False 2016-01-16   \n",
       "43870883   47651989         40    129296         6.0        False 2015-05-28   \n",
       "54923315   94462931         47    258396        11.0        False 2016-10-20   \n",
       "8454429     6050884         23    268443        11.0         <NA> 2013-05-25   \n",
       "49222232   54835059         27    559044         3.0        False 2015-08-25   \n",
       "36030559   63271437         38   1173210         2.0        False 2015-11-27   \n",
       "5238794     2079036         49    752987         1.0         <NA> 2013-02-21   \n",
       "90775044  121659336         15   1321497         3.0        False 2017-07-10   \n",
       "89161168  119319219         16   2081095         1.0        False 2017-06-18   \n",
       "47662202   52700401         49   1113847         2.0        False 2015-07-30   \n",
       "\n",
       "         holiday_type    locale           city  \\\n",
       "52470497           no        no         Cuenca   \n",
       "43870883           no        no        Machala   \n",
       "54923315           no        no          Quito   \n",
       "8454429            no        no         Ambato   \n",
       "49222232           no        no          Daule   \n",
       "36030559        Event  National           Loja   \n",
       "5238794            no        no          Quito   \n",
       "90775044           no        no         Ibarra   \n",
       "89161168           no        no  Santo Domingo   \n",
       "47662202           no        no          Quito   \n",
       "\n",
       "                                   state type  cluster          family  class  \\\n",
       "52470497                           Azuay    B        6       GROCERY I   1045   \n",
       "43870883                          El Oro    C        3       GROCERY I   1032   \n",
       "54923315                       Pichincha    A       14       GROCERY I   1010   \n",
       "8454429                       Tungurahua    D        9        CLEANING   3026   \n",
       "49222232                          Guayas    D        1    BREAD/BAKERY   2716   \n",
       "36030559                            Loja    D        4        CLEANING   3040   \n",
       "5238794                        Pichincha    A       11           DAIRY   2116   \n",
       "90775044                        Imbabura    C       15    BREAD/BAKERY   2714   \n",
       "89161168  Santo Domingo de los Tsachilas    C        3  PREPARED FOODS   2962   \n",
       "47662202                       Pichincha    A       11            DELI   2654   \n",
       "\n",
       "          perishable  \n",
       "52470497           0  \n",
       "43870883           0  \n",
       "54923315           0  \n",
       "8454429            0  \n",
       "49222232           1  \n",
       "36030559           0  \n",
       "5238794            1  \n",
       "90775044           1  \n",
       "89161168           1  \n",
       "47662202           1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.X Brainstorm ideas for imputing missing values\n",
    "\n",
    "Creating NaN sales for missing days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward and Backward fill --> items, stores, holidays\n",
    "df[\"sales\"].fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "df[\"sales\"].fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "# Interpolate between missing datapoints --> sales\n",
    "\n",
    "df[\"column_name\"].interpolate(method=\"linear\", inplace=True)\n",
    "\n",
    "df[\"column_name\"].interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "df[\"column_name\"].interpolate(method=\"polynomial\", order=2, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  include all daily dates in the range, filling missing dates with NaNs\n",
    "df = df.reindex(pd.date_range(start=df.index.min(), end=df.index.max(), freq=\"D\"))\n",
    "\n",
    "all_dates = pd.date_range(start=df[\"date\"].min(), end=df[\"date\"].max(), freq=\"D\")\n",
    "all_combinations = pd.MultiIndex.from_product(\n",
    "    [df[\"store_nbr\"].unique(), df[\"item_nbr\"].unique(), all_dates],\n",
    "    names=[\"store_nbr\", \"item_nbr\", \"date\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Missing sales data: Closed Stores\n",
    "\n",
    "- Sales for all items a given store and date are NA\n",
    "- Action: Impute with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Missing sales data: New product\n",
    "\n",
    "•\tBefore the very first sale of an item, all observations are kept as NA\n",
    "\n",
    "•\tAfter the very first sale of an item, we go to step 3:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3  Missing sales data: Stockout on store level\n",
    "\n",
    "•      Perishable good: when there are missing values for two consecutive days for a given item and store \n",
    "\n",
    "•      Nonperishable goods: when there are missing values for 7 consecutive days for a given item and store\n",
    "\n",
    "•      Action: Impute with algorithm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 Missing sales data: Zero sales\n",
    "\n",
    "•\tAll other cases\n",
    "\n",
    "•\tAction: Impute with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.X Negative values imputing to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"unit_sales\"] = df[\"unit_sales\"].clip(lower=0)  # --> negatives inputed to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 Promotional Data \n",
    "\n",
    "•   All missing values are interpreted a day with no promotion\n",
    "\n",
    "•   Action: Inpute onpromotion N/A with False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing N/A values in boolean columns with False\n",
    "def sales_fill_onpromotion(df):\n",
    "\n",
    "    df[\"onpromotion\"] = df[\"onpromotion\"].fillna(False)\n",
    "\n",
    "    # df[\"onpromotion\"] = df[\"onpromotion\"].fillna(MISSING) #MISSING results in error, due to boolean dtype\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# To-do: Discuss Does the N/A of onpromotion need to be filled with False or with MISSING?\n",
    "MISSING = \"missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged_test = sales_fill_onpromotion(df_merged)\n",
    "\n",
    "# df_merged_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Feature construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.X Extracting datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime_features(df):\n",
    "    \"\"\"\n",
    "    Extracting datetime features\n",
    "    year, month, day of month, weekday (1-7), week number-year, week_year_date\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure the date column is sorted\n",
    "    df = df.sort_values(\"date\")\n",
    "\n",
    "    # df[\"year\"] = df[\"date\"].dt.year\n",
    "    # df[\"month\"] = df[\"date\"].dt.month\n",
    "    # df[\"day\"] = df[\"date\"].dt.day\n",
    "\n",
    "    # Adjusting weekday to start from 1 (Monday) to 7 (Sunday)\n",
    "    df[\"weekday\"] = df[\"date\"].dt.dayofweek + 1\n",
    "\n",
    "    # Adding week number-year feature\n",
    "    df[\"week_number\"] = df[\"date\"].dt.isocalendar().week\n",
    "    df[\"week_year\"] = df[\"week_number\"].astype(str).str.zfill(2) + df[\"year\"].astype(\n",
    "        str\n",
    "    )\n",
    "\n",
    "    # Convert week_year to datetime with monday as startdate of week\n",
    "    df[\"week_year_date\"] = pd.to_datetime(\n",
    "        df[\"year\"].astype(str) + df[\"week_number\"].astype(str).str.zfill(2) + \"1\",\n",
    "        format=\"%Y%W%w\",\n",
    "    )\n",
    "\n",
    "    # Adding trend feature: number of weeks since the start of the dataset\n",
    "    start_date = df[\"date\"].min()\n",
    "    df[\"weeks_since_start\"] = ((df[\"date\"] - start_date).dt.days / 7).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = extract_datetime_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.X Promotion\n",
    "\n",
    "The number of days a item was on promotion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY FROM OLD NOTEBOOK\n",
    "# TO-DO 1: transform with new df names\n",
    "# TO-DO 2: total promotion days month --> week\n",
    "\n",
    "\n",
    "def onpromotion_month_count(df):\n",
    "\n",
    "    if \"onpromotion\" in df.columns:\n",
    "\n",
    "        df[\"onpromotion_month_count\"] = df.groupby(\n",
    "            [\"item_nbr\", \"store_nbr\", \"day\", \"month\", \"year\"]\n",
    "        )[\"onpromotion\"].transform(\"sum\")\n",
    "\n",
    "        print(\n",
    "            \"Change: 'onpromotion' column transformed to 'onpromotion_month_count' feature.\"\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        print(\"The DataFrame does not contain an 'onpromotion' column.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_agg = (\n",
    "    onpromotion_month_count(df_0)  # Transformation to 'onpromotion_month_count' feature\n",
    "    .drop(\n",
    "        columns=[\"id\", \"date\", \"onpromotion\"]\n",
    "    )  # Drop unnecessary columns \"id\", \"date\", \"onpromotion\"\n",
    "    .groupby([\"month\", \"year\", \"store_nbr\", \"item_nbr\"])\n",
    "    .agg({\"unit_sales\": \"sum\", \"onpromotion_month_count\": \"sum\"})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.X Store closed on 25-12 and 01-01 \n",
    "\n",
    "--> can we also use this feature to include the excluded stores with >9 days data, due to closing or later openings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_case_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
